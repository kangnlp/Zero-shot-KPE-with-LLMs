{"doc": "The main result of this paper is an explicit disperser for two independent sources on n bits , each of entropy k = n o(1) . Put differently , setting N = 2n and K = 2k , we construct explicit N N Boolean matrices for which no K K sub-matrix is monochromatic . Viewed as adjacency matrices of bipartite graphs , this gives an explicit construction of K-Ramsey bipartite graphs of size N . This greatly improves the previous bound of k = o(n) of Barak , Kindler , Shaltiel , Sudakov and Wigderson 4 . It also significantly improves the 25-year record of k = ~ O(n) on the special case of Ramsey graphs , due to Frankl and Wilson 9 . The construction uses (besides \"classical\" extractor ideas) almost all of the machinery developed in the last couple of years for extraction from independent sources , including: Bourgain's extractor for 2 independent sources of some entropy rate &lt; 1/2 5 Raz's extractor for 2 independent sources , one of which has any entropy rate &gt; 1/2 18 Rao's extractor for 2 independent block-sources of entropy n (1) 17 The \"Challenge-Response\" mechanism for detecting \"entropy concentration\" of 4 . The main novelty comes in a bootstrap procedure which allows the Challenge-Response mechanism of 4 to be used with sources of less and less entropy , using recursive calls to itself . Subtleties arise since the success of this mechanism depends on restricting the given sources , and so recursion constantly changes the original sources . These are resolved via a new construct , in between a disperser and an extractor , which behaves like an extractor on sufficiently large subsources of the given ones . This version is only an extended abstract , please see the full version , available on the authors' homepages , for more details . INTRODUCTION This paper deals with randomness extraction from weak random sources . Here a weak random source is a distribution which contains some entropy . The extraction task is to design efficient algorithms (called extractors) to convert this entropy into useful form , namely a sequence of independent unbiased bits . Beyond the obvious motivations (potential use of physical sources in pseudorandom generators and in derandomization) , extractors have found applications in a variety of areas in theoretical computer science where randomness does not seem an issue , such as in efficient constructions of communication networks 24 , 7 , error correcting codes 22 , 12 , data structures 14 and more. Most work in this subject over the last 20 years has focused on what is now called seeded extraction , in which the extractor is given as input not only the (sample from the) defective random source , but also a few truly random bits (called the seed) . A comprehensive survey of much of this body of work is 21 . Another direction , which has been mostly dormant till about two years ago , is (seedless , deterministic) extraction from a few independent weak sources . This", "label": ["sum-product theorem", "distribution", "explicit disperser", "construction of disperser", "extractors", "recursion", "subsource somewhere extractor", "structure", "bipartite graph", "extractors", "independent sources", "extractor", "tools", "ramsey graphs", "disperser", "polynomial time computable disperser", "resiliency", "theorem", "ramsey graphs", "block-sources", "deficiency", "termination", "entropy", "ramsey graph", "independent sources", "algorithms", "independent source", "subsource", "dispersers", "randomness extraction"], "stemmed_label": ["sum-product theorem", "distribut", "explicit dispers", "construct of dispers", "extractor", "recurs", "subsourc somewher extractor", "structur", "bipartit graph", "extractor", "independ sourc", "extractor", "tool", "ramsey graph", "dispers", "polynomi time comput dispers", "resili", "theorem", "ramsey graph", "block-sourc", "defici", "termin", "entropi", "ramsey graph", "independ sourc", "algorithm", "independ sourc", "subsourc", "dispers", "random extract"]}
{"doc": "This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf ) . We show that an intuitive idf -based probability function for the probability of a term being informative assumes disjoint document events . By assuming documents to be independent rather than disjoint , we arrive at a Poisson-based probability of being informative . The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models . INTRODUCTION AND BACKGROUND The inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of retrieved objects . With N being the total number of documents , and n(t) being the number of documents in which term t occurs , the idf is defined as follows: idf(t) := - log n(t) N , 0 &lt;= idf(t) &lt; Ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well , this has been shown in numerous applications . Also , it is well known that the combination of a document-specific term Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. SIGIR'03 , July 28August 1 , 2003 , Toronto , Canada. Copyright 2003 ACM 1-58113-646-3/03/0007 ... $ 5.00. weight and idf works better than idf alone . This approach is known as tf-idf , where tf(t , d) (0 &lt;= tf(t , d) &lt;= 1) is the so-called term frequency of term t in document d . The idf reflects the discriminating power (informativeness) of a term , whereas the tf reflects the occurrence of a term. The idf alone works better than the tf alone does . An explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as \"noisy\" terms . We use the notion of \"noisy\" terms rather than \"fre-quent\" terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as within-document frequency) of a term in a document . We associate \"noise\" with the document frequency of a term in a collection , and we associate \"occurrence\" with the within-document frequency of a term . The tf of a noisy term might be high in a document , but noisy terms are not good candidates for representing a document . Therefore , the removal of noisy terms (known as \"stopword removal\") is essential when applying tf . In a tf-idf approach , the removal of stopwords is conceptually obsolete , if stopwords are just words with a low idf . From a", "label": ["inverse document frequency (idf)", "independent and disjoint documents", "computer science", "information search", "probability theories", "poisson based probability", "term frequency", "probabilistic retrieval models", "probability of being informative", "independent documents", "disjoint documents", "normalisation", "relevance-based ranking of retrieved objects", "information theory", "noise probability", "frequency-based term noise probability", "poisson-based probability of being informative", "assumptions", "collection space", "poisson distribution", "probabilistic information retrieval", "document space", "document retrieval", "entropy", "frequency-based probability", "document frequency", "inverse document frequency", "information theory", "independence assumption", "inverse document frequency", "maximal informative signal"], "stemmed_label": ["invers document frequenc (idf)", "independ and disjoint document", "comput scienc", "inform search", "probabl theori", "poisson base probabl", "term frequenc", "probabilist retriev model", "probabl of be inform", "independ document", "disjoint document", "normalis", "relevance-bas rank of retriev object", "inform theori", "nois probabl", "frequency-bas term nois probabl", "poisson-bas probabl of be inform", "assumpt", "collect space", "poisson distribut", "probabilist inform retriev", "document space", "document retriev", "entropi", "frequency-bas probabl", "document frequenc", "invers document frequenc", "inform theori", "independ assumpt", "invers document frequenc", "maxim inform signal"]}
{"doc": "In the present paper , we will describe the design and implementation of a real-time distributed system of Web crawling running on a cluster of machines . The system crawls several thousands of pages every second , includes a high-performance fault manager , is platform independent and is able to adapt transparently to a wide range of configurations without incurring additional hardware expenditure . We will then provide details of the system architecture and describe the technical choices for very high performance crawling . Finally , we will discuss the experimental results obtained , comparing them with other documented systems . INTRODUCTION With the World Wide Web containing the vast amount of information (several thousands in 1993 , 3 billion today) that it does and the fact that it is ever expanding , we need a way to find the right information (multimedia of textual). We need a way to access the information on specific subjects that we require. To solve the problems above several programs and algorithms were designed that index the web , these various designs are known as search engines , spiders , crawlers , worms or knowledge robots graph in its simplest terms . The pages are the nodes on the graph and the links are the arcs on the graph . What makes this so difficult is the vast amount of data that we have to handle, and then we must also take into account the fact that the World Wide Web is constantly growing and the fact that people are constantly updating the content of their web pages. Any High performance crawling system should offer at least the following two features. Firstly , it needs to be equipped with an intelligent navigation strategy , i.e. enabling it to make decisions regarding the choice of subsequent actions to be taken (pages to be downloaded etc). Secondly , its supporting hardware and software architecture should be optimized to crawl large quantities of documents per unit of time (generally per second) . To this we may add fault tolerance (machine crash , network failure etc.) and considerations of Web server resources. Recently we have seen a small interest in these two field . Studies on the first point include crawling strategies for important pages 9 , 17 , topic-specific document downloading 5 , 6 , 18 , 10 , page recrawling to optimize overall refresh frequency of a Web archive 8 , 7 or scheduling the downloading activity according to time 22 . However , little research has been devoted to the second point , being very difficult to implement 20 , 13 . We will focus on this latter point in the rest of this paper. Indeed , only a few crawlers are equipped with an optimized scalable crawling system , yet details of their internal workings often remain obscure (the majority being proprietary solutions). The only system to have been given a fairly in-depth description in existing literature is Mercator by Heydon and Najork of DEC/Compaq 13 used in the AltaVista search engine (some details also exist", "label": ["breadth first crawling", "hierarchical cooperation", "limiting disk access", "fault tolerance", "dominos nodes", "dominos process", "dominos distributed database", "breadth-first crawling", "repetitive crawling", "url caching", "dominos generic server", "document fingerprint", "deep web crawling", "dominos rpc concurrent", "random walks and sampling", "web crawler", "maintaiability and configurability", "deep web crawling", "high availability system", "real-time distributed system", "crawling system", "high performance crawling system", "high availability", "erlang development kit", "targeted crawling"], "stemmed_label": ["breadth first crawl", "hierarch cooper", "limit disk access", "fault toler", "domino node", "domino process", "domino distribut databas", "breadth-first crawl", "repetit crawl", "url cach", "domino gener server", "document fingerprint", "deep web crawl", "domino rpc concurr", "random walk and sampl", "web crawler", "maintaiabl and configur", "deep web crawl", "high avail system", "real-tim distribut system", "crawl system", "high perform crawl system", "high avail", "erlang develop kit", "target crawl"]}
{"doc": "This paper presents a technical overview of the Hiperlan/2 3G interworking concept . It does not attempt to provide any business justification or plan for Public Access operation . After a brief resume of public access operation below , section 2 then introduces an overview of the technologies concerned . Section 3 describes the system approach and presents the current reference architecture used within the BRAN standardisation activity . Section 4 then goes on to cover in more detail the primary functions of the system such as authentication , mobility , quality of service (QoS) and subscription . It is worth noting that since the Japanese WLAN standard HiSWANa is very similar to Hiperlan/2 , much of the technical information within this paper is directly applicable to this system , albeit with some minor changes to the authentication scheme . Additionally the high level 3G and external network interworking reference architecture is also applicable to IEEE 802.11 . Finally , section 5 briefly introduces the standardisation relationships between ETSI BRAN , WIG , 3GPP , IETF , IEEE 802.11 and MMAC HSWA . 1.1 . Public access operation Recently , mobile business professionals have been looking for a more efficient way to access corporate information systems and databases remotely through the Internet backbone. However , the high bandwidth demand of the typical office applications , such as large email attachment downloading , often calls for very fast transmission capacity . Indeed certain hot spots , like hotels , airports and railway stations are a natural place to use such services . However , in these places the time available for information download typically is fairly limited. In light of this , there clearly is a need for a public wireless access solution that could cover the demand for data intensive applications and enable smooth on-line access to corporate data services in hot spots and would allow a user to roam from a private , micro cell network (e.g. , a Hiperlan/2 Network) to a wide area cellular network or more specifically a 3G network. Together with high data rate cellular access , Hiperlan/2 has the potential to fulfil end user demands in hot spot environments . Hiperlan/2 offers a possibility for cellular operators to offer additional capacity and higher bandwidths for end users without sacrificing the capacity of the cellular users , as Hiperlans operate on unlicensed or licensed exempt frequency bands . Also , Hiperlan/2 has the QoS mechanisms that are capable to meet the mechanisms that are available in the 3G systems . Furthermore , interworking solutions enable operators to utilise the existing cellular infrastructure investments and well established roaming agreements for Hiperlan/2 network subscriber management and billing. Technology overview This section briefly introduces the technologies that are addressed within this paper. 2.1 . Hiperlan/2 summary Hiperlan/2 is intended to provide local wireless access to IP, Ethernet , IEEE 1394 , ATM and 3G infrastructure by both stationary and moving terminals that interact with access points. The intention is that access points are connected to an IP , Ethernet ,", "label": ["hiperlan/2", "interworking", "3g", "etsi", "bran", "wig", "public access"], "stemmed_label": ["hiperlan/2", "interwork", "3g", "etsi", "bran", "wig", "public access"]}
{"doc": "Many exploration and manipulation tasks benefit from a coherent integration of multiple views onto complex information spaces . This paper proposes the concept of Illustrative Shadows for a tight integration of interactive 3D graphics and schematic depictions using the shadow metaphor . The shadow metaphor provides an intuitive visual link between 3D and 2D visualizations integrating the different displays into one combined information display . Users interactively explore spatial relations in realistic shaded virtual models while functional correlations and additional textual information are presented on additional projection layers using a semantic network approach . Manipulations of one visualization immediately influence the others , resulting in an in-formationally and perceptibly coherent presentation . INTRODUCTION In many areas knowledge about structures and their meaning as well as their spatial and functional relations are required to comprehend possible effects of an intervention. For example , engineers must understand the construction of machines as a prerequisite for maintenance whereas the spatial composition of molecules and hence possible reactions are of importance for the discovering of new drugs in chemistry . Medical students need to imagine the wealth of spatial and functional correlations within the human body to master anatomy. To date , novices as well as domain experts are required to consult several , often voluminous documents in parallel to extract information for a certain intervention . Spatial relations , characteristics of structures inherently three-dimensional , such as the shape and location of structures , however , are difficult to convey on paper . Besides requiring a significant amount of images to illustrate spatial relations between only a few structures , the mental integration of multiple views to form a three-dimensional picture in mind is demanding . Spatial relations can be conveyed more ef-fectively by means of 3D models A href=\"102.html#8\" 18 . Using interactive 3D graphics , available to more and more people due to recent advances in consumer graphics hardware , the user may actively explore the spatial correlations of structures within a photorealistic virtual model (see upper left of A href=\"102.html#1\" Figure 1). Here , the visual realism of the model facilitates recognition on real encounters. Information about functional correlations , such as the interplay of muscles causing an upward motion of the human foot , has been traditionally provided by means of text and illustrations as found in textbooks . Simple , non-photorealistic drawings enriched with annotations and metagraphical symbols can be extremely powerful in conveying complex relationships and procedures (see upper right of A href=\"102.html#1\" Figure 1). Abstraction techniques reduce the complexity of the depicted structures to illustrate the important aspects thereby guiding the attention of the viewer to relevant details . In contrast to the visualization of spatial relations , 3D graphics add no significant value to the illustration of functional correlations. Figure 1: Illustrative Shadows provide an intuitive , visual link between spatial (3d) and non-spatial (2d) information displays integrating them into one combined information display. M . tibialis anterior M . extensor hallucis longus M . extensor digitorum longus M . tibialis posterior M . flexor digitorum longus", "label": ["information visualization", "spreading activation"], "stemmed_label": ["inform visual", "spread activ"]}
{"doc": "The current boom of the Web is associated with the revenues originated from on-line advertising . While search-based advertising is dominant , the association of ads with a Web page (during user navigation) is becoming increasingly important . In this work , we study the problem of associating ads with a Web page , referred to as content-targeted advertising , from a computer science perspective . We assume that we have access to the text of the Web page , the keywords declared by an advertiser , and a text associated with the advertiser's business . Using no other information and operating in fully automatic fashion , we propose ten strategies for solving the problem and evaluate their effectiveness . Our methods indicate that a matching strategy that takes into account the semantics of the problem (referred to as AAK for \"ads and keywords\") can yield gains in average precision figures of 60% compared to a trivial vector-based strategy . Further , a more sophisticated impedance coupling strategy , which expands the text of the Web page to reduce vocabulary impedance with regard to an advertisement , can yield extra gains in average precision of 50% . These are first results . They suggest that great accuracy in content-targeted advertising can be attained with appropriate algorithms . INTRODUCTION The emergence of the Internet has opened up new marketing opportunities . In fact , a company has now the possibility of showing its advertisements (ads) to millions of people at a low cost . During the 90's , many companies invested heavily on advertising in the Internet with apparently no concerns about their investment return 16 . This situation radically changed in the following decade when the failure of many Web companies led to a dropping in supply of cheap venture capital and a considerable reduction in on-line advertising investments 15 , 16 . It was clear then that more effective strategies for on-line advertising were required . For that , it was necessary to take into account short-term and long-term interests of the users related to their information needs 9 , 14 . As a consequence, many companies intensified the adoption of intrusive techniques for gathering information of users mostly without their consent 8 . This raised privacy issues which stimu-lated the research for less invasive measures 16 . More recently , Internet information gatekeepers as , for example , search engines , recommender systems , and comparison shopping services , have employed what is called paid placement strategies 3 . In such methods , an advertiser company is given prominent positioning in advertisement lists in return for a placement fee . Amongst these methods, the most popular one is a non-intrusive technique called keyword targeted marketing 16 . In this technique , keywords extracted from the user's search query are matched against keywords associated with ads provided by advertisers . A ranking of the ads , which also takes into consideration the amount that each advertiser is willing to pay , is computed. The top ranked ads are displayed in the", "label": ["", "advertisements", "triggering page", "bayesian networks", "advertising", "matching", "knn", "web", "content-targeted advertising", "impedance coupling"], "stemmed_label": ["", "advertis", "trigger page", "bayesian network", "advertis", "match", "knn", "web", "content-target advertis", "imped coupl"]}
{"doc": "The recently promulgated IT model curriculum contains IT fundamentals as one of its knowledge areas . It is intended to give students a broad understanding of (1) the IT profession and the skills that students must develop to become successful IT professionals and (2) the academic discipline of IT and its relationship to other disciplines . As currently defined , the IT fundamentals knowledge area requires 33 lecture hours to complete . The model curriculum recommends that the material relevant to the IT fundamentals knowledge area be offered early in the curriculum , for example in an introduction to IT course; however , many institutions will have to include additional material in an introductory IT course . For example , the Introduction of IT course at Georgia Southern University is used to introduce students to the available second disciplines (an important part of the Georgia Southern IT curriculum aimed at providing students with in-depth knowledge of an IT application domain) , some productivity tools , and SQL . For many programs there may be too much material in an introductory IT course . This paper describes how Georgia Southern University resolved this dilemma . INTRODUCTION The recently promulgated IT Model Curriculum , available at http://sigite.acm.org/activities/curriculum/ , consists of 12 knowledge areas including IT fundamentals (ITF) . ITF is intended to provide students with a set of foundation skills and provide an overview of the discipline of IT and its relationship to other computing disciplines . It is also intended to help students understand the diverse contexts in which IT is used and the challenges inherent in the diffusion of innovative technology. Given its foundational nature , it will not come as a surprise that the model curriculum recommends that ITF is covered early in a student's program of study , and it seems most logical that this knowledge area be covered in an introductory course in a baccalaureate program in IT. The IT Model curriculum recommends a minimum coverage of 33 lecture hours for the ITF knowledge area; however , a typical 3-credit semester course gives an instructor , at most , 45 lecture hours , and many programs will have to include additional material in an introductory course . For example , an important element of the IT program at Georgia Southern University is the inclusion of second disciplines , coherent sets of 7 courses in an IT application area , such as electronic broadcasting , law enforcement , music technology , and supply chain management ( 5 , 6 ) . Since students must begin introductory courses in their second discipline relatively early in their academic program , it is important that they be exposed to the range of second disciplines available to them early , and the most appropriate place to do this is in the introductory IT course . Also , students enrolling in the introductory IT course at Georgia Southern are not expected to have taken a computer literacy course beforehand , and it has become clear that many are weak in the use of spreadsheets .", "label": ["it fundamentals knowledge area", "it model curriculum"], "stemmed_label": ["it fundament knowledg area", "it model curriculum"]}
{"doc": "Information retrieval systems (e.g. , web search engines) are critical for overcoming information overload . A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users , resulting in inherently non-optimal retrieval performance . For example , a tourist and a programmer may use the same word \"java\" to search for different information , but the current search systems would return the same results . In this paper , we study how to infer a user's interest from the user's search context and use the inferred implicit user model for personalized search . We present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval . We develop an intelligent client-side web search agent (UCAIR) that can perform eager implicit feedback , e.g. , query expansion based on previous queries and immediate result reranking based on clickthrough information . Experiments on web search show that our search agent can improve search accuracy over the popular Google search engine . INTRODUCTION Although many information retrieval systems (e.g. , web search engines and digital library systems) have been successfully deployed, the current retrieval systems are far from optimal . A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users 17 . This inherent non-optimality is seen clearly in the following two cases: Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. CIKM'05 , October 31November 5 , 2005 , Bremen , Germany. Copyright 2005 ACM 1-59593-140-6/05/0010 ... $ 5.00. (1) Different users may use exactly the same query (e.g. , \"Java\") to search for different information (e.g. , the Java island in Indonesia or the Java programming language) , but existing IR systems return the same results for these users . Without considering the actual user , it is impossible to know which sense \"Java\" refers to in a query . (2) A user's information needs may change over time . The same user may use \"Java\" sometimes to mean the Java island in Indonesia and some other times to mean the programming language . Without recognizing the search context , it would be again impossible to recognize the correct sense. In order to optimize retrieval accuracy , we clearly need to model the user appropriately and personalize search according to each individual user . The major goal of user modeling for information retrieval is to accurately model a user's information need , which is, unfortunately , a very difficult task . Indeed , it is even hard for a user to precisely describe what his/her information need is. What information is available", "label": ["user model", "interactive retrieval", "personalized search", "information retrieval systems", "user modelling", "implicit feedback", "retrieval accuracy", "clickthrough information", "ucair"], "stemmed_label": ["user model", "interact retriev", "person search", "inform retriev system", "user model", "implicit feedback", "retriev accuraci", "clickthrough inform", "ucair"]}
{"doc": "Nearest neighbour (NN) searches and k nearest neighbour (k-NN) searches are widely used in pattern recognition and image retrieval . An NN (k-NN) search finds the closest object (closest k objects) to a query object . Although the definition of the distance between objects depends on applications , its computation is generally complicated and time-consuming . It is therefore important to reduce the number of distance computations . TLAESA (Tree Linear Approximating and Eliminating Search Algorithm) is one of the fastest algorithms for NN searches . This method reduces distance computations by using a branch and bound algorithm . In this paper we improve both the data structure and the search algorithm of TLAESA . The proposed method greatly reduces the number of distance computations . Moreover , we extend the improved method to an approximation search algorithm which ensures the quality of solutions . Experimental results show that the proposed method is efficient and finds an approximate solution with a very low error rate . Introduction NN and k-NN searches are techniques which find the closest object (closest k objects) to a query object from a database . These are widely used in pattern recognition and image retrieval . We can see examples of their applications to handwritten character recognition in (Rico-Juan & Mico 2003) and (Mico & Oncina 1998) , and so on . In this paper we consider NN (k-NN) algorithms that can work in any metric space . For any x , y , z in a metric space , the distance function d( , ) satisfies the following properties: d(x , y) = 0 x = y, d(x , y) = d(y , x), d(x , z) d(x , y) + d(y , z). Although the definition of the distance depends on applications , its calculation is generally complicated and time-consuming . We particularly call the calculation of d( , ) a distance computation. Copyright c 2006 , Australian Computer Society , Inc . This paper appeared at Twenty-Ninth Australasian Computer Science Conference (ACSC2006) , Hobart , Tasmania , Australia , January 2006 . Conferences in Research and Practice in Information Technology , Vol . 48 . Vladimir Estivill-Castro and Gill Dobbie , Ed . Reproduction for academic , not-for profit purposes permitted provided this text is included. For the NN and k-NN searches in metric spaces, some methods that can manage a large set of objects efficiently have been introduced(Hjaltason & Samet 2003) . They are categorized into two groups. The methods in the first group manage objects with a tree structure such as vp-tree(Yianilos 1993) , M-tree (Ciaccia , Patella & Zezula 1997) , sa-tree (Navarro 2002) and so forth . The methods in the second group manage objects with a distance matrix , which stores the distances between objects . The difference between two groups is caused by their approaches to fast searching . The former aims at reducing the com-putational tasks in the search process by managing objects effectively . The latter works toward reducing the number of distance computations because generally their", "label": ["approximation search", "tlaesa", "distance computaion", "k nearest neighbour search", "nearest neighbour search"], "stemmed_label": ["approxim search", "tlaesa", "distanc computaion", "k nearest neighbour search", "nearest neighbour search"]}
{"doc": "Programs in embedded languages contain invariants that are not automatically detected or enforced by their host language . We show how to use macros to easily implement partial evaluation of embedded interpreters in order to capture invariants encoded in embedded programs and render them explicit in the terms of their host language . We demonstrate the effectiveness of this technique in improving the results of a value flow analysis . 1 . One Language , Many Languages Every practical programming language contains small programming languages . For example , C's printf 18 supports a string-based output formatting language , and Java 3 supports a declarative sub-language for laying out GUI elements in a window . PLT Scheme 9 offers at least five such languages: one for formatting console output; two for regular expression matching; one for sending queries to a SQL server; and one for laying out HTML pages. In many cases , though not always , programs in these embedded special-purpose programming languages are encoded as strings . Library functions consume these strings and interpret them . Often the interpreters consume additional arguments , which they use as inputs to the little programs. Take a look at this expression in PLT Scheme: (regexp-match &quot;http://( a-z. *)/( a-z *)/&quot; line) The function regexp-match is an interpreter for the regular expression language . It consumes two arguments: a string in the regular expression language , which we consider a program , and another string , which is that program's input . A typical use looks like the example above . The first string is actually specified at the call site, while the second string is often given by a variable or an expression that reads from an input port . The interpreter attempts to match the regular expression and the second string. In PLT Scheme , the regular expression language allows programmers to specify subpatterns via parentheses . Our running example contains two such subexpressions: ( a-z. *) and ( a-z *) . If the regular expression interpreter fails to match the regular expression and the string , it produces false ( #f ); otherwise it produces a list with n + 1 elements: the first one for the overall match plus one per subexpression . Say line stands for &quot;http://aaa.bbb.edu/zzz/&quot; In this case , the regular expression matches the string , and regexp-match produces the list (list &quot;http://aaa.bbb.edu/zzz/&quot; &quot;aaa.bbb.edu&quot; &quot;zzz&quot;) The rest of the Scheme program extracts the pieces from this list and computes with them. The regexp-match expression above is a simplified excerpt from the PLT Web Server 12 . Here is a slightly larger fragment: (let ( r (regexp-match &quot;http://( a-z. *)/( a-z *)/&quot; line) ) (if r (process-url (third r) (dispatch (second r))) (log-error line))) Notice how the then-clause of the if -expression extracts the second 16 and third elements from r without any checks to confirm the length of the list . After all , the programmer knows that if r is not false, then it is a list of three elements . The embedded program says so;", "label": ["macros", "interpreter", "value flow analysis", "flow analysis", "set-based analysis", "partial evaluation", "embedded language", "partial evaluation", "regular expression", "embedded languages", "scheme"], "stemmed_label": ["macro", "interpret", "valu flow analysi", "flow analysi", "set-bas analysi", "partial evalu", "embed languag", "partial evalu", "regular express", "embed languag", "scheme"]}
{"doc": "Many real life sequence databases grow incrementally . It is undesirable to mine sequential patterns from scratch each time when a small set of sequences grow , or when some new sequences are added into the database . Incremental algorithm should be developed for sequential pattern mining so that mining can be adapted to incremental database updates . However , it is nontrivial to mine sequential patterns incrementally , especially when the existing sequences grow incrementally because such growth may lead to the generation of many new patterns due to the interactions of the growing subsequences with the original ones . In this study , we develop an efficient algorithm , IncSpan , for incremental mining of sequential patterns , by exploring some interesting properties . Our performance study shows that IncSpan outperforms some previously proposed incremental algorithms as well as a non-incremental one with a wide margin . INTRODUCTION Sequential pattern mining is an important and active research topic in data mining 1 , 5 , 4 , 8 , 13 , 2 , with broad applications , such as customer shopping transaction analysis , mining web logs , mining DNA sequences , etc. There have been quite a few sequential pattern or closed sequential pattern mining algorithms proposed in the previous work , such as 10 , 8 , 13 , 2 , 12 , 11 , that mine frequent subsequences from a large sequence database efficiently . These algorithms work in a one-time fashion: mine the entire database and obtain the set of results . However , in many applications , databases are updated incrementally . For example , customer shopping transaction database is growing daily due to the appending of newly purchased items for existing customers for their subsequent purchases and/or insertion of new shopping sequences for new customers . Other examples include Weather sequences and patient treatment sequences which grow incrementally with time . The existing sequential mining algorithms are not suitable for handling this situation because the result mined from the old database is no longer valid on the updated database , and it is intolerably inefficient to mine the updated databases from scratch. There are two kinds of database updates in applications: (1) inserting new sequences (denoted as INSERT) , and (2) appending new itemsets/items to the existing sequences (denoted as APPEND) . A real application may contain both. It is easier to handle the first case: INSERT . An important property of INSERT is that a frequent sequence in DB = DB db must be frequent in either DB or db (or both) . If a sequence is infrequent in both DB and db, it cannot be frequent in DB , as shown in Figure 1 . This property is similar to that of frequent patterns , which has been used in incremental frequent pattern mining 3 , 9 , 14 . Such incremental frequent pattern mining algorithms can be easily extended to handle sequential pattern mining in the case of INSERT. It is far trickier to handle the second case , APPEND ,", "label": ["database updates", "sequence database", "shared projection", "frequent itemsets", "optimization", "buffering pattern", "sequential pattern", "buffering patterns", "reverse pattern matching", "incremental mining"], "stemmed_label": ["databas updat", "sequenc databas", "share project", "frequent itemset", "optim", "buffer pattern", "sequenti pattern", "buffer pattern", "revers pattern match", "increment mine"]}
{"doc": "A technical infrastructure for storing , querying and managing RDF data is a key element in the current semantic web development . Systems like Jena , Sesame or the ICS-FORTH RDF Suite are widely used for building semantic web applications . Currently , none of these systems supports the integrated querying of distributed RDF repositories . We consider this a major shortcoming since the semantic web is distributed by nature . In this paper we present an architecture for querying distributed RDF repositories by extending the existing Sesame system . We discuss the implications of our architecture and propose an index structure as well as algorithms for query processing and optimization in such a distributed context . MOTIVATION The need for handling multiple sources of knowledge and information is quite obvious in the context of semantic web applications. First of all we have the duality of schema and information content where multiple information sources can adhere to the same schema. Further , the re-use , extension and combination of multiple schema files is considered to be common practice on the semantic web 7 . Despite the inherently distributed nature of the semantic web , most current RDF infrastructures (for example 4 ) store information locally as a single knowledge repository , i.e. , RDF models from remote sources are replicated locally and merged into a single model. Distribution is virtually retained through the use of namespaces to distinguish between different models . We argue that many interesting applications on the semantic web would benefit from or even require an RDF infrastructure that supports real distribution of information sources that can be accessed from a single point . Beyond Copyright is held by the author/owner(s). WWW2004 , May 1722 , 2004 , New York , New York , USA. ACM 1-58113-844-X/04/0005. the argument of conceptual adequacy , there are a number of technical reasons for real distribution in the spirit of distributed databases: Freshness: The commonly used approach of using a local copy of a remote source suffers from the problem of changing information . Directly using the remote source frees us from the need of managing change as we are always working with the original. Flexibility: Keeping different sources separate from each other provides us with a greater flexibility concerning the addition and removal of sources . In the distributed setting , we only have to adjust the corresponding system parameters. In many cases , it will even be unavoidable to adopt a distributed architecture , for example in scenarios in which the data is not owned by the person querying it . In this case , it will often not be permitted to copy the data . More and more information providers , however, create interfaces that can be used to query the information . The same holds for cases where the information sources are too large to just create a single model containing all the information , but they still can be queried using a special interface (Musicbrainz is an example of this case) . Further , we might want", "label": ["index structure", "external sources", "query optimization", "distributed architecture", "repositories", "rdf", "infrastructure", "rdf querying", "optimization", "index structures", "semantic web", "join ordering problem"], "stemmed_label": ["index structur", "extern sourc", "queri optim", "distribut architectur", "repositori", "rdf", "infrastructur", "rdf queri", "optim", "index structur", "semant web", "join order problem"]}
{"doc": "We bridge the gap between functional evaluators and abstract machines for the λ-calculus , using closure conversion , transformation into continuation-passing style , and defunctionalization . We illustrate this approach by deriving Krivine's abstract machine from an ordinary call-by-name evaluator and by deriving an ordinary call-by-value evaluator from Felleisen et al.'s CEK machine . The first derivation is strikingly simpler than what can be found in the literature . The second one is new . Together , they show that Krivine's abstract machine and the CEK machine correspond to the call-by-name and call-by-value facets of an ordinary evaluator for the λ-calculus . We then reveal the denotational content of Hannan and Miller's CLS machine and of Landin's SECD machine . We formally compare the corresponding evaluators and we illustrate some degrees of freedom in the design spaces of evaluators and of abstract machines for the λ-calculus with computational effects . Finally , we consider the Categorical Abstract Machine and the extent to which it is more of a virtual machine than an abstract machine Introduction and related work In Hannan and Miller's words 23 , Section 7 , there are fundamental differences between denotational definitions and definitions of abstract machines . While a functional programmer tends to be familiar with denotational definitions 36 , he typically wonders about the following issues: Design: How does one design an abstract machine? How were existing abstract machines , starting with Landin's SECD machine , designed? How does one make variants of an existing abstract machine? How does one extend an existing abstract machine to a bigger source language? How does one go about designing a new abstract machine? How does one relate two abstract machines? Correctness: How does one prove the correctness of an abstract machine? Assuming it implements a reduction strategy, should one prove that each of its transitions implements a part of this strategy? Or should one characterize it in reference to a given evaluator , or to another abstract machine? A variety of answers to these questions can be found in the literature . Landin invented the SECD machine as an implementation model for functional languages 26 , and Plotkin proved its correctness in connection with an evaluation function 30 , Section 2 . Krivine discovered an abstract machine from a logical standpoint 25 , and Cregut proved its correctness in reference to a reduction strategy; he also generalized it from weak to strong normalization 7 . Curien discovered the Categorical Abstract Machine from a categorical standpoint 6 , 8 . Felleisen et al . invented the CEK machine from an operational standpoint 16 , 17 , 19 . Hannan and Miller discovered the CLS machine from a proof-theoretical standpoint 23 . Many people derived , invented , or (re-)discovered Krivine's machine . Many others proposed modifications of existing machines . And recently , Rose presented a method to construct abstract machines from reduction rules 32 , while Hardin , Maranget , and Pagano presented a method to extract the reduction strategy of a machine by extracting axioms from its transitions and", "label": ["call-by-name", "interpreters", "closure conversion", "evaluator", "defunctionalization", "call-by-value", "transformation into continuation-passing style (cps)", "abstract machines", "abstract machine"], "stemmed_label": ["call-by-nam", "interpret", "closur convers", "evalu", "defunction", "call-by-valu", "transform into continuation-pass style (cps)", "abstract machin", "abstract machin"]}
{"doc": "Although most time-series data mining research has concentrated on providing solutions for a single distance function , in this work we motivate the need for a single index structure that can support multiple distance measures . Our specific area of interest is the efficient retrieval and analysis of trajectory similarities . Trajectory datasets are very common in environmental applications , mobility experiments , video surveillance and are especially important for the discovery of certain biological patterns . Our primary similarity measure is based on the Longest Common Subsequence (LCSS) model , that offers enhanced robustness , particularly for noisy data , which are encountered very often in real world applications . However , our index is able to accommodate other distance measures as well , including the ubiquitous Euclidean distance , and the increasingly popular Dynamic Time Warping (DTW) . While other researchers have advocated one or other of these similarity measures , a major contribution of our work is the ability to support all these measures without the need to restructure the index . Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall . The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW . INTRODUCTION In this work we present an efficient and compact , external memory index for fast detection of similar trajectories . Trajectory data are prevalent in diverse fields of interest such as meteorology , GPS tracking , wireless applications , video tracking 5 and motion capture 18 . Recent advances in mobile computing , sensor and GPS technology have made it possible to collect large amounts of spatiotemporal data and The research of this author was supported by NSF ITR 0220148 , NSF CAREER 9907477 , NSF IIS 9984729 , and NRDRP Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . SIGKDD '03 , August 24-27 , 2003 , Washington, DC , USA. Copyright 2003 ACM 1-58113-737-0/03/0008... $ 5.00. there is increasing interest in performing data analysis tasks over such data 17 . In mobile computing , users equipped with mobile devices move in space and register their location at different time instances to spatiotemporal databases via wireless links . In environmental information systems, tracking animals and weather conditions is very common and large datasets can be created by storing locations of observed objects over time . Human motion data generated by tracking simultaneously various body joints are also multidimensional trajectories . In this field of computer graphics fundamental operations include the clustering of similar movements , leading to a multitude", "label": ["dynamic time warping", "indexing", "trajectory", "distance function", "dynamic time warping (dtw)", "similarity", "longest common subsequence", "trajectories", "longest common subsequence (lcss)", "measure"], "stemmed_label": ["dynam time warp", "index", "trajectori", "distanc function", "dynam time warp (dtw)", "similar", "longest common subsequ", "trajectori", "longest common subsequ (lcss)", "measur"]}
{"doc": "INTRODUCTION Typical Web search engines are designed to run short queries against a huge collection of hyperlinked documents quickly and cheaply , and are often tuned for the types of queries people submit most often 2 . Many other types of applications exist for which large , open collections like the Web would be a valuable resource . However , these applications may require much more advanced support from information retrieval technology than is currently available . In particular , an application may have to describe more complex information needs , with a varied set of properties and data models , including aspects of the user's context and goals . In this paper we present an overview of one such application , the REAP project , whose main purpose is to provide reader-specific practice for improved reading comprehension . (REAP stands for REAder-specific Practice.) A key component of REAP is an advanced search model that can find documents satisfying a set of diverse and possibly complex lexical constraints , including a passage's topic , reading level (e.g . 3rd grade) , use of syntax (simple vs . complex sentence structures) , and vocabulary that is known or unknown to the student . Searching is performed on a database of documents automatically gathered from the Web which have been analyzed and annotated with a rich set of linguistic metadata . The Web is a potentially valuable resource for providing reading material of interest to the student because of its extent , variety , and currency for popular topics . SYSTEM DESCRIPTION Here we describe the high-level design of the REAP information retrieval system , including document database requirements and construction , annotations , and a brief description of the retrieval model. 2.1 Database Construction Our goal is to present passages that are interesting to students, whether they are on current topics such as pop stars or sports events , or related to particular classroom projects . To this end, we use the Web as our source of reading practice materials because of its extent , variety , and currency of information. We want coverage of topics in the database to be deeper in areas that are more likely to be of interest to students. Coverage of other areas is intended to be broad , but more shallow . We therefore gather documents for the database using focused crawling 3 . The current prototype uses a page's reading difficulty to set priority for all links from that page equally , based on the distance from the target reading level range . We plan to explore more refined use of annotations to direct the crawl on a link-by-link basis . In our prototype , we collected 5 million pages based on an initial set of 20,000 seed pages acquired from the Google Kids Directory 7 . Our goal is to have at least 20 million pages that focus on material for grades 1 through 8 . The document database must be large enough that the most important lexical constraints are satisfied by at least a", "label": ["computer-assisted learning", "user model", "searching", "reading comprehension", "information retrieval", "information retrieval"], "stemmed_label": ["computer-assist learn", "user model", "search", "read comprehens", "inform retriev", "inform retriev"]}
{"doc": "Web pages (and resources , in general) can be characterized according to their geographical locality . For example , a web page with general information about wildflowers could be considered a global page , likely to be of interest to a ge-ographically broad audience . In contrast , a web page with listings on houses for sale in a specific city could be regarded as a local page , likely to be of interest only to an audience in a relatively narrow region . Similarly , some search engine queries (implicitly) target global pages , while other queries are after local pages . For example , the best results for query wildflowers are probably global pages about wildflowers such as the one discussed above . However , local pages that are relevant to , say , San Francisco are likely to be good matches for a query houses for sale that was issued by a San Francisco resident or by somebody moving to that city . Unfortunately , search engines do not analyze the geographical locality of queries and users , and hence often produce sub-optimal results . Thus query wildflowers might return pages that discuss wildflowers in specific U.S . states (and not general information about wildflowers) , while query houses for sale might return pages with real estate listings for locations other than that of interest to the person who issued the query . Deciding whether an unseen query should produce mostly local or global pages--without placing this burden on the search engine users--is an important and challenging problem , because queries are often ambiguous or underspecify the information they are after . In this paper , we address this problem by first defining how to categorize queries according to their (often implicit) geographical locality . We then introduce several alternatives for automatically and efficiently categorizing queries in our scheme , using a variety of state-of-the-art machine learning tools . We report a thorough evaluation of our classifiers using a large sample of queries from a real web search engine , and conclude by discussing how our query categorization approach can help improve query result quality . INTRODUCTION Web pages (and resources , in general) can be characterized according to their geographical locality . For example , a web page with general information about wildflowers could be considered a global page , likely to be of interest to a ge-ographically broad audience . In contrast , a web page with listings on houses for sale in a specific city could be regarded as a local page , likely to be of interest only to an audience in a relatively narrow region . Earlier research 9 has addressed the problem of automatically computing the \"geographical scope\" of web resources. Often search engine queries (implicitly) target global web pages , while other queries are after local pages . For example, the best results for query wildflowers are probably global pages about wildflowers discussing what types of climates wildflowers grow in , where wildflowers can be purchased, or what types of wildflower species", "label": ["geographical locality", "categorization scheme", "query modification", "web search", "query categorization / query classification", "web queries", "search engines", "global page", "local page", "information retrieval", "search engine", "query classification"], "stemmed_label": ["geograph local", "categor scheme", "queri modif", "web search", "queri categor / queri classif", "web queri", "search engin", "global page", "local page", "inform retriev", "search engin", "queri classif"]}
{"doc": "Participation in social networking sites has dramatically increased in recent years . Services such as Friendster , Tribe , or the Facebook allow millions of individuals to create online profiles and share personal information with vast networks of friends - and , often , unknown numbers of strangers . In this paper we study patterns of information revelation in online social networks and their privacy implications . We analyze the online behavior of more than 4,000 Carnegie Mellon University students who have joined a popular social networking site catered to colleges . We evaluate the amount of information they disclose and study their usage of the site's privacy settings . We highlight potential attacks on various aspects of their privacy , and we show that only a minimal percentage of users changes the highly permeable privacy preferences . EVOLUTION OF ONLINE NETWORKING In recent years online social networking has moved from niche phenomenon to mass adoption . Although the concept dates back to the 1960s (with University of Illinois Plato computer-based education tool , see 16 ) , viral growth and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. WPES'05, November 7 , 2005 , Alexandria , Virginia , USA. Copyright 2005 ACM 1-59593-228-3/05/0011 ... $ 5.00. commercial interest only arose well after the advent of the Internet. 1 The rapid increase in participation in very recent years has been accompanied by a progressive diversification and sophistication of purposes and usage patterns across a multitude of different sites . The Social Software Weblog 2 now groups hundreds of social networking sites in nine categories , including business , common interests , dating , face-to -face facilitation , friends , pets , and photos. While boundaries are blurred , most online networking sites share a core of features: through the site an individual offers a \"profile\" - a representation of their sel ves (and, often , of their own social networks) - to others to peruse, with the intention of contacting or being contacted by others , to meet new friends or dates (Friendster, 3 Orkut 4 ) , find new jobs (LinkedIn 5 ) , receive or provide recommendations (Tribe 6 ) , and much more. It is not unusual for successful social networking sites to experience periods of viral growth with participation expanding at rates topping 20% a month . Liu and Maes estimate in 18 that \"well over a million self-descriptive personal profiles are available across different web-based social networks\" in the United States , and Leonard , already in 2004 , reported in 16 that world-wide \" s even million people have accounts on Friendster . ... Two million are registered to", "label": ["information relevation", "privacy", "social networking sites", "information revelation", "privacy risk", "online privacy", "online social networking", "online behavior", "college", "social network theory", "facebook", "stalking", "re-identification", "data visibility", "privacy perference"], "stemmed_label": ["inform relev", "privaci", "social network site", "inform revel", "privaci risk", "onlin privaci", "onlin social network", "onlin behavior", "colleg", "social network theori", "facebook", "stalk", "re-identif", "data visibl", "privaci perfer"]}
{"doc": "Topic distillation is the process of finding authoritative Web pages and comprehensive \"hubs\" which reciprocally endorse each other and are relevant to a given query . Hyperlink-based topic distillation has been traditionally applied to a macroscopic Web model where documents are nodes in a directed graph and hyperlinks are edges . Macroscopic models miss valuable clues such as banners , navigation panels , and template-based inclusions , which are embedded in HTML pages using markup tags . Consequently , results of macroscopic distillation algorithms have been deteriorating in quality as Web pages are becoming more complex . We propose a uniform fine-grained model for the Web in which pages are represented by their tag trees (also called their Document Object Models or DOMs) and these DOM trees are interconnected by ordinary hyperlinks . Surprisingly , macroscopic distillation algorithms do not work in the fine-grained scenario . We present a new algorithm suitable for the fine-grained model . It can dis-aggregate hubs into coherent regions by segmenting their DOMtrees . Mutual endorsement between hubs and authorities involve these regions , rather than single nodes representing complete hubs . Anecdotes and measurements using a 28-query , 366000-document benchmark suite , used in earlier topic distillation research , reveal two benefits from the new algorithm: distillation quality improves , and a by-product of distillation is the ability to extract relevant snippets from hubs which are only partially relevant to the query . Introduction Kleinberg's Hyperlink Induced Topic Search (HITS) 14 and the PageRank algorithm 3 underlying Google have revolutionized ranking technology for Web search engines. PageRank evaluates the \"prestige score\" of a page as roughly proportional to the sum of prestige scores of pages citing it (Note: To view the HTML version using Netscape , add the following line to your ~/.Xdefaults or ~/.Xresources file: Netscape*documentFonts.charset*adobe-fontspecific: iso-8859-1 For printing use the PDF version , as browsers may not print the mathematics properly.) Copyright is held by author/owner. WWW10 , May 15 , 2001 , Hong Kong. ACM1-58113-348-0/01/0005. using hyperlinks . HITS also identifies collections of resource links or \"hubs\" densely coupled to authoritative pages on a topic . The model of the Web underlying these and related systems is a directed graph with pages (HTML files) as nodes and hyperlinks as edges. Since those papers were published , the Web has been evolving in fascinating ways , apart from just getting larger. Web pages are changing from static files to dynamic views generated from complex templates and backing semi-structured databases . A variety of hypertext-specific idioms such as navigation panels , advertisement banners , link exchanges, and Web-rings , have been emerging. There is also a migration of Web content from syntac-tic HTML markups towards richly tagged , semi-structured XML documents (http://www.w3.org/XML/) interconnected at the XML element level by semantically rich links (see, e.g. , the XLink proposal at http://www.w3.org/TR/xlink/). These refinements are welcome steps to implementing what Berners-Lee and others call the semantic Web (http://www. w3.org/1999/04/13-tbl.html) , but result in document , file, and site boundaries losing their traditional significance. Continual experiments performed by", "label": ["pagerank algorithm", "segmentation", "hits", "link localization", "topic distillation", "dom", "document object model", "xml", "microscopic distillation", "text analysis", "minimum description length principle", "google", "hub fragmentation", "hyperlink", "topic distillation"], "stemmed_label": ["pagerank algorithm", "segment", "hit", "link local", "topic distil", "dom", "document object model", "xml", "microscop distil", "text analysi", "minimum descript length principl", "googl", "hub fragment", "hyperlink", "topic distil"]}
{"doc": "In this paper we present the context of the work of the Curriculum Committee on IT2005 , the IT curriculum volume described in the Overview Draft document of the Joint Task Force for Computing Curriculum 2004 . We also provide a brief introduction to the history and work of the Information Assurance Education community . These two perspectives provide the foundation for the main thrust of the paper , which is a description of the Information Assurance and Security (IAS) component of the IT2005 document . Finally , we end the paper with an example of how IAS is being implemented at BYU as a \"pervasive theme\" that is woven throughout the curriculum and conclude with some observations about the first year's experience . INTRODUCTION In December 2001 a meeting (CITC-1) of interested parties from fifteen four-year IT programs from the US along with representatives from IEEE , ACM , and ABET began work on the formalization of Information Technology as an accredited academic discipline . The effort has evolved into SIGITE , the ACM SIG for Information Technology Education . During this evolution three main efforts have proceeded in parallel: 1) Definition of accreditation standards for IT programs , 2) Creation of a model curriculum for four-year IT programs , and 3) Description of the characteristics that distinguish IT programs from the sister disciplines in computing. One of the biggest challenges during the creation of the model curriculum was understanding and presenting the knowledge area that was originally called \"security\" . Some of us were uncomfortable with the term because it was not broad enough to cover the range of concepts that we felt needed to be covered . We became aware of a community that had resolved many of the issues associated with the broader context we were seeking, Information Assurance . Information assurance has been defined as \"a set of measures intended to protect and defend information and information systems by ensuring their availability , integrity, authentication , confidentiality , and non-repudiation . This includes providing for restoration of information systems by incorporating protection , detection , and reaction capabilities.\" The IA community and work done by IA educators became useful in defining requisite security knowledge for information technology education programs. We believe that the Information Technology and the Information Assurance Education communities have much to share . At the 9 th Colloquium for Information System Security Education in Atlanta we introduced CC2005 and IT2005 to the IA Education community 1 . In the current paper we introduce the history and current state of IA education to the SIGITE community . In addition , we demonstrate how significant concepts from the Information Assurance community have been integrated into IT2005. 1.1 CC2005 and IT2005 In the first week of December of 2001 representatives from 15 undergraduate information technology (IT) programs from across the country gathered together near Provo , Utah , to develop a community and begin to establish academic standards for this rapidly growing discipline . This first Conference on Information Technology Curriculum (CITC-1) was also attended by representatives", "label": ["information assurance", "it2005 volume", "pervasive themes", "byu curriculum", "nietp program", "training standards", "in-service training development", "committee on national security systems", "citc-1", "information technology", "it", "cc2005", "ia", "sigite curriculum committee", "education", "it2005", "security knowledge", "information assurance", "ias"], "stemmed_label": ["inform assur", "it2005 volum", "pervas theme", "byu curriculum", "nietp program", "train standard", "in-servic train develop", "committe on nation secur system", "citc-1", "inform technolog", "it", "cc2005", "ia", "sigit curriculum committe", "educ", "it2005", "secur knowledg", "inform assur", "ia"]}
{"doc": "Perceptual user interfaces (PUIs) are an important part of ubiquitous computing . Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers . We propose an interactive machine-learning (IML) model that allows users to train , classify/view and correct the classifications . The concept and implementation details of IML are discussed and contrasted with classical machine learning models . Evaluations of two algorithms are also presented . We also briefly describe Image Processing with Crayons (Crayons) , which is a tool for creating new camera-based interfaces using a simple painting metaphor . The Crayons tool embodies our notions of interactive machine learning . INTRODUCTION Perceptual user interfaces (PUIs) are establishing the need for machine learning in interactive settings . PUIs like VideoPlace 8 , Light Widgets 3 , and Light Table 15,16 all use cameras as their perceptive medium . Other systems use sensors other than cameras such as depth scanners and infrared sensors 13,14,15 . All of these PUIs require machine learning and computer vision techniques to create some sort of a classifier . This classification component of the UI often demands great effort and expense . Because most developers have little knowledge on how to implement recognition in their UIs this becomes problematic . Even those who do have this knowledge would benefit if the classifier building expense were lessened . We suggest the way to decrease this expense is through the use of a visual image classifier generator , which would allow developers to add intelligence to interfaces without forcing additional programming . Similar to how Visual Basic allows simple and fast development , this tool would allow for fast integration of recognition or perception into a UI. Implementation of such a tool , however , poses many problems . First and foremost is the problem of rapidly creating a satisfactory classifier . The simple solution is to using behind-the-scenes machine learning and image processing. Machine learning allows automatic creation of classifiers, however , the classical models are generally slow to train , and not interactive . The classical machine-learning (CML) model is summarized in Figure 1 . Prior to the training of the classifier , features need to be selected . Training is then performed \"off-line\" so that classification can be done quickly and efficiently . In this model classification is optimized at the expense of longer training time . Generally, the classifier will run quickly so it can be done real-time . The assumption is that training will be performed only once and need not be interactive . Many machine-learning algorithms are very sensitive to feature selection and suffer greatly if there are very many features. Feature Selection Train Classify Interactive Use Figure 1 Classical machine learning model With CML , it is infeasible to create an interactive tool to create classifiers . CML requires the user to choose the features and wait an extended amount of time for the algorithm to train . The selection of features is very problematic for most interface designers . If one is designing", "label": ["classification", "perceptual interface", "image processing", "perceptive user interfaces", "perceptual user iinterfaces", "machine learning", "image/pixel classifier", "predict correct behaviour", "classification design loop", "interactive machine learning", "interaction", "crayons prototype", "image processing with crayons", "crayons design process", "classical machine learning"], "stemmed_label": ["classif", "perceptu interfac", "imag process", "percept user interfac", "perceptu user iinterfac", "machin learn", "image/pixel classifi", "predict correct behaviour", "classif design loop", "interact machin learn", "interact", "crayon prototyp", "imag process with crayon", "crayon design process", "classic machin learn"]}
{"doc": "The paper presents a method for pruning frequent itemsets based on background knowledge represented by a Bayesian network . The interestingness of an itemset is defined as the absolute difference between its support estimated from data and from the Bayesian network . Efficient algorithms are presented for finding interestingness of a collection of frequent itemsets , and for finding all attribute sets with a given minimum interestingness . Practical usefulness of the algorithms and their efficiency have been verified experimentally . INTRODUCTION Finding frequent itemsets and association rules in database tables has been an active research area in recent years. Unfortunately , the practical usefulness of the approach is limited by huge number of patterns usually discovered . For larger databases many thousands of association rules may be produced when minimum support is low . This creates a secondary data mining problem: after mining the data, we are now compelled to mine the discovered patterns . The problem has been addressed in literature mainly in the context of association rules , where the two main approaches are Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. KDD'04 , August 2225 , 2004 , Seattle , Washington , USA. Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. sorting rules based on some interestingness measure , and pruning aiming at removing redundant rules. Full review of such methods is beyond the scope of this paper . Overviews of interestingness measures can be found for example in 3 , 13 , 11 , 32 , some of the papers on rule pruning are 30 , 31 , 7 , 14 , 28 , 16 , 17 , 33 . Many interestingness measures are based on the divergence between true probability distributions and distributions obtained under the independence assumption . Pruning methods are usually based on comparing the confidence of a rule to the confidence of rules related to it. The main drawback of those methods is that they tend to generate rules that are either obvious or have already been known by the user . This is to be expected , since the most striking patterns which those methods select can also easily be discovered using traditional methods or are known directly from experience. We believe that the proper way to address the problem is to include users background knowledge in the process. The patterns which diverge the most from that background knowledge are deemed most interesting . Discovered patterns can later be applied to improve the background knowledge itself. Many approaches to using background knowledge in machine learning are focused on using background knowledge to speed up the hypothesis discovery process and not on discovering interesting patterns . Those", "label": ["bayesian network", "frequent itemsets", "association rules", "interestingness", "emerging pattern", "association rule", "background knowledge", "frequent itemset"], "stemmed_label": ["bayesian network", "frequent itemset", "associ rule", "interesting", "emerg pattern", "associ rule", "background knowledg", "frequent itemset"]}
{"doc": "The emergence of several radio technologies , such as Bluetooth and IEEE 802.11 , operating in the 2.4 GHz unlicensed ISM frequency band , may lead to signal interference and result in significant performance degradation when devices are colocated in the same environment . The main goal of this paper is to evaluate the effect of mutual interference on the performance of Bluetooth and IEEE 802.11b systems . We develop a simulation framework for modeling interference based on detailed MAC and PHY models . First , we use a simple simulation scenario to highlight the effects of parameters , such as transmission power , offered load , and traffic type . We then turn to more complex scenarios involving multiple Bluetooth piconets and WLAN devices . Introduction The proliferation of mobile computing devices including laptops , personal digital assistants (PDAs) , and wearable computers has created a demand for wireless personal area networks (WPANs) . WPANs allow closely located devices to share information and resources. A key challenge in the design of WPANs is adapting to a hostile radio environment that includes noise , time-varying channels , and abundant electromagnetic interference . Today , most radio technologies considered by WPANs (Bluetooth Special Interest Group 2 , and IEEE 802.15) employ the 2.4 GHz ISM frequency band , which is also used by Local Area Network (WLAN) devices implementing the IEEE 802.11b standard specifications 9 . It is anticipated that some interference will result from all these technologies operating in the same environment . WLAN devices operating in proximity to WPAN devices may significantly impact the performance of WPAN and vice versa. The main goal of this paper is to present our findings on the performance of these systems when operating in close proximity to each other . Our results are based on detailed models for the MAC , PHY , and wireless channel . Recently , a number of research activities has led to the development of tools for wireless network simulation 1,16 . While some of these tools include a PHY layer implementation , it is often abstracted to a discrete channel model that does not implement interference per se . Therefore , in order to model interference and capture the time and frequency collisions , we chose to implement an integrated MAC-PHY module. Efforts to study interference in the 2.4 GHz band are relatively recent . For example , interference caused by microwave ovens operating in the vicinity of a WLAN network has been investigated 17 and requirements on the signal-to-noise ratio (SNR) are presented by Kamerman and Erkocevic 11 . Corresponding author. E-mail: nada.golmie@nist.gov In addition , there has been several attempts at quantifying the impact of interference on both the WLAN and Bluetooth performance . Published results can be classified into at least three categories depending on whether they rely on analysis, simulation , or experimental measurements. Analytical results based on probability of packet collision were obtained by Shellhammer 13 , Ennis 4 , and Zyren 18 for the WLAN packet error and by Golmie 6 for the", "label": ["evaluation", "packet loss", "performance degradation", "ieee 802.11b", "simulation framework", "bluetooth", "interference", "hop rate", "tranmission power", "topology", "wpans", "wlan", "offered load"], "stemmed_label": ["evalu", "packet loss", "perform degrad", "ieee 802.11b", "simul framework", "bluetooth", "interfer", "hop rate", "tranmiss power", "topolog", "wpan", "wlan", "offer load"]}
{"doc": "What makes a peripheral or ambient display more effective at presenting awareness information than another ? Presently , little is known in this regard and techniques for evaluating these types of displays are just beginning to be developed . In this article , we focus on one aspect of a peripheral display's effectiveness-its ability to communicate information at a glance . We conducted an evaluation of the InfoCanvas , a peripheral display that conveys awareness information graphically as a form of information art , by assessing how well people recall information when it is presented for a brief period of time . We compare performance of the InfoCanvas to two other electronic information displays , a Web portal style and a text-based display , when each display was viewed for a short period of time . We found that participants noted and recalled significantly more information when presented by the InfoCanvas than by either of the other displays despite having to learn the additional graphical representations employed by the InfoCanvas . Introduction The Peripheral awareness displays are systems that reside in a user's environment within the periphery of the user's attention . As such , the purpose of these displays is not for monitoring vital tasks . Rather , peripheral displays best serve as communication media that people can opportunistically examine to maintain information awareness 11 , 17 . The term ambient display 22 has been used to describe systems like this as well , but to avoid confusion, throughout this document we use this term to describe peripheral awareness systems that generally convey only one piece of information . We use the term peripheral display to describe peripheral awareness systems that may present multiple information items . Both peripheral and ambient displays are designed not to distract people from their tasks at hand , but to be subtle, calm reminders that can be occasionally noticed . In addition to presenting information , the displays also frequently contribute to the aesthetics of the locale in which they are deployed 1 . Dozens of peripheral/ambient displays have been created in many shapes and form factors . Some displays , such as the dangling string 21 , tangible displays including water lamps and pinwheels 4 , and the Information Percolator 7 have utilized physical (and often everyday) objects . Other displays , such as Informative Artwork 8 and the Digital Family Portrait 16 use electronic displays to represent information in a graphical manner . All these systems primarily communicate one item of information. Other peripheral/ambient displays exist that are capable of conveying more than one information item simultaneously . The Digital Family Portrait , although primarily intended to allow geographically separated family members maintain awareness of each other , allows for the optional displaying of additional information such as weather 16 . Audio cues , instead of visual displays , have also been utilized in peripheral displays to convey multiple nuggets of information in the Audio Aura system 15 . The Kandinsky system 5 attempts to create artistic collages of various pieces of information", "label": ["evaluation", "peripheral display", "graphical representation", "awareness information", "ambient display", "text-based display", "information conveyance", "infocanvas display", "peripheral display", "information recall", "empirical evaluation", "information visualization", "web portal-like display"], "stemmed_label": ["evalu", "peripher display", "graphic represent", "awar inform", "ambient display", "text-bas display", "inform convey", "infocanva display", "peripher display", "inform recal", "empir evalu", "inform visual", "web portal-lik display"]}
{"doc": "Recent computer technologies have enabled fast high-quality 3D graphics on personal computers , and also have made the development of 3D graphical applications easier . However , most of such technologies do not sufficiently support layout and behavior aspects of 3D graphics . Geometric constraints are , in general , a powerful tool for specifying layouts and behaviors of graphical objects , and have been applied to 2D graphical user interfaces and specialized 3D graphics packages . In this paper , we present Chorus3D , a geometric constraint library for 3D graphical applications . It enables programmers to use geometric constraints for various purposes such as geometric layout , constrained dragging , and inverse kinematics . Its novel feature is to handle scene graphs by processing coordinate transformations in geometric constraint satisfaction . We demonstrate the usefulness of Chorus3D by presenting sample constraint-based 3D graphical applications . INTRODUCTION Recent advances in commodity hardware have enabled fast high-quality 3D graphics on personal computers . Also , software technologies such as VRML and Java 3D have made the development of 3D graphical applications easier . However, most of such technologies mainly focus on rendering aspects of 3D graphics , and do not sufficiently support layout and behavior aspects. Constraints are , in general , a powerful tool for specifying layouts and behaviors of graphical objects. It is widely recognized that constraints facilitate describing geometric layouts and behaviors of diagrams in 2D graphical user interfaces such as drawing editors , and therefore constraint solvers for this purpose have been extensively studied 3 , 7, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to distribute to lists , requires prior specific permission and/or fee. Int . Symp . on Smart Graphics, June 11-13 , 2002 , Hawthorne , NY , USA. Copyright 2002 ACM 1-58113-555-6/02/0600... $ 5.00 8 , 9 , 11 , 12 , 13 , 17 , 18 . Also , many specialized 3D graphics packages enable the specification of object layouts and behaviors by using constraints or similar functions. It is natural to consider that various 3D graphical applications can also be enhanced by incorporating constraints . It might seem sufficient for this purpose to modify existing 2D geometric constraint solvers to support 3D geometry . It is, however , insufficient in reality because of the essential difference between the ways of specifying 2D and 3D graphics; typical 2D graphics handles only simple coordinate systems, whereas most 3D graphics requires multiple coordinate systems with complex relations such as rotations to treat scene graphs . It means that we need to additionally support coordinate transformations in 3D geometric constraint solvers. In this paper , we present Chorus3D , a geometric constraint library for 3D graphical applications . The novel", "label": ["layout", "scene graphs", "3d graphics", "geometric layout", "constraint satisfaction", "3d graphical applications", "geometric constraints", "graphical objects", "behaviors", "coordinate transformation"], "stemmed_label": ["layout", "scene graph", "3d graphic", "geometr layout", "constraint satisfact", "3d graphic applic", "geometr constraint", "graphic object", "behavior", "coordin transform"]}
{"doc": "We propose an In-Network Data-Centric Storage (INDCS) scheme for answering ad-hoc queries in sensor networks . Previously proposed In-Network Storage (INS) schemes suffered from Storage Hot-Spots that are formed if either the sensors' locations are not uniformly distributed over the coverage area , or the distribution of sensor readings is not uniform over the range of possible reading values . Our K-D tree based Data-Centric Storage (KDDCS) scheme maintains the invariant that the storage of events is distributed reasonably uniformly among the sensors . KDDCS is composed of a set of distributed algorithms whose running time is within a poly-log factor of the diameter of the network . The number of messages any sensor has to send , as well as the bits in those messages , is poly-logarithmic in the number of sensors . Load balancing in KDDCS is based on defining and distributively solving a theoretical problem that we call the Weighted Split Median problem . In addition to analytical bounds on KDDCS individual algorithms , we provide experimental evidence of our scheme's general efficiency , as well as its ability to avoid the formation of storage hot-spots of various sizes , unlike all previous INDCS schemes . INTRODUCTION Sensor networks provide us with the means of effectively monitoring and interacting with the physical world . As an illustrative example of the type of sensor network application that concerns us here , consider an emergency/disaster scenario where sensors are deployed in the area of the disaster 17 . It is the responsibility of the sensor network to sense and store events of potential interest. An event is composed of one or more attributes (e.g . temperature, carbon monoxide level , etc.) , the identity of the sensor that sensed the event , and the time when the event was sensed . As first responders move through the disaster area with hand-held devices , they issue queries about recent events in the network . For example , the first responder might ask for the location of all sensor nodes that recorded high carbon monoxide levels in the last 15 minutes , or he might ask whether any sensor node detected movement in the last minute . Queries are picked up by sensors in the region of the first responder . The sensor network is then responsible for answering these queries . The first responders use these query answers to make decisions on how to best manage the emergency. The ad-hoc queries of the first responders will generally be multi-dimensional range queries 9 , that is , the queries concern sensor readings that were sensed over a small time window in the near past and that fall in a given range of the attribute values . In-Network Storage (INS) is a storage technique that has been specifically presented to efficiently process this type of queries . INS involves storing events locally in the sensor nodes . Storage may be in-network because it is more efficient than shipping all the data (i.e. , raw sensor readings) out of the network (for example to", "label": ["quality of data (qod)", "kddcs", "routing algorithm", "power-aware", "energy saving", "sensor network", "sensor network", "distributed algorithms", "weighted split median problem", "dim", "data persistence", "storage hot-spots", "ad-hoc queries"], "stemmed_label": ["qualiti of data (qod)", "kddc", "rout algorithm", "power-awar", "energi save", "sensor network", "sensor network", "distribut algorithm", "weight split median problem", "dim", "data persist", "storag hot-spot", "ad-hoc queri"]}
{"doc": "Topic tracking is complicated when the stories in the stream occur in multiple languages . Typically , researchers have trained only English topic models because the training stories have been provided in English . In tracking , non-English test stories are then machine translated into English to compare them with the topic models . We propose a native language hypothesis stating that comparisons would be more effective in the original language of the story . We first test and support the hypothesis for story link detection . For topic tracking the hypothesis implies that it should be preferable to build separate language-specific topic models for each language in the stream . We compare different methods of incrementally building such native language topic models . INTRODUCTION Topic detection and tracking (TDT) is a research area concerned with organizing a multilingual stream of news broadcasts as it arrives over time . TDT investigations sponsored by the U.S . government include five different tasks: story link detection , clustering (topic detection) , topic tracking , new event (first story) detection , and story segmentation . The present research focuses on topic tracking , which is similar to filtering in information retrieval . Topics are defined by a small number of (training) stories, typically one to four , and the task is to find all the stories on those topics in the incoming stream. TDT evaluations have included stories in multiple languages since 1999 . TDT2 contained stories in English and Mandarin . TDT3 and TDT4 included English , Mandarin , and Arabic . Machine-translations into English for all non-English stories were provided , allowing participants to ignore issues of story translation. All TDT tasks have at their core a comparison of two text models. In story link detection , the simplest case , the comparison is between pairs of stories , to decide whether given pairs of stories are on the same topic or not . In topic tracking , the comparison is between a story and a topic , which is often represented as a centroid of story vectors , or as a language model covering several stories. Our focus in this research was to explore the best ways to compare stories and topics when stories are in multiple languages . We began with the hypothesis that if two stories originated in the same language , it would be best to compare them in that language, rather than translating them both into another language for comparison . This simple assertion , which we call the native language hypothesis , is easily tested in the TDT story link detection task. The picture gets more complex in a task like topic tracking , which begins with a small number of training stories (in English) to define each topic . New stories from a stream must be placed into these topics . The streamed stories originate in different languages, but are also available in English translation . The translations have been performed automatically by machine translation algorithms, and are inferior to manual translations . At the beginning", "label": ["topic models", "", "classification", "crosslingual", "native topic models", "similarity", "story link", "topic tracking", "native language hypothesis", "multilingual topic tracking", "multilingual", "arabic", "tdt", "machine translation"], "stemmed_label": ["topic model", "", "classif", "crosslingu", "nativ topic model", "similar", "stori link", "topic track", "nativ languag hypothesi", "multilingu topic track", "multilingu", "arab", "tdt", "machin translat"]}
{"doc": "Backup of websites is often not considered until after a catastrophic event has occurred to either the website or its webmaster . We introduce \"lazy preservation\" digital preservation performed as a result of the normal operation of web crawlers and caches . Lazy preservation is especially suitable for third parties; for example , a teacher reconstructing a missing website used in previous classes . We evaluate the effectiveness of lazy preservation by reconstructing 24 websites of varying sizes and composition using Warrick , a web-repository crawler . Because of varying levels of completeness in any one repository , our reconstructions sampled from four different web repositories: Google (44%) , MSN (30%) , Internet Archive (19%) and Yahoo (7%) . We also measured the time required for web resources to be discovered and cached (10-103 days) as well as how long they remained in cache after deletion (7-61 days) . INTRODUCTION \"My old web hosting company lost my site in its entirety (duh!) when a hard drive died on them. Needless to say that I was peeved , but I do notice that it is available to browse on the wayback machine.. . Does anyone have any ideas if I can download my full site?\" - A request for help at archive.org 25 Websites may be lost for a number of reasons: hard drive crashes , file system failures , viruses , hacking , etc . A lost website may be restored if care was taken to create a backup beforehand , but sometimes webmasters are negligent in backing up their websites , and in cases such as fire , flooding , or death of the website owner , backups are frequently unavailable . In these cases , webmasters and third parties may turn to the Internet Archive (IA) \"Wayback Machine\" for help. According to a representative from IA , they have performed over 200 website recoveries in the past year for various individuals . Although IA is often helpful , it is strictly a best-effort approach that performs sporadic , incomplete and slow crawls of the Web (IA is at least 6 months out-of-date 16 ). Another source of missing web content is in the caches of search engines (SEs) like Google , MSN and Yahoo that scour the Web looking for content to index . Unfortunately , the SEs do not preserve canonical copies of all the web resources they cache , and it is assumed that the SEs do not keep web pages long after they have been removed from a web server. We define lazy preservation as the collective digital preservation performed by web archives and search engines on behalf of the Web at large . It exists as a preservation service on top of distributed , incomplete , and potentially unreliable web repositories . Lazy preservation requires no individual effort or cost for Web publishers , but it also provides no quality of service guarantees . We explore the effectiveness of lazy preservation by downloading 24 websites of various sizes and subject matter and reconstructing them using a", "label": ["search engines (ses)", "cached resources", "web repositories", "recovery", "reconstruction", "crawling", "caching", "lazy preservation", "search engine", "digital preservation"], "stemmed_label": ["search engin (ses)", "cach resourc", "web repositori", "recoveri", "reconstruct", "crawl", "cach", "lazi preserv", "search engin", "digit preserv"]}
{"doc": "This paper considers the problem of using Support Vector Machines (SVMs) to learn concepts from large scale imbalanced data sets . The objective of this paper is twofold . Firstly , we investigate the effects of large scale and imbalance on SVMs . We highlight the role of linear non-separability in this problem . Secondly , we develop a both practical and theoretical guaranteed meta-algorithm to handle the trouble of scale and imbalance . The approach is named Support Cluster Machines (SCMs) . It incorporates the informative and the representative under-sampling mechanisms to speedup the training procedure . The SCMs differs from the previous similar ideas in two ways , (a) the theoretical foundation has been provided , and (b) the clustering is performed in the feature space rather than in the input space . The theoretical analysis not only provides justification , but also guides the technical choices of the proposed approach . Finally , experiments on both the synthetic and the TRECVID data are carried out . The results support the previous analysis and show that the SCMs are efficient and effective while dealing with large scale imbalanced data sets . INTRODUCTION In the context of concept modelling , this paper considers the problem of how to make full use of the large scale annotated data sets . In particular , we study the behaviors of Support Vector Machines (SVMs) on large scale imbalanced data sets , not only because its solid theoretical foundations but also for its empirical success in various applications. 1.1 Motivation Bridging the semantic gap has been becoming the most challenging problem of Multimedia Information Retrieval (MIR) . Currently , there are mainly two types of methods to bridge the gap 8 . The first one is relevance feedback which attempts to capture the user's precise needs through iterative feedback and query refinement . Another promising direction is concept modelling . As noted by Hauptmann 14 , this splits the semantic gap between low level features and user information needs into two , hopefully smaller gaps: (a) mapping the low-level features into the intermediate semantic concepts and (b) mapping these concepts into user needs . The automated image annotation methods for CBIR and the high level feature extraction methods in CBVR are all the efforts to model the first mapping . Of these methods, supervised learning is one of the most successful ones . An early difficulty of supervised learning is the lack of annotated training data . Currently , however , it seems no longer a problem . This is due to both the techniques developed to leverage surrounding texts of web images and the large scale collaborative annotation . Actually , there is an underway effort named Large Scale Concept Ontology for Multimedia Understanding (LSCOM) , which intends to annotate 1000 concepts in broadcast news video 13 . The initial fruits of this effort have been harvested in the practice of TRECVID hosted by National Institute of Standards and Technology (NIST) 1 . In TRECVID 2005 , 39 concepts are annotated by multiple participants", "label": ["support vector machines", "concept modelling", "concept modelling", "imbalance", "support vector machines (svms)", "large scale", "clustering", "imbalanced data", "kernel k-means", "support cluster machines (scms)", "trecvid", "meta-algorithm", "large scale data", "shrinking techniques", "clusters", "kernel k-means"], "stemmed_label": ["support vector machin", "concept model", "concept model", "imbal", "support vector machin (svms)", "larg scale", "cluster", "imbalanc data", "kernel k-mean", "support cluster machin (scms)", "trecvid", "meta-algorithm", "larg scale data", "shrink techniqu", "cluster", "kernel k-mean"]}
{"doc": "This paper studies the problem of automatic acquisition of the query languages supported by a Web information resource . We describe a system that automatically probes the search interface of a resource with a set of test queries and analyses the returned pages to recognize supported query operators . The automatic acquisition assumes the availability of the number of matches the resource returns for a submitted query . The match numbers are used to train a learning system and to generate classification rules that recognize the query operators supported by a provider and their syntactic encodings . These classification rules are employed during the automatic probing of new providers to determine query operators they support . We report on results of experiments with a set of real Web resources . INTRODUCTION Searching for relevant information is a primary activity on the Web. Often , people search for information using general-purpose search engines , such as Google or Yahoo!, which collect and index billions of Web pages . However, there exists an important part of the Web that remains unavailable for centralized indexing . This so-called \"hidden\" part of the Web includes the content of local databases and document collections accessible through search interfaces offered by various small- and middle-sized Web sites , including company sites , university sites , media sites , etc . According to the study conducted by BrightPlanet in 2000 6 , the size of the Hidden Web is about 400 to 550 times larger than the commonly defined (or \"Visible\") World Wide Web . This surprising discovery has fed new research on collecting and organizing the Hidden Web resources 1 , 2 , 15 , 17 , 19 . Commercial approaches to the Hidden Web are usually in the shape of Yahoo!-like directories which organize local sites belonging to specific domains . Some important examples of such directories are InvisibleWeb 1 and BrightPlanet 2 whose gateway site , CompletePlanet 3 , is a directory as well as a meta-search engine . For each database incorporated into its search , the meta-search engine is provided with a manually written \"wrapper\" , a software component that specifies how to submit queries and extract query answers embedded into HTML-formatted result pages. Similar to the Visible Web , search resources on the Hidden Web are highly heterogeneous . In particular , they use different document retrieval models , such as Boolean or vector-space models . They allow different operators for the query formulation and , moreover , the syntax of supported operators can vary from one site to another . Conventionally, query languages are determined manually; reading the help pages associated with a given search interface , probing the interface with sample queries and checking the result pages is often the method of choice. The manual acquisition of Web search interfaces has important drawbacks . First , the manual approach is hardly scalable to thousands of search resources that compose the Hidden Web . Second , the manual testing of Web resources with probe queries is often error-prone due to the inability", "label": ["query operators", "automatic acquisition", "learning", "hidden web", "search interface", "web resources", "machine learning", "search engine", "query languages", "hidden web", "web interfaces"], "stemmed_label": ["queri oper", "automat acquisit", "learn", "hidden web", "search interfac", "web resourc", "machin learn", "search engin", "queri languag", "hidden web", "web interfac"]}
{"doc": "Clustering algorithms typically operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed (dis)similarity measure between the data points in feature space . This makes the type of clusters identified highly dependent on the assumed similarity measure . Building on recent work in this area , we formally define a class of spatially varying dissimilarity measures and propose algorithms to learn the dissimilarity measure automatically from the data . The idea is to identify clusters that are compact with respect to the unknown spatially varying dissimilarity measure . Our experiments show that the proposed algorithms are more stable and achieve better accuracy on various textual data sets when compared with similar algorithms proposed in the literature . INTRODUCTION Clustering plays a major role in data mining as a tool to discover structure in data . Object clustering algorithms operate on a feature vector representation of the data and find clusters that are compact with respect to an assumed (dis)similarity measure between the data points in feature space . As a consequence , the nature of clusters identified by a clustering algorithm is highly dependent on the assumed similarity measure . The most commonly used dissimilarity measure , namely the Euclidean metric , assumes that the dissimilarity measure is isotropic and spatially invariant , and it is effective only when the clusters are roughly spherical and all of them have approximately the same size , which is rarely the case in practice 8 . The problem of finding non-spherical clusters is often addressed by utilizing a feature weighting technique . These techniques discover a single set of weights such that relevant features are given more importance than irrelevant features . However , in practice , each cluster may have a different set of relevant features . We consider Spatially Varying Dissimilarity (SVaD) measures to address this problem. Diday et . al . 4 proposed the adaptive distance dynamic clusters (ADDC) algorithm in this vain . A fuzzified version of ADDC , popularly known as the Gustafson-Kessel (GK) algorithm 7 uses a dynamically updated covariance matrix so that each cluster can have its own norm matrix . These algorithms can deal with hyperelliposoidal clusters of various sizes and orientations . The EM algorithm 2 with Gaussian probability distributions can also be used to achieve similar results . However , the above algorithms are computationally expensive for high-dimensional data since they invert covariance matrices in every iteration . Moreover , matrix inversion can be unstable when the data is sparse in relation to the dimensionality. One possible solution to the problems of high computation and instability arising out of using covariance matrices is to force the matrices to be diagonal , which amounts to weighting each feature differently in different clusters . While this restricts the dissimilarity measures to have axis parallel isometry , the weights also provide a simple interpretation of the clusters in terms of relevant features , which is important in knowledge discovery . Examples of such algorithms are SCAD and Fuzzy-SKWIC", "label": ["clustering", "feature weighting", "spatially varying dissimilarity (svad)", "learning dissimilarity measures", "clustering", "dissimilarity measure"], "stemmed_label": ["cluster", "featur weight", "spatial vari dissimilar (svad)", "learn dissimilar measur", "cluster", "dissimilar measur"]}
{"doc": "Kernel machines have been shown as the state-of-the-art learning techniques for classification . In this paper , we propose a novel general framework of learning the Unified Kernel Machines (UKM) from both labeled and unlabeled data . Our proposed framework integrates supervised learning , semi-supervised kernel learning , and active learning in a unified solution . In the suggested framework , we particularly focus our attention on designing a new semi-supervised kernel learning method , i.e. , Spectral Kernel Learning (SKL) , which is built on the principles of kernel target alignment and unsupervised kernel design . Our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved . Empirical results have shown that our method is more effective and robust to learn the semi-supervised kernels than traditional approaches . Based on the framework , we present a specific paradigm of unified kernel machines with respect to Kernel Logistic Regresions (KLR) , i.e. , Unified Kernel Logistic Regression (UKLR) . We evaluate our proposed UKLR classification scheme in comparison with traditional solutions . The promising results show that our proposed UKLR paradigm is more effective than the traditional classification approaches . INTRODUCTION Classification is a core data mining technique and has been actively studied in the past decades . In general , the goal of classification is to assign unlabeled testing examples with a set of predefined categories . Traditional classification methods are usually conducted in a supervised learning way , in which only labeled data are used to train a predefined classification model . In literature , a variety of statistical models have been proposed for classification in the machine learning and data mining communities . One of the most popular and successful methodologies is the kernel-machine techniques , such as Support Vector Machines (SVM) 25 and Kernel Logistic Regressions (KLR) 29 . Like other early work for classification , traditional kernel-machine methods are usually performed in the supervised learning way , which consider only the labeled data in the training phase. It is obvious that a good classification model should take advantages on not only the labeled data , but also the unlabeled data when they are available . Learning on both labeled and unlabeled data has become an important research topic in recent years . One way to exploit the unlabled data is to use active learning 7 . The goal of active learning is to choose the most informative example from the unlabeled data for manual labeling . In the past years , active learning has been studied for many classification tasks 16 . Another emerging popular technique to exploit unlabeled data is semi-supervised learning 5 , which has attracted a surge of research attention recently 30 . A variety of machine-learning techniques have been proposed for semi-supervised learning , in which the most well-known approaches are based on the graph Laplacians methodology 28 , 31 , 5 . While promising results have been popularly reported in this research topic , there is so far few comprehensive semi-supervised learning scheme applicable for large-scale classification", "label": ["active learning", "data mining", "classification", "unified kernel machine(ukm)", "kernel machines", "spectral kernel learning (skl)", "kernel logistic regressions", "supervised learning", "supervised learning", "semi-supervised learning", "active learning", "classification", "unsuper-vised kernel design", "framework", "spectral kernel learning", "semi-supervised kernel learning"], "stemmed_label": ["activ learn", "data mine", "classif", "unifi kernel machine(ukm)", "kernel machin", "spectral kernel learn (skl)", "kernel logist regress", "supervis learn", "supervis learn", "semi-supervis learn", "activ learn", "classif", "unsuper-vis kernel design", "framework", "spectral kernel learn", "semi-supervis kernel learn"]}
{"doc": "A physically compact , low cost , high performance 3D graphics accelerator is presented . It supports shaded rendering of triangles and antialiased lines into a double-buffered 24-bit true color frame buffer with a 24-bit Z-buffer . Nearly the only chips used besides standard memory parts are 11 ASICs (of four types) . Special geometry data reformatting hardware on one ASIC greatly speeds and simplifies the data input pipeline . Floating-point performance is enhanced by another ASIC: a custom graphics microprocessor , with specialized graphics instructions and features . Screen primitive rasterization is carried out in parallel by five drawing ASICs , employing a new partitioning of the back-end rendering task . For typical rendering cases , the only system performance bottleneck is that intrinsically imposed by VRAM . INTRODUCTION To expand the role of 3D graphics in the mainstream computer industry , cost effective , physically small , usable performance 3D shaded graphics architectures must be developed . For such systems, new features and sheer performance at any price can no longer be the driving force behind the architecture; instead , the focus must be on affordable desktop systems. The historical approach to achieving low cost in 3D graphics systems has been to compromise both performance and image quality. But now , falling memory component prices are bringing nearly ideal frame buffers into the price range of the volume market: double buffered 24-bit color with a 24-bit Z-buffer . The challenge is to drive these memory chips at their maximum rate with a minimum of supporting rendering chips , keeping the total system cost and physical size to an absolute minimum . To achieve this , graphics architectures must be repartitioned to reduce chip count and internal bus sizes, while still supporting existing 2D and 3D functionality. This paper describes a new 3D graphics system , Leo , designed to these philosophies . For typical cases , Leo's only performance limit is that intrinsically imposed by VRAM . This was achieved by a combination of new architectural techniques and advances in VLSI technology . The result is a system without performance or image quality compromises , at an affordable cost and small physical size. The Leo board set is about the size of one and a half paperback novels ; the complete workstation is slightly larger than two copies of Foley and Van Dam 7 . Leo supports both the traditional requirements of the 2D X window system and the needs of 3D rendering: shaded triangles , antialiased vectors , etc. ARCHITECTURAL ALTERNATIVES A generic pipeline for 3D shaded graphics is shown in Figure 1 . ( 7 Chapter 18 is a good overview of 3D graphics hardware pipeline issues .) This pipeline is truly generic , as at the top level nearly every commercial 3D graphics accelerator fits this abstraction . Where individual systems differ is in the partitioning of this rendering pipeline , especially in how they employ parallelism . Two major areas have been subject to separate optimization: the floating-point intensive initial stages of processing up to ,", "label": ["input processing", "3d graphics hardware", "parallel algorithms", "video output", "general graphics processing", "parallel graphics algorithms", "small physical size", "geometry data", "3d shaded graphics", "rendering", "screen space rendering", "antialiased lines", "floating-point microprocessors", "low cost", "floating point processing", "gouraud shading"], "stemmed_label": ["input process", "3d graphic hardwar", "parallel algorithm", "video output", "gener graphic process", "parallel graphic algorithm", "small physic size", "geometri data", "3d shade graphic", "render", "screen space render", "antialias line", "floating-point microprocessor", "low cost", "float point process", "gouraud shade"]}
{"doc": "Data dissemination through wireless channels for broadcasting information to consumers is becoming quite common . Many dissemination schemes have been proposed but most of them push data to wireless channels for general consumption . Push based broadcast 1 is essentially asymmetric , i.e. , the volume of data being higher from the server to the users than from the users back to the server . Push based scheme requires some indexing which indicates when the data will be broadcast and its position in the broadcast . Access latency and tuning time are the two main parameters which may be used to evaluate an indexing scheme . Two of the important indexing schemes proposed earlier were tree based and the exponential indexing schemes . None of these schemes were able to address the requirements of location dependent data (LDD) which is highly desirable feature of data dissemination . In this paper , we discuss the broadcast of LDD in our project DAta in Your Space (DAYS) , and propose a scheme for indexing LDD . We argue that this scheme , when applied to LDD , significantly improves performance in terms of tuning time over the above mentioned schemes . We prove our argument with the help of simulation results . INTRODUCTION Wireless data dissemination is an economical and efficient way to make desired data available to a large number of mobile or static users . The mode of data transfer is essentially asymmetric, that is , the capacity of the transfer of data (downstream communication) from the server to the client (mobile user) is significantly larger than the client or mobile user to the server (upstream communication) . The effectiveness of a data dissemination system is judged by its ability to provide user the required data at anywhere and at anytime . One of the best ways to accomplish this is through the dissemination of highly personalized Location Based Services (LBS) which allows users to access personalized location dependent data . An example would be someone using their mobile device to search for a vegetarian restaurant . The LBS application would interact with other location technology components or use the mobile user's input to determine the user's location and download the information about the restaurants in proximity to the user by tuning into the wireless channel which is disseminating LDD. We see a limited deployment of LBS by some service providers . But there are every indications that with time some of the complex technical problems such as uniform location framework , calculating and tracking locations in all types of places , positioning in various environments , innovative location applications , etc. , will be resolved and LBS will become a common facility and will help to improve market productivity and customer comfort . In our project called DAYS , we use wireless data broadcast mechanism to push LDD to users and mobile users monitor and tune the channel to find and download the required data . A simple broadcast , however , is likely to cause significant performance degradation in the", "label": ["containment", "indexing scheme", "access efficiency", "indexing", "wireless data broadcast", "mapping function", "location based services", "wireless", "energy conservation", "location dependent data", "broadcast", "push based architecture", "data dissemination", "data staging"], "stemmed_label": ["contain", "index scheme", "access effici", "index", "wireless data broadcast", "map function", "locat base servic", "wireless", "energi conserv", "locat depend data", "broadcast", "push base architectur", "data dissemin", "data stage"]}
{"doc": "Bagging frequently improves the predictive performance of a model . An online version has recently been introduced , which attempts to gain the benefits of an online algorithm while approximating regular bagging . However , regular online bagging is an approximation to its batch counterpart and so is not lossless with respect to the bagging operation . By operating under the Bayesian paradigm , we introduce an online Bayesian version of bagging which is exactly equivalent to the batch Bayesian version , and thus when combined with a lossless learning algorithm gives a completely lossless online bagging algorithm . We also note that the Bayesian formulation resolves a theoretical problem with bagging , produces less variability in its estimates , and can improve predictive performance for smaller data sets . Introduction In a typical prediction problem , there is a trade-off between bias and variance , in that after a certain amount of fitting , any increase in the precision of the fit will cause an increase in the prediction variance on future observations . Similarly , any reduction in the prediction variance causes an increase in the expected bias for future predictions . Breiman (1996) introduced bagging as a method of reducing the prediction variance without affecting the prediction bias . As a result , predictive performance can be significantly improved. Bagging , short for \"Bootstrap AGGregatING\" , is an ensemble learning method . Instead of making predictions from a single model fit on the observed data , bootstrap samples are taken of the data , the model is fit on each sample , and the predictions are averaged over all of the fitted models to get the bagged prediction . Breiman (1996) explains that bagging works well for unstable modeling procedures , i.e . those for which the conclusions are sensitive to small changes in the data . He also gives a theoretical explanation of how bagging works , demonstrating the reduction in mean-squared prediction error for unstable c 2004 Herbert K . H . Lee and Merlise A . Clyde. Lee and Clyde procedures . Breiman (1994) demonstrated that tree models , among others , are empirically unstable. Online bagging (Oza and Russell , 2001) was developed to implement bagging sequentially , as the observations appear , rather than in batch once all of the observations have arrived . It uses an asymptotic approximation to mimic the results of regular batch bagging, and as such it is not a lossless algorithm . Online algorithms have many uses in modern computing . By updating sequentially , the update for a new observation is relatively quick compared to re-fitting the entire database , making real-time calculations more feasible. Such algorithms are also advantageous for extremely large data sets where reading through the data just once is time-consuming , so batch algorithms which would require multiple passes through the data would be infeasible. In this paper , we consider a Bayesian version of bagging (Clyde and Lee , 2001) based on the Bayesian bootstrap (Rubin , 1981) . This overcomes a technical", "label": ["classification", "dirichlet distribution", "online bagging", "bootstrap", "classification tree", "bayesian bootstrap", "mean-squared prediction error", "bayesian bagging", "bagging", "lossless learning algorithm"], "stemmed_label": ["classif", "dirichlet distribut", "onlin bag", "bootstrap", "classif tree", "bayesian bootstrap", "mean-squar predict error", "bayesian bag", "bag", "lossless learn algorithm"]}
{"doc": "Table is a commonly used presentation scheme , especially for describing relational information . However , table understanding remains an open problem . In this paper , we consider the problem of table detection in web documents . Its potential applications include web mining , knowledge management , and web content summarization and delivery to narrow-bandwidth devices . We describe a machine learning based approach to classify each given table entity as either genuine or non-genuine . Various features re ecting the layout as well as content characteristics of tables are studied . In order to facilitate the training and evaluation of our table classi er , we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database . The database consists of 1,393 HTML les collected from hundreds of di erent web sites and contains 11,477 leaf &lt;TABLE&gt; elements , out of which 1,740 are genuine tables . Experiments were conducted using the cross validation method and an F-measure of 95 : 89% was achieved . INTRODUCTION The increasing ubiquity of the Internet has brought about a constantly increasing amount of online publications . As a compact and e cient way to present relational information , tables are used frequently in web documents . Since tables are inherently concise as well as information rich , the automatic understanding of tables has many applications including knowledge management , information retrieval , web Copyright is held by the author/owner(s). WWW2002 , May 711 , 2002 , Honolulu , Hawaii , USA. ACM 1-58113-449-5/02/0005. mining , summarization , and content delivery to mobile devices . The processes of table understanding in web documents include table detection , functional and structural analysis and nally table interpretation 6 . In this paper, we concentrate on the problem of table detection . The web provides users with great possibilities to use their own style of communication and expressions . In particular , people use the &lt;TABLE&gt; tag not only for relational information display but also to create any type of multiple-column layout to facilitate easy viewing , thus the presence of the &lt;TABLE&gt; tag does not necessarily indicate the presence of a relational table . In this paper , we de ne genuine tables to be document entities where a two dimensional grid is semantically signi cant in conveying the logical relations among the cells 10 . Conversely, Non-genuine tables are document entities where &lt;TABLE&gt; tags are used as a mechanism for grouping contents into clusters for easy viewing only . Figure 1 gives a few examples of genuine and non-genuine tables . While genuine tables in web documents could also be created without the use of &lt;TABLE&gt; tags at all , we do not consider such cases in this article as they seem very rare from our experience . Thus , in this study, Table detection refers to the technique which classi es a document entity enclosed by the &lt;TABLE&gt;&lt;/TABLE&gt; tags as genuine or non-genuine tables. Several researchers have reported their work on web table detection", "label": ["table detection", "table ground truthing protocol", "layout analysis", "classifers", "word group", "presentation", "information retrieval", "algorithms", "support vector machine", "classifcation schemes", "machine learning", "table detection", "layout", "machine learning based approach", "content type", "decision tree", "html document"], "stemmed_label": ["tabl detect", "tabl ground truth protocol", "layout analysi", "classif", "word group", "present", "inform retriev", "algorithm", "support vector machin", "classifc scheme", "machin learn", "tabl detect", "layout", "machin learn base approach", "content type", "decis tree", "html document"]}
{"doc": "For hardware accelerated rendering , photon mapping is especially useful for simulating caustic lighting effects on non-Lambertian surfaces . However , an efficient hardware algorithm for the computation of the k nearest neighbours to a sample point is required . Existing algorithms are often based on recursive spatial subdivision techniques , such as kd-trees . However , hardware implementation of a tree-based algorithm would have a high latency , or would require a large cache to avoid this latency on average . We present a neighbourhood-preserving hashing algorithm that is low-latency and has sub-linear access time . This algorithm is more amenable to fine-scale parallelism than tree-based recursive spatial subdivision , and maps well onto coherent block-oriented pipelined memory access . These properties make the algorithm suitable for implementation using future programmable fragment shaders with only one stage of dependent texturing . Introduction Photon mapping , as described by Jensen , is a technique for reconstructing the incoming light field at surfaces everywhere in a scene from sparse samples generated by forward light path tracing . In conjunction with path tracing , photon mapping can be used to accelerate the computation of both diffuse and specular global illumination . It is most effective for specular or glossy reflectance effects , such as caustics . The benefits of migrating photo-realistic rendering techniques towards a real-time , hardware-assisted implementation are obvious . Recent work has shown that it is possible to implement complex algorithms , such as ray-tracing, using the programmable features of general-purpose hardware accelerators and/or specialised hardware . We are interested in hardware support for photon mapping: specifically , the application of photon maps to the direct visualisation of caustics on non-Lambertian surfaces , since diffuse global illumination effects are probably best handled in a real-time renderer using alternative techniques such as irradiance . Central to photon mapping is the search for the set of photons nearest to the point being shaded . This is part of the interpolation step that joins light paths propagated from light sources with rays traced from the camera during rendering, and it is but one application of the well-studied k nearest neighbours (kNN) problem. Jensen uses the kd-tree , data structure to find these nearest photons . However , solving the kNN problem via kd-trees requires a search that traverses the tree . Even if the tree is stored as a heap , traversal still requires random-order memory access and memory to store a stack . More importantly, a search-path pruning algorithm , based on the data already examined , is required to avoid accessing all data in the tree. This introduces serial dependencies between one memory lookup and the next . Consequently , a hardware implementation of a kd-tree-based kNN solution would either have high latency , or would require a large cache to avoid such latency. In either case a custom hardware implementation would be required . These properties motivated us to look at alternatives to tree search. Since photon mapping is already making an approximation by using kNN interpolation , we conjectured that", "label": ["photon mapping", "block hashing (bh)", "hashing techniques", "aknn", "knn", "accelerator"], "stemmed_label": ["photon map", "block hash (bh)", "hash techniqu", "aknn", "knn", "acceler"]}
{"doc": "In this paper we study the problem of assigning unit-size tasks to related machines when only limited online information is provided to each task . This is a general framework whose special cases are the classical multiple-choice games for the assignment of unit-size tasks to identical machines . The latter case was the subject of intensive research for the last decade . The problem is intriguing in the sense that the natural extensions of the greedy oblivious schedulers , which are known to achieve near-optimal performance in the case of identical machines , are proved to perform quite poorly in the case of the related machines . In this work we present a rather surprising lower bound stating that any oblivious scheduler that assigns an arbitrary number of tasks to n related machines would need log n polls of machine loads per task , in order to achieve a constant competitive ratio versus the optimum offline assignment of the same input sequence to these machines . On the other hand , we prove that the missing information for an oblivious scheduler to perform almost optimally , is the amount of tasks to be inserted into the system . In particular , we provide an oblivious scheduler that only uses O(loglog n) polls , along with the additional information of the size of the input sequence , in order to achieve a constant competitive ratio vs . the optimum offline assignment . The philosophy of this scheduler is based on an interesting exploitation of the slowfit concept ( 1 , 5 , 3 ; for a survey see 6 , 9 , 16 ) for the assignment of the tasks to the related machines despite the restrictions on the provided online information , in combination with a layered induction argument for bounding the tails of the number of tasks passing from slower to faster machines . We finally use this oblivious scheduler as the core of an adaptive scheduler that does not demand the knowledge of the input sequence and yet achieves almost the same performance . INTRODUCTION The problem of the Online Assignment of Tasks to Related Machines is defined as follows: there are n machines possibly of different speeds , that are determined by a speed vector c , and an input sequence of m independent tasks to be assigned to these machines. The tasks arrive sequentially , along with their associated weights (positive reals) and have to be assigned immediately and uniquely to the machines of the system . The size of the input sequence as well as the weights of the tasks are determined by an oblivious adversary (denoted here by ADV) . Each task has to be assigned upon its arrival to one of the machines , using the following information: (possibly a portion of) the online information of current status of the system, the offline information of the machine speeds , and its own weight. The tasks are considered to be of infinite duration (permanent tasks) and no preemption is allowed . The cost of an online", "label": ["oblivious scheduler", "hops", "related machines", "limited information", "lower bounds", "online information", "scheduling", "online load balancing", "input sequence", "unit-size task", "related machines"], "stemmed_label": ["oblivi schedul", "hop", "relat machin", "limit inform", "lower bound", "onlin inform", "schedul", "onlin load balanc", "input sequenc", "unit-s task", "relat machin"]}
{"doc": "This paper describes ongoing research into the application of machine learning techniques for improving access to governmental information in complex digital libraries . Under the auspices of the GovStat Project , our goal is to identify a small number of semantically valid concepts that adequately spans the intellectual domain of a collection . The goal of this discovery is twofold . First we desire a practical aid for information architects . Second , automatically derived document-concept relationships are a necessary precondition for real-world deployment of many dynamic interfaces . The current study compares concept learning strategies based on three document representations: keywords , titles , and full-text . In statistical and user-based studies , human-created keywords provide significant improvements in concept learning over both title-only and full-text representations . Categories and Subject Descriptors INTRODUCTION The GovStat Project is a joint effort of the University of North Carolina Interaction Design Lab and the University of Maryland Human-Computer Interaction Lab 1 . Citing end-user difficulty in finding governmental information (especially statistical data) online , the project seeks to create an integrated model of user access to US government statistical information that is rooted in realistic data models and innovative user interfaces . To enable such models and interfaces , we propose a data-driven approach , based on data mining and machine learning techniques . In particular , our work analyzes a particular digital library--the website of the Bureau of Labor Statistics 2 (BLS)--in efforts to discover a small number of linguistically meaningful concepts , or \"bins,\" that collectively summarize the semantic domain of the site. The project goal is to classify the site's web content according to these inferred concepts as an initial step towards data filtering via active user interfaces (cf. 13 ). Many digital libraries already make use of content classification, both explicitly and implicitly; they divide their resources manually by topical relation; they organize content into hi-erarchically oriented file systems . The goal of the present 1 http://www.ils.unc.edu/govstat 2 http://www.bls.gov 151 research is to develop another means of browsing the content of these collections . By analyzing the distribution of terms across documents , our goal is to supplement the agency's pre-existing information structures . Statistical learning technologies are appealing in this context insofar as they stand to define a data-driven--as opposed to an agency-driven-navigational structure for a site. Our approach combines supervised and unsupervised learning techniques . A pure document clustering 12 approach to such a large , diverse collection as BLS led to poor results in early tests 6 . But strictly supervised techniques 5 are inappropriate , too . Although BLS designers have defined high-level subject headings for their collections , as we discuss in Section 2 , this scheme is less than optimal . Thus we hope to learn an additional set of concepts by letting the data speak for themselves. The remainder of this paper describes the details of our concept discovery efforts and subsequent evaluation . In Section 2 we describe the previously existing , human-created conceptual structure of the BLS website . This section", "label": ["information architecture", "information architecture", "bls", "digital libraries", "document classification", "machine learning", "topic discovery", "document representation", "interface design", "clustering", "relational browser"], "stemmed_label": ["inform architectur", "inform architectur", "bl", "digit librari", "document classif", "machin learn", "topic discoveri", "document represent", "interfac design", "cluster", "relat browser"]}
{"doc": "The development of microarray technology has supplied a large volume of data to many fields . In particular , it has been applied to prediction and diagnosis of cancer , so that it expectedly helps us to exactly predict and diagnose cancer . To precisely classify cancer we have to select genes related to cancer because extracted genes from microarray have many noises . In this paper , we attempt to explore many features and classifiers using three benchmark datasets to systematically evaluate the performances of the feature selection methods and machine learning classifiers . Three benchmark datasets are Leukemia cancer dataset , Colon cancer dataset and Lymphoma cancer data set . Pearson's and Spearman's correlation coefficients , Euclidean distance , cosine coefficient , information gain , mutual information and signal to noise ratio have been used for feature selection . Multi-layer perceptron , k-nearest neighbour , support vector machine and structure adaptive selforganizing map have been used for classification . Also , we have combined the classifiers to improve the performance of classification . Experimental results show that the ensemble with several basis classifiers produces the best recognition rate on the benchmark dataset . Introduction The need to study whole genome such as Human Genomic Project (HGP) is recently increasing because fragmentary knowledge about life phenomenon with complex control functions of molecular-level is limited. DNA chips have been developed during that process because understanding the functions of genome sequences is essential at that time. The development of DNA microarray technology has been produced large amount of gene data and has made it easy to monitor the expression patterns of thousands of genes simultaneously under particular experimental environments and conditions (Harrington et al . 2000). Also , we can analyze the gene information very rapidly and precisely by managing them at one time (Eisen et al. 1999). Microarray technology has been applied to the field of accurate prediction and diagnosis of cancer and expected that it would help them . Especially accurate classification of cancer is very important issue for treatment of cancer. Many researchers have been studying many problems of cancer classification using gene expression profile data and attempting to propose the optimal classification technique to work out these problems (Dudoit et al . 2000, Ben-Dor et al . 2000) as shown in Table . Some produce better results than others , but there have been still no comprehensive work to compare the possible feature selection methods and classifiers . We need a thorough effort to give the evaluation of the possible methods to solve the problems of analyzing gene expression data. The gene expression data usually consist of huge number of genes , and the necessity of tools analysing them to get useful information gets radical . There is research that systematically analyzes the results of test using a variety of feature selection methods and classifiers for selecting informative genes to help classification of cancer and classifying cancer (Ryu et al . 2002) . However , the results were not verified enough because only one benchmark dataset was used .", "label": ["classification", "mlp", "sasom", "gene expression profile", "svm", "knn", "biological data mining", "ensemble classifier", "feature selection"], "stemmed_label": ["classif", "mlp", "sasom", "gene express profil", "svm", "knn", "biolog data mine", "ensembl classifi", "featur select"]}
{"doc": "Machine learning and data mining have found a multitude of successful applications in microarray analysis , with gene clustering and classification of tissue samples being widely cited examples . Low-level microarray analysis often associated with the pre-processing stage within the microarray life-cycle has increasingly become an area of active research , traditionally involving techniques from classical statistics . This paper explores opportunities for the application of machine learning and data mining methods to several important low-level microarray analysis problems: monitoring gene expression , transcript discovery , genotyping and resequencing . Relevant methods and ideas from the machine learning community include semi-supervised learning , learning from heterogeneous data , and incremental learning . INTRODUCTION DNA microarrays have revolutionized biological research over the short time since their inception 2; 27; 28; 29 . Although most widely used for parallel measurement of gene expression 27; 28 , microarrays are starting to find common application in other areas of genomics and transcriptomics, including genomic re-sequencing 30; 31 , genotyping 32; 33 , and transcript discovery 34 . Research labs armed with microarrays have been able to partake in a range of studies , including finding gene function 35; 36; 37 ; correcting mistaken database annotations 36; 7 ; performing linkage analyses; determining specific genes involved in biological pathways; identifying genes that are important at certain times of development (or that are turned on/off over a course of treatment); elucidating gene regulatory networks 13 ; diagnosing disease in tissue sam-Figure 1: The relationship between low-level and high-level microarray analysis. ples 38; 39; 40; 41 ; tioners' misdiagnoses 38 . The common thread among these high-level microarray analysis problems is that they answer sophisticated questions of direct biological interest to medical researchers (such as \"which genes are being co-expressed under treatment X?\") , where the raw data used are estimates of biologically meaningful parameters (such as the expression level estimates for thousands of genes). In contrast to these so-called high-level problems , low-level microarray analysis 19 is concerned with the preceding step in the microarray assay cycle (Figure 1) given raw data straight from a scanner which has no direct biological interpretation , clean and summarize this data to produce the biologically meaningful parameter estimates (such as expression level estimates) that are later used in high-level analyses . In low-level analysis , more consideration is generally given to the behavior of the underlying molecular biology , microarray technology , and experimental design than in high-level analysis . This makes generative methods readily applicable in low-level problems , facilitating the formulation of confidence SIGKDD Explorations. and even identifying medical practi statements such as p-values in gene expression calls . Hence, while high-level problems have been tackled with discriminative approaches , such as those found in machine learning and data mining , in addition to classical statistical methods , the low-level analysis community has traditionally called upon only the latter. In this paper we argue that low-level microarray analysis poses a number of interesting problems for the data mining and machine learning community , distinct to the traditional high-level microarray", "label": ["low-level analysis", "data mining", "transductive learning", "learning from heterogeneous data", "heterogeneous data", "semi-supervised learning", "incremental learning", "transcript discovery", "microarray", "gene expression estimation", "statistics", "genotyping", "low-level microarray analysis", "re-sequencing"], "stemmed_label": ["low-level analysi", "data mine", "transduct learn", "learn from heterogen data", "heterogen data", "semi-supervis learn", "increment learn", "transcript discoveri", "microarray", "gene express estim", "statist", "genotyp", "low-level microarray analysi", "re-sequenc"]}
{"doc": "Ada95 is an object-oriented programming language . Pack-ages are basic program units in Ada 95 to support OO programming , which allow the specification of groups of logically related entities . Thus , the cohesion of a package is mainly about how tightly the entities are encapsulated in the package . This paper discusses the relationships among these entities based on dependence analysis and presents the properties to obtain these dependencies . Based on these , the paper proposes an approach to measure the package cohesion , which satisfies the properties that a good measure should have . INTRODUCTION Cohesion is one of the most important software features during its development . It tells us the tightness among the components of a software module . The higher the cohesion of a module , the more understandable , modifiable and maintainable the module is . A software system should have high cohesion and low coupling. Researchers have developed several guidelines to measure cohesion of a module 1 , 3 , 4 . Since more and more applications are object-oriented , the approaches to measure cohesion of object-oriented (OO) programs have become an important research field. Generally , each object-oriented programming language provides facilities to support OO features , such as data abstraction, encapsulation and inheritance . Each object consists of a set of attributes to represent the states of objects and a set of operations on attributes . Thus , in OO environment , the cohesion is mainly about how tightly the attributes and operations are encapsulated. There are several approaches proposed in literature to measure OO program cohesion 2 , 5 , 6 , 7 , 11 , 12 . Most approaches are based on the interaction between operations and attributes . The cohesion is measured as the number of the interactions . Generally only the references from operations to attributes are considered. And few care about the interactions of attributes to attributes and operations to operations at the same time . This might lead to bias when measuring the cohesion of a class . For example , when designing the trigonometric function lib class , we might set a global variable to record the temporal result . The variable is referred in all the operations of the class . According to methods based on the interaction between operations and attributes 6 , 7 , the cohesion is the maximum 1 . In fact , there are no relations among the operations if the calls are not taken into account . In this view , its cohesion is 0 . The difference is caused by considering only the references from operations to attributes, while not considering the inter-operation relations. In our previous work , we have done some research in measuring OO program cohesion 10 , 13 , 14 . Our approach overcomes the limitations of previous class cohesion measures , which consider only one or two of the three facets . Since the OO mechanisms in different programming languages are different from each other, this paper applies our measure to Ada", "label": ["object-oriented", "ada95", "cohesion", "dependence", "measurement", "oo programming", "measure", "cohesion"], "stemmed_label": ["object-ori", "ada95", "cohes", "depend", "measur", "oo program", "measur", "cohes"]}
{"doc": "Public administrations of all over the world invest an enormous amount of resources in e-government . How the success of e-government can be measured is often not clear . E-government involves many aspects of public administration ranging from introducing new technology to business process (re-)engineering . The measurement of the effectiveness of e-government is a complicated endeavor . In this paper current practices of e-government measurement are evaluated . A number of limitations of current measurement instruments are identified . Measurement focuses predominantly on the front (primarily counting the number of services offered) and not on the back-office processes . Interpretation of measures is difficult as all existing measurement instruments lack a framework depicting the relationships between the indicators and the use of resources . The different measures may fit the aim of the owners of the e-governmental services , however , due to conflicting aims and priorities little agreement exists on a uniform set of measures , needed for comparison of e-government development . Traditional methods of measuring e-government impact and resource usage fall short of the richness of data required for the effective evaluation of e-government strategies . INTRODUCTION Public institutions as well as business organizations use the Internet to deliver a wide range of information and services at an increasing level of sophistication 24 . However , Web sites and related business processes and information systems are so complex that it is difficult for governments to determine adequate measures for evaluating the efficiency and effectiveness of the spending of their public money . Moreover only measuring the front of public websites is a too narrow view on e-government . E-government involves the collaboration and communication between stakeholders and integration of cross-agency business processes. An important aim of having a well-funded theory on measuring e-government is to allow comparison or benchmarking . By examining the results of such benchmarks we might be able to distinct good from bad practices and to give directives to designers of e-governmental services . Moreover is should help to identify how effective public money is spend . Thus evaluating the relationship between results and resources used. Comparison can have different goals . Selecting between options is just one of them . Principally we can distinct three types of comparison: 1) comparison of alternatives (e.g . to make a choice between alternative solutions) , 2) vertical comparison over time (e.g . in order to measure improvement of versions) and 3) horizontal comparison (e.g . benchmarking different solutions). Whatever type of comparison we choose , we can only compare if we have a set of preferred outcomes . Many measurement instruments currently applied are not described in such way that the preferences underlying the instrument have been made explicitly . In this research we describe different measurement instruments used and we will discuss their strengths and weaknesses. The goal of this paper is to evaluate how current measurement instruments the impact of e-government . The combined research methodology of literature research and case study was chosen to answer the goal of this research . Case study research", "label": ["e-government", "law", "benchmark", "interoperability", "evaluation", "measurement", "architectures", "e-government", "business process", "public administration"], "stemmed_label": ["e-govern", "law", "benchmark", "interoper", "evalu", "measur", "architectur", "e-govern", "busi process", "public administr"]}
{"doc": "We give the first on-line poly-logarithmic competitve algorithm for minimizing average flow time with preemption on related machines , i.e. , when machines can have different speeds . This also yields the first poly-logarithmic polynomial time approximation algorithm for this problem . More specifically , we give an O(log P log S)-competitive algorithm , where P is the ratio of the biggest and the smallest processing time of a job , and S is the ratio of the highest and the smallest speed of a machine . Our algorithm also has the nice property that it is non-migratory . The scheduling algorithm is based on the concept of making jobs wait for a long enough time before scheduling them on slow machines . INTRODUCTION We consider the problem of scheduling jobs that arrive over time in multiprocessor environments . This is a fundamental scheduling problem and has many applications , e.g., servicing requests in web servers . The goal of a scheduling Work done as part of the \"Approximation Algorithms\" partner group of MPI-Informatik , Germany. Supported by an IBM faculty development award and a travel grant from the Max Plank Society. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. STOC'06 , May 2123 , 2006 , Seattle , Washington , USA. Copyright 2006 ACM 1-59593-134-1/06/0005 ... $ 5.00. algorithm is to process jobs on the machines so that some measure of performance is optimized . Perhaps the most natural measure is the average flow time of the jobs . Flow time of a job is defined as the difference of its completion time and its release time , i.e. , the time it spends in the system. This problem has received considerable attention in the recent past 1 , 2 , 10 . All of these works make the assumption that the machines are identical , i.e. , they have the same speed . But it is very natural to expect that in a heterogenous processing environment different machines will have different processing power , and hence different speeds . In this paper , we consider the problem of scheduling jobs on machines with different speeds , which is also referred to as related machines in the scheduling literature . We allow for jobs to be preempted . Indeed , the problem turns out to be intractable if we do not allow preemption . Kellerer et . al . 9 showed that the problem of minimizing average flow time without preemption has no online algorithm with o(n) competitive ratio even on a single machine . They also showed that it is hard to get a polynomial time O(n 1/2 )-approximation algorithm for this problem", "label": ["non-migratory algorithm", "flow-time", "average flow time", "approximation algorithms", "processing time", "competitive ratio", "related machines", "poly-logarithmic factor", "preemption", "multiprocessor environment", "scheduling", "scheduling"], "stemmed_label": ["non-migratori algorithm", "flow-tim", "averag flow time", "approxim algorithm", "process time", "competit ratio", "relat machin", "poly-logarithm factor", "preemption", "multiprocessor environ", "schedul", "schedul"]}
{"doc": "In this paper , we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communications . A personal profile , called CommunityNet , is established for each individual based on a novel algorithm incorporating contact , content , and time information simultaneously . It can be used for personal social capital management . Clusters of CommunityNets provide a view of informal networks for organization management . Our new algorithm is developed based on the combination of dynamic algorithms in the social network field and the semantic content classification methods in the natural language processing and machine learning literatures . We tested CommunityNets on the Enron Email corpus and report experimental results including filtering , prediction , and recommendation capabilities . We show that the personal behavior and intention are somewhat predictable based on these models . For instance , &quot;to whom a person is going to send a specific email&quot; can be predicted by one's personal social network and content analysis . Experimental results show the prediction accuracy of the proposed adaptive algorithm is 58% better than the social network-based predictions , and is 75% better than an aggregated model based on Latent Dirichlet Allocation with social network enhancement . Two online demo systems we developed that allow interactive exploration of CommunityNet are also discussed . INTRODUCTION Working in the information age , the most important is not what you know , but who you know 1 . A social network , the graph of relationships and interactions within a group of individuals , plays a fundamental role as a medium for the spread of information , ideas , and influence . At the organizational level, personal social networks are activated for recruitment , partnering, and information access . At the individual level , people exploit their networks to advance careers and gather information. Informal network within formal organizations is a major , but hard to acquire , factor affecting companies' performance. Krackhardt 2 showed that companies with strong informal networks perform five or six times better than those with weak networks , especially on the long-term performance . Friend and advice networks drive enterprise operations in a way that , if the real organization structure does not match the informal networks, then a company tends to fail 3 . Since Max Weber first studied modern bureaucracy structures in the 1920s , decades of related social scientific researches have been mainly relying on questionnaires and interviews to understand individuals' thoughts and behaviors for sensing informal networks . However , data collection is time consuming and seldom provides timely, continuous , and dynamic information . This is usually the biggest hurdle in social studies. Personal Social Network (PSN) could provide an organizing principle for advanced user interfaces that offer information management and communication services in a single integrated system . One of the most pronounced examples is the networking study by Nardi et al . 4 , who coined the term intensional networks to describe personal social networks", "label": ["user behavior modeling", "information dissemination", "personal information management"], "stemmed_label": ["user behavior model", "inform dissemin", "person inform manag"]}
{"doc": "Object-oriented software development practices are being rapidly adopted within increasingly complex systems , including reactive , real-time and concurrent system applications . While data modeling is performed very well under current object-oriented development practices , behavioral modeling necessary to capture critical information in real-time , reactive , and concurrent systems is often lacking . Addressing this deficiency , we offer an approach for modeling and analyzing concurrent object-oriented software designs through the use of behavioral design patterns , allowing us to map stereotyped UML objects to colored Petri net (CPN) representations in the form of reusable templates . The resulting CPNs are then used to model and analyze behavioral properties of the software architecture , applying the results of the analysis to the original software design . Introduction Object-oriented software development practices are being rapidly adopted within increasingly complex systems , including reactive , real-time and concurrent system applications . In practice , though , object-oriented software design techniques are still predominantly focused on the creation of static class models . Dynamic architectural models capturing the overall behavioral properties of the software system are often constructed using ad hoc techniques with little consideration given to the resulting performance or reliability implications until the project reaches implementation . Efforts to analyze behavioral issues of these architectures occur through opportunistic rather than systematic approaches and are inherently cumbersome , unreliable , and unrepeatable. One means of improving the behavioral modeling capabilities of object-oriented architecture designs is to integrate formalisms with the object-oriented specifications . Using this technique, object-oriented design artifacts are captured in a format such as the Unified Modeling Language (UML) 1 , which is intuitive to the software architect . The native object-oriented design is then augmented by integrating an underlying formal representation capable of providing the necessary analytical tools . The particular method used in this research 2 is to integrate colored Petri nets (CPNs) 3 with object-oriented architecture designs captured in terms of UML communication diagrams. Specifically , this paper will present a method to systematically translate a UML software architecture design into an underlying CPN model using a set of pre-defined CPN templates based on a set of object behavioral roles . These behavioral roles are based on the object structuring criteria found in the COMET method 4 , but are not dependent on any given method and are applicable across application domains . This paper will also demonstrate some of the analytical benefits provided by constructing a CPN representation of the UML software architecture . After a survey of related research , Section 2 descries the concept of behavioral design pattern templates for modeling concurrent objects . Section 3 discusses how we construct an overall CPN model of the concurrent software architecture by interconnecting the individual behavioral design pattern templates . Section 4 describes the validation of the approach. 1.1 Related Research There are many existing works dealing with the use of Petri nets for describing software behavior . As they relate to this paper, the existing works can be broadly categorized into the modeling of software code", "label": ["software architecture", "behavioral design patterns", "colored petri nets", "comet"], "stemmed_label": ["softwar architectur", "behavior design pattern", "color petri net", "comet"]}
{"doc": "This paper is concerned with `intranet search' . By intranet search , we mean searching for information on an intranet within an organization . We have found that search needs on an intranet can be categorized into types , through an analysis of survey results and an analysis of search log data . The types include searching for definitions , persons , experts , and homepages . Traditional information retrieval only focuses on search of relevant documents , but not on search of special types of information . We propose a new approach to intranet search in which we search for information in each of the special types , in addition to the traditional relevance search . Information extraction technologies can play key roles in such kind of `search by type' approach , because we must first extract from the documents the necessary information in each type . We have developed an intranet search system called `Information Desk' . In the system , we try to address the most important types of search first - finding term definitions , homepages of groups or topics , employees' personal information and experts on topics . For each type of search , we use information extraction technologies to extract , fuse , and summarize information in advance . The system is in operation on the intranet of Microsoft and receives accesses from about 500 employees per month . Feedbacks from users and system logs show that users consider the approach useful and the system can really help people to find information . This paper describes the architecture , features , component technologies , and evaluation results of the system . INTRODUCTION Internet search has made significant progress in recent years . In contrast , intranet search does not seem to be so successful . The IDC white paper entitled \"The high cost of not finding information\" 13 reports that information workers spend from 15% to 35% of their work time on searching for information and 40% of information workers complain that they cannot find the information they need to do their jobs on their company intranets. Many commercial systems 35 , 36 , 37 , 38 , 39 have been developed for intranet search . However , most of them view intranet search as a problem of conventional relevance search . In relevance search , when a user types a query , the system returns a list of ranked documents with the most relevant documents on the top. Relevance search can only serve average needs well . It cannot, however , help users to find information in a specific type , e.g., definitions of a term and experts on a topic . The characteristic of intranet search does not seem to be sufficiently leveraged in the commercial systems. In this paper , we try to address intranet search in a novel approach. We assume that the needs of information access on intranets can be categorized into searches for information in different types . An analysis on search log data on the intranet of Microsoft and", "label": ["search needs", "metadata extraction", "features", "architecture", "experimentation", "definition search", "information desk", "information extraction", "expert finding", "algorithms", "intranet search", "human factors", "information retrieval", "component technologies", "intranet search", "types of information"], "stemmed_label": ["search need", "metadata extract", "featur", "architectur", "experiment", "definit search", "inform desk", "inform extract", "expert find", "algorithm", "intranet search", "human factor", "inform retriev", "compon technolog", "intranet search", "type of inform"]}
{"doc": "Motivated by recent surfacing viruses that can spread over the air interfaces , in this paper , we investigate the potential disastrous threat of node compromise spreading in wireless sensor networks . Originating from a single infected node , we assume such a compromise can propagate to other sensor nodes via communication and preestablished mutual trust . We focus on the possible epidemic breakout of such propagations where the whole network may fall victim to the attack . Based on epidemic theory , we model and analyze this spreading process and identify key factors determining potential outbreaks . In particular , we perform our study on random graphs precisely constructed according to the parameters of the network , such as distance , key sharing constrained communication and node recovery , thereby reflecting the true characteristics therein . The analytical results provide deep insights in designing potential defense strategies against this threat . Furthermore , through extensive simulations , we validate our model and perform investigations on the system dynamics . Index Terms-- Sensor Networks , Epidemiology , Random Key Predistribution , Random Graph . Introduction As wireless sensor networks are unfolding their vast potential in a plethora of application environments 1 , security still remains one of the most critical challenges yet to be fully addressed . In particular , a vital problem in the highly distributed and resource constrained environment is node compromise , where a sensor node can be completely captured and manipulated by the adversary. While extensive work has focused on designing schemes that can either defend and delay node capture or timely identify and revoke compromised nodes themselves 5 , little attention has been paid to the node compromise process itself . Inspired by recently emerged viruses that can spread over air interfaces , we identify in this paper the threat of epidemic spreading of node compromises in large scale wireless sensor networks and present a model that captures the unique characteristic of wireless sensor networks in conjunction with pairwise key schemes . In particular , we identify the key factors determining the potential epidemic outbreaks that in turn can be employed to devise corresponding defense strategies. A . Motivation Due to its scarce resources and hence low defense capabilities , node compromises can be expected to be common phenomena for wireless sensor networks in unattended and hostile environments . While extensive research efforts, including those from ourselves 15 , have been engineered toward designing resilient network security mechanisms 12 , 13 , the compromise itself and in particular the propagation of node compromise (possible epidemics) have attracted little attention. While node compromise , thanks to physical capture and succeeding analysis , is naturally constrained by the adver-sary's capability , software originated compromises can be much more damaging . Specifically , the recently surfaced virus Cabir 1 that can spread over the air interface has unveiled a disastrous threat for wireless sensor networks. Inescapably , viruses targeting wireless sensor networks will emerge . Consequently , node compromise by way of virus spreading (over the air interface) can effortlessly", "label": ["random key predistribution", "sensor networks", "random graph", "epidemiology"], "stemmed_label": ["random key predistribut", "sensor network", "random graph", "epidemiolog"]}
{"doc": "The literature is very broad considering routing protocols in wireless sensor networks (WSNs) . However , security of these routing protocols has fallen beyond the scope so far . Routing is a fundamental functionality in wireless networks , thus hostile interventions aiming to disrupt and degrade the routing service have a serious impact on the overall operation of the entire network . In order to analyze the security of routing protocols in a precise and rigorous way , we propose a formal framework encompassing the definition of an adversary model as well as the \"general\" definition of secure routing in sensor networks . Both definitions take into account the feasible goals and capabilities of an adversary in sensor environments and the variety of sensor routing protocols . In spirit , our formal model is based on the simulation paradigm that is a successfully used technique to prove the security of various cryptographic protocols . However , we also highlight some differences between our model and other models that have been proposed for wired or wireless networks . Finally , we illustrate the practical usage of our model by presenting the formal description of a simple attack against an authenticated routing protocol , which is based on the well-known TinyOS routing . INTRODUCTION Routing is a fundamental function in every network that is based on multi-hop communications , and wireless sensor networks are no exceptions. Consequently , a multitude of routing protocols have been proposed for sensor networks in the recent past . However , most of these protocols have not been designed with security requirements in mind . This means that they can badly fail in hostile environments . Paradoxically , research on wireless sensor networks have been mainly fuelled by their potential applications in military settings where the environment is hostile . The natural question that may arise is why then security of routing protocols for sensor networks has fallen beyond the scope of research so far. We believe that one important reason for this situation is that the design principles of secure routing protocols for wireless sensor networks are poorly understood today . First of all , there is no clear definition of what secure routing should mean in this context . Instead , the usual approach, exemplified in 10 , is to list different types of possible attacks against routing in wireless sensor networks , and to define routing security implicitly as resistance to (some of) these attacks . However , there are several problems with this approach . For instance , a given protocol may resist a different set of attacks than another one . How to compare these protocols? Shall we call them both secure routing protocols ? Or on what grounds should we declare one protocol more secure than another? Another problem is that it is quite difficult to carry out a rigorous analysis when only a list of potential attack types are given . How can we be sure that all possible attacks of a given type has been considered in the analysis? It is", "label": ["simulatability", "adversary model", "routing protocols", "sensor networks", "provable security"], "stemmed_label": ["simulat", "adversari model", "rout protocol", "sensor network", "provabl secur"]}
{"doc": "We investigate whether it is possible to encrypt a database and then give it away in such a form that users can still access it , but only in a restricted way . In contrast to conventional privacy mechanisms that aim to prevent any access to individual records , we aim to restrict the set of queries that can be feasibly evaluated on the encrypted database . We start with a simple form of database obfuscation which makes database records indistinguishable from lookup functions . The only feasible operation on an obfuscated record is to look up some attribute Y by supplying the value of another attribute X that appears in the same record (i.e. , someone who does not know X cannot feasibly retrieve Y ) . We then (i) generalize our construction to conjunctions of equality tests on any attributes of the database , and (ii) achieve a new property we call group privacy . This property ensures that it is easy to retrieve individual records or small subsets of records from the encrypted database by identifying them precisely , but \"mass harvesting\" queries matching a large number of records are computationally infeasible . Our constructions are non-interactive . The database is transformed in such a way that all queries except those ex-plicitly allowed by the privacy policy become computationally infeasible , i.e. , our solutions do not rely on any access-control software or hardware . INTRODUCTION Conventional privacy mechanisms usually provide all-or-nothing privacy . For example , secure multi-party computation schemes enable two or more parties to compute some joint function while revealing no information about their respective inputs except what is leaked by the result of the computation . Privacy-preserving data mining aims to com-pletely hide individual records while computing global statistical properties of the database . Search on encrypted data and private information retrieval enable the user to retrieve data from an untrusted server without revealing the query. In this paper , we investigate a different concept of privacy. Consider a data owner who wants to distribute a database to potential users . Instead of hiding individual data entries , he wants to obfuscate the database so that only certain queries can be evaluated on it , i.e. , the goal is to ensure that the database , after it has been given out to users , can be accessed only in the ways permitted by the privacy policy. Note that there is no interaction between the data owner and the user when the latter accesses the obfuscated database. Our constructions show how to obfuscate the database before distributing it to users so that only the queries permitted by the policy are computationally feasible . This concept of privacy is incomparable to conventional definitions because , depending on the policy , a permitted query may or even should reveal individual data entries. For example , a college alumni directory may be obfuscated in such a way that someone who already knows a person's name and year of graduation is able to look up that person's email address", "label": ["database privacy", "obfuscation"], "stemmed_label": ["databas privaci", "obfusc"]}
{"doc": "Peer-to-Peer (P2P ) data integration systems have recently attracted significant attention for their ability to manage and share data dispersed over different peer sources . While integrating data for answering user queries , it often happens that inconsistencies arise , because some integrity constraints specified on peers' global schemas may be violated . In these cases , we may give semantics to the inconsistent system by suitably \"repairing\" the retrieved data , as typically done in the context of traditional data integration systems . However , some specific features of P2P systems , such as peer autonomy and peer preferences (e.g. , different source trusting ) , should be properly addressed to make the whole approach effective . In this paper , we face these issues that were only marginally considered in the literature . We first present a formal framework for reasoning about autonomous peers that exploit individual preference criteria in repairing the data . The idea is that queries should be answered over the best possible database repairs with respect to the preferences of all peers , i.e. , the states on which they are able to find an agreement . Then , we investigate the computational complexity of dealing with peer agreements and of answering queries in P2P data integration systems . It turns out that considering peer preferences makes these problems only mildly harder than in traditional data integration systems . INTRODUCTION Peer-to-Peer (P2P ) data integration systems are networks of autonomous peers that have recently emerged as an effective architecture for decentralized data sharing , integration, and querying . Indeed , P2P systems offer transparent access to the data stored at (the sources of) each peer , by means of the global schema equipped with for modeling its domain of interest; moreover , pair of peers with the same domain of interest one peer and the system is in charge of accessing each peer containing relevant data separately , and combining local results into a global answer by suitably exploiting the mapping rules. P2P systems can be considered the natural evolution of traditional data integration systems , which have received considerable attention in the last few years , and which have already become a key technology for managing enormous amounts of information dispersed over many data sources. In fact , P2P systems have attracted significant attention recently , both in the development of efficient distributed algorithms for the retrieval of relevant information and for answering user queries (see , e.g. , 9 , 21 , 12 , 13 ) , and in the investigation of its theoretical underpinnings (see , e.g. , 16, 3 , 20 , 11 , 9 , 5 ). In this paper , we continue along this latter line of research, by investigating some important theoretical issues . In particular , we consider an expressive framework where integrity constraints are specified on peer schemas in order to enhance their expressiveness , so that each peer can be in fact considered a completely specified data integration system . In this scenario , it may", "label": ["peer-to-peer systems", "data integration systems"], "stemmed_label": ["peer-to-p system", "data integr system"]}
{"doc": "In this paper we study market share rules , rules that have a certain market share statistic associated with them . Such rules are particularly relevant for decision making from a business perspective . Motivated by market share rules , in this paper we consider statistical quantitative rules (SQ rules) that are quantitative rules in which the RHS can be any statistic that is computed for the segment satisfying the LHS of the rule . Building on prior work , we present a statistical approach for learning all significant SQ rules , i.e. , SQ rules for which a desired statistic lies outside a confidence interval computed for this rule . In particular we show how resampling techniques can be effectively used to learn significant rules . Since our method considers the significance of a large number of rules in parallel , it is susceptible to learning a certain number of &quot;false&quot; rules . To address this , we present a technique that can determine the number of significant SQ rules that can be expected by chance alone , and suggest that this number can be used to determine a &quot;false discovery rate&quot; for the learning procedure . We apply our methods to online consumer purchase data and report the results . INTRODUCTION Rule discovery is widely used in data mining for learning interesting patterns . Some of the early approaches for rule learning were in the machine learning literature 11 , 12 , 21 . More recently there have been many algorithms 1 , 25 , 28 , 31 proposed in the data mining literature , most of which are based on the concept of association rules 1 . While all these various approaches have been successfully used in many applications 8, 22 , 24 , there are still situations that these types of rules do not capture . The problem studied in this paper is motivated by market share rules , a specific type of rule that cannot be represented as association rules . Informally , a market share rule is a rule that specifies the market share of a product or a firm under some conditions. The results we report in this paper are from real user-level Web browsing data provided to us by comScore Networks . The data consists of browsing behavior of 100,000 users over 6 months . In addition to customer specific attributes , two attributes in a transaction that are used to compute the market share are the site at which a purchase was made and the purchase amount . Consider the example rules below that we discovered from the data: (1) Household Size = 3 35K &lt; Income &lt; 50K ISP = Dialup marketshare Expedia = 27.76% , support = 2.1% (2) Region = North East Household Size = 1 marketshare Expedia = 25.15% , support = 2.2% (3) Education = College Region = West 50 &lt; Household Eldest Age &lt; 55 marketshare Expedia = 2.92%, support=2.2% (4) 18 &lt; Household Eldest Age &lt; 20 marketshare Expedia = 8.16% , support = 2.4% The", "label": ["nonparametric methods", "resampling", "rule discovery", "market share rules", "statistical quantitative rules"], "stemmed_label": ["nonparametr method", "resampl", "rule discoveri", "market share rule", "statist quantit rule"]}
{"doc": "Prolonging the network lifetime is one of the most important designing objectives in wireless sensor networks (WSN) . We consider a heterogeneous cluster-based WSN , which consists of two types of nodes: powerful cluster-heads and basic sensor nodes . All the nodes are randomly deployed in a specific area . To better balance the energy dissipation , we use a simple mixed communication modes where the sensor nodes can communicate with cluster-heads in either single-hop or multi-hop mode . Given the initial energy of the basic sensor nodes , we derive the optimal communication range and identify the optimal mixed communication mode to maximize the WSN's lifetime through optimizations . Moreover , we also extend our model from 2-D space to 3-D space . INTRODUCTION A WIRELESS sensor network consists of a large amount of sensor nodes , which have wireless communication capability and some level of ability for signal processing. Distributed wireless sensor networks enable a variety of applications for sensing and controlling the physical world 1 , 2 . One of the most important applications is the monitor of a specific geographical area (e.g. , to detect and monitor the environmental changes in forests) by spreading a great number of wireless sensor nodes across the area 3 6 . Because of the sensor nodes' self constraints (generally tiny size , low-energy supply , weak computation ability , etc.) , it is challenging to develop a scalable , robust , and long-lived wireless sensor network . Much research effort has focused on this area which result in many new technologies and methods to address these problems in recent years . The combination of clustering and data-fusion is one of the most effective approaches to construct the large-scale and energy-efficient data gathering sensor networks 7 9 . In particular , the authors of 9 develop a distributed algorithm called Low-Energy Adaptive Clustering Hierarchy (LEACH) for homogeneous sensor networks where each sensor elects itself as a cluster-head with some probability and The research reported in this paper was supported in part by the U.S. National Science Foundation CAREER Award under Grant ECS-0348694. the cluster reconfiguration scheme is used to balance the energy load . The LEACH allows only single-hop clusters to be constructed . On the other hand , in 10 we proposed the similar clustering algorithms where sensors communicate with their cluster-heads in multi-hop mode . However , in these homogeneous sensor networks , the requirement that every node is capable of aggregating data leads to the extra hardware cost for all the nodes . Instead of using homogeneous sensor nodes and the cluster reconfiguration scheme , the authors of 11 focus on the heterogeneous sensor networks in which there are two types of nodes: supernodes and basic sensor nodes . The supernodes act as the cluster-heads . The basic sensor nodes communicate with their closest cluster-heads via multi-hop mode . The authors of 11 formulate an optimization problem to minimize the network cost which depends on the nodes' densities and initial energies. In addition , The authors of 12 obtain the", "label": ["wireless sensor networks", "heterogeneous cluster-based sensor network", "optimization", "network lifetime", "optimal transmission range", "energy optimization", "voronoi cell", "numerical model", "clustering"], "stemmed_label": ["wireless sensor network", "heterogen cluster-bas sensor network", "optim", "network lifetim", "optim transmiss rang", "energi optim", "voronoi cell", "numer model", "cluster"]}
{"doc": "In this paper we study how we can design an effective parallel crawler . As the size of the Web grows , it becomes imperative to parallelize a crawling process , in order to finish downloading pages in a reasonable amount of time . We first propose multiple architectures for a parallel crawler and identify fundamental issues related to parallel crawling . Based on this understanding , we then propose metrics to evaluate a parallel crawler , and compare the proposed architectures using 40 million pages collected from the Web . Our results clarify the relative merits of each architecture and provide a good guideline on when to adopt which architecture . INTRODUCTION A crawler is a program that downloads and stores Web pages , often for a Web search engine . Roughly , a crawler starts off by placing an initial set of URLs , S , in a queue, where all URLs to be retrieved are kept and prioritized. From this queue , the crawler gets a URL (in some order), downloads the page , extracts any URLs in the downloaded page , and puts the new URLs in the queue . This process is repeated until the crawler decides to stop . Collected pages are later used for other applications , such as a Web search engine or a Web cache. As the size of the Web grows , it becomes more difficult to retrieve the whole or a significant portion of the Web using a single process . Therefore , many search engines often run multiple processes in parallel to perform the above task , so that download rate is maximized . We refer to this type of crawler as a parallel crawler. In this paper we study how we should design a parallel crawler , so that we can maximize its performance (e.g., download rate) while minimizing the overhead from parallelization . We believe many existing search engines already use some sort of parallelization , but there has been little scientific research conducted on this topic . Thus , little has been known on the tradeoffs among various design choices for a parallel crawler . In particular , we believe the following issues make the study of a parallel crawler challenging and interesting: Overlap: When multiple processes run in parallel to download pages , it is possible that different processes download the same page multiple times . One process may not be aware that another process has already downloaded the page . Clearly , such multiple downloads should be minimized to save network bandwidth and increase the crawler's effectiveness . Then how can we coordinate the processes to prevent overlap? Quality: Often , a crawler wants to download \"important\" pages first , in order to maximize the \"quality\" of the downloaded collection . However , in a parallel crawler , each process may not be aware of the whole image of the Web that they have collectively downloaded so far . For this reason , each process may make a crawling decision solely based on its own", "label": ["guideline", "architecture", "parallelization", "web spider", "parallel crawler", "web crawler", "model evaluation"], "stemmed_label": ["guidelin", "architectur", "parallel", "web spider", "parallel crawler", "web crawler", "model evalu"]}
{"doc": "Unlike non-time-critical applications like email and file transfer , network games demand timely data delivery to maintain the seemingly interactive presence of players in the virtual game world . Yet the inherently large transmission delay mean and variance of 3G cellular links make on-time game data delivery difficult . Further complicating the timely game data delivery problem is the frequent packet drops at these links due to inter-symbol interference , fading and shadowing at the physical layer . In this paper , we propose a proxy architecture that enhances the timeliness and reliability of data delivery of interactive games over 3G wireless networks . In particular , a performance enhancing proxy is designed to optimize a new time-critical data type -- variable-deadline data , where the utility of a datum is inversely proportional to the time required to deliver it . We show how a carefully designed and configured proxy can noticeably improve the delivery of network game data . INTRODUCTION While network gaming has long been projected to be an application of massive economic growth , as seen in the recent explosive development on the wired Internet in South Korea and Japan , deployment of similar network games on 3G wireless networks continues to be slow and difficult . One reason is that unlike their wired counterparts , wireless links are notoriously prone to errors due to channel fading , shadowing and inter-symbol interference . While 3G wireless networks, such as High Speed Downlink Packet Access (HSDPA) of 3rd Generation Partnership Project (3GPP) Release 5 (R5) 1 and CDMA 1x EvDO of 3GPP2 5 , combat wireless link failures at the MAC and physical layer with an elaborate system of channel coding , retransmission , modulation and spreading , with resulting packet loss rate being reduced to negligible 1 to 2% , the detrimental side-effect to network gaming is the large and often unpredictable transmission delay mean and variance 15 . Such large and variable delays greatly reduce the necessary interactivity of network game players and deteriorate the overall gaming experience. In a separate development , a new 3G network element called IP Multimedia Subsystem (IMS) 3 has been intro-duced in 3GPP specifications R5 and later , as shown in Figure 1 . The Session Initiation Protocol (SIP)-based IMS provides a multitude of multimedia services: from establishing connections from the legacy telephone networks to the new IP core network using Voice over IP (VoIP) , to delivering streaming services such as video as a value-added service to mobile users (UE) . Strategically located as a pseudo-gateway to the private and heavily provisioned 3G networks, it is foreseeable that IMS will continue to enlarge and enrich its set of multimedia services in future wireless networks. In this paper , we propose a performance enhancing proxy (PEP) called (W)ireless (I)nteractive (N)etwork (G)aming Proxy (WING) to improve the timely delivery of network game data in 3G wireless networks . WING is located inside IMS as an application service on top of the myriad of 207 services that IMS already provides . In a nutshell", "label": ["wireless networks", "3g wireless network", "time critical data", "network gaming", "congestion control", "loss-optimized", "rlc configuration", "proxy architecture"], "stemmed_label": ["wireless network", "3g wireless network", "time critic data", "network game", "congest control", "loss-optim", "rlc configur", "proxi architectur"]}
{"doc": "In this paper , we present a method for real-time visual simulation of diverse dynamic phenomena using programmable graphics hardware . The simulations we implement use an extension of cellular automata known as the coupled map lattice (CML) . CML represents the state of a dynamic system as continuous values on a discrete lattice . In our implementation we store the lattice values in a texture , and use pixel-level programming to implement simple next-state computations on lattice nodes and their neighbors . We apply these computations successively to produce interactive visual simulations of convection , reaction-diffusion , and boiling . We have built an interactive framework for building and experimenting with CML simulations running on graphics hardware , and have integrated them into interactive 3D graphics applications . Introduction Interactive 3D graphics environments , such as games , virtual environments , and training and flight simulators are becoming increasingly visually realistic , in part due to the power of graphics hardware . However , these scenes often lack rich dynamic phenomena , such as fluids , clouds , and smoke , which are common to the real world. A recent approach to the simulation of dynamic phenomena , the coupled map lattice Kaneko 1993 , uses a set of simple local operations to model complex global behavior . When implemented using computer graphics hardware , coupled map lattices (CML) provide a simple , fast and flexible method for the visual simulation of a wide variety of dynamic systems and phenomena. In this paper we will describe the implementation of CML systems with current graphics hardware , and demonstrate the flexibility and performance of these systems by presenting several fast interactive 2D and 3D visual simulations . Our CML boiling simulation runs at speeds ranging from 8 iterations per second for a 128x128x128 lattice to over 1700 iterations per second for a 64x64 lattice. Section 2 describes CML and other methods for simulating natural phenomena . Section 3 details our implementation of CML simulations on programmable graphics hardware , and Section 4 describes the specific simulations we have implemented . In Section 5 we discuss limitations of current hardware and investigate some solutions . Section 6 concludes. CML and Related Work The standard approach to simulating natural phenomena is to solve equations that describe their global behavior . For example , multiple techniques have been applied to solving the Navier-Stokes fluid equations Fedkiw , et al . 2001;Foster and Metaxas 1997;Stam 1999 . While their results are typically numerically and visually accurate , many of these simulations require too much computation (or small lattice sizes) to be integrated into interactive graphics applications such as games . CML models , instead of solving for the global behavior of a phenomenon , model the behavior by a number of very simple local operations . When aggregated, these local operations produce a visually accurate approximation to the desired global behavior. Figure 1: 3D coupled map lattice simulations running on graphics hardware . Left: Boiling . Right: Reaction-Diffusion . A coupled map lattice is a mapping", "label": ["coupled map lattice", "visual simulation", "reaction-diffusion", "dynamic phenomena", "multipass rendering", "simulation", "cml", "graphic hardware", "graphics hardware"], "stemmed_label": ["coupl map lattic", "visual simul", "reaction-diffus", "dynam phenomena", "multipass render", "simul", "cml", "graphic hardwar", "graphic hardwar"]}
{"doc": "A common measure of the quality or effectiveness of a virtual environment (VE) is the amount of presence it evokes in users . Presence is often defined as the sense of being there in a VE . There has been much debate about the best way to measure presence , and presence researchers need , and have sought , a measure that is reliable , valid , sensitive , and objective . We hypothesized that to the degree that a VE seems real , it would evoke physiological responses similar to those evoked by the corresponding real environment , and that greater presence would evoke a greater response . To examine this , we conducted three experiments , the results of which support the use of physiological reaction as a reliable , valid , sensitive , and objective presence measure . The experiments compared participants' physiological reactions to a non-threatening virtual room and their reactions to a stressful virtual height situation . We found that change in heart rate satisfied our requirements for a measure of presence , change in skin conductance did to a lesser extent , and that change in skin temperature did not . Moreover , the results showed that inclusion of a passive haptic element in the VE significantly increased presence and that for presence evoked: 30FPS &gt; 20FPS &gt; 15FPS . Introduction Virtual environments (VEs) are the most sophisticated human-computer interfaces yet developed . The effectiveness of a VE might be defined in terms of enhancement of task performance, effectiveness for training , improvement of data comprehension, etc . A common metric of VE quality is the degree to which the VE creates in the user the subjective illusion of presence a sense of being in the virtual , as opposed to the real , environment . Since presence is a subjective condition , it has most commonly been measured by self-reporting , either during the VE experience or immediately afterwards by questionnaires . There has been vigorous debate as to how to best measure presence Barfield et al. 1995; Ellis 1996; Freeman et al . 1998; IJsselsteijn and de Ridder 1998; Lombard and Ditton 1997; Regenbrecht and Schubert 1997; Schubert et al . 1999; Sheridan 1996; Slater 1999; Witmer and Singer 1998 . In order to study a VE's effectiveness in evoking presence, researchers need a well-designed and verified measure of the phenomena . This paper reports our evaluation of three physiological measures heart rate , skin conductance , and skin temperature as alternate operational measures of presence in stressful VEs . Since the concept and idea of measuring presence are heavily debated , finding a measure that could find wide acceptance would be ideal . In that hope , we investigated the reliability , validity , sensitivity , and objectivity of each physiological measure. Figure 1 . Side view of the virtual environment . Subjects start in the Training Room and later enter the Pit Room. 1.2 . Physiological Reaction as a Surrogate Measure of Presence As VE system and technology designers , we have", "label": ["presence", "haptics", "measurement", "frame rate", "virtual environment", "presence", "physiology"], "stemmed_label": ["presenc", "haptic", "measur", "frame rate", "virtual environ", "presenc", "physiolog"]}
{"doc": "A new statistical formula for identifying 2-character words in Chinese text , called the contextual information formula , was developed empirically by performing stepwise logistic regression using a sample of sentences that had been manually segmented . Contextual information in the form of the frequency of characters that are adjacent to the bigram being processed as well as the weighted document frequency of the overlapping bigrams were found to be significant factors for predicting the probablity that the bigram constitutes a word . Local information (the number of times the bigram occurs in the document being segmented) and the position of the bigram in the sentence were not found to be useful in determining words . The contextual information formula was found to be significantly and substantially better than the mutual information formula in identifying 2-character words . The method can also be used for identifying multi-word terms in English text . INTRODUCTION Chinese text is different from English text in that there is no explicit word boundary . In English text , words are separated by spaces . Chinese text (as well as text of other Oriental languages) is made up of ideographic characters , and a word can comprise one , two or more such characters , without explicit indication where one word ends and another begins. This has implications for natural language processing and information retrieval with Chinese text . Text processing techniques that have been developed for Western languages deal with words as meaningful text units and assume that words are easy to identify . These techniques may not work well for Chinese text without some adjustments . To apply these techniques to Chinese text , automatic methods for identifying word boundaries accurately have to be developed . The process of identifying word boundaries has been referred to as text segmentation or , more accurately , word segmentation. Several techniques have been developed for Chinese text segmentation . They can be divided into: 1. statistical methods , based on statistical properties and frequencies of characters and character strings in a corpus (e.g . 13 and 16 ). 2. dictionary-based methods , often complemented with grammar rules . This approach uses a dictionary of words to identify word boundaries . Grammar rules are often used to resolve conflicts (choose between alternative segmentations) and to improve the segmentation (e.g . 4 , 8 , 19 and 20 ). 3. syntax-based methods , which integrate the word segmentation process with syntactic parsing or part-of-speech tagging (e.g . 1 ). 4. conceptual methods , that make use of some kind of semantic processing to extract information and store it in a knowledge representation scheme . Domain knowledge is used for disambiguation (e.g . 9 ). Many researchers use a combination of methods (e.g . 14 ). The objective of this study was to empirically develop a statistical formula for Chinese text segmentation . Researchers have used different statistical methods in segmentation , most of which were based on theoretical considerations or adopted from other fields . In this study , we developed", "label": ["logistic regression", "statistical formula", "word boundary identification", "chinese text segmentation", "word boundary", "natural language processing", "mutual information", "regression model", "contextual information", "multi-word terms"], "stemmed_label": ["logist regress", "statist formula", "word boundari identif", "chines text segment", "word boundari", "natur languag process", "mutual inform", "regress model", "contextu inform", "multi-word term"]}
{"doc": "Automated trust negotiation is an approach which establishes trust between strangers through the bilateral , iterative disclosure of digital credentials . Sensitive credentials are protected by access control policies which may also be communicated to the other party . Ideally , sensitive information should not be known by others unless its access control policy has been satisfied . However , due to bilateral information exchange , information may flow to others in a variety of forms , many of which cannot be protected by access control policies alone . In particular , sensitive information may be inferred by observing negotiation participants' behavior even when access control policies are strictly enforced . In this paper , we propose a general framework for the safety of trust negotiation systems . Compared to the existing safety model , our framework focuses on the actual information gain during trust negotiation instead of the exchanged messages . Thus , it directly reflects the essence of safety in sensitive information protection . Based on the proposed framework , we develop policy databases as a mechanism to help prevent unauthorized information inferences during trust negotiation . We show that policy databases achieve the same protection of sensitive information as existing solutions without imposing additional complications to the interaction between negotiation participants or restricting users' autonomy in defining their own policies . INTRODUCTION Automated trust negotiation (ATN) is an approach to access control and authentication in open , flexible systems such as the Internet . ATN enables open computing by as-Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. CCS'05 , November 711 , 2005 , Alexandria , Virginia , USA. Copyright 2005 ACM 1-59593-226-7/05/0011 ... $ 5.00. signing an access control policy to each resource that is to be made available to entities from different domains . An access control policy describes the attributes of the entities allowed to access that resource , in contrast to the traditional approach of listing their identities . To satisfy an access control policy , a user has to demonstrate that they have the attributes named in the policy through the use of digital credentials . Since one's attributes may also be sensitive , the disclosure of digital credentials is also protected by access control policies. A trust negotiation is triggered when one party requests access to a resource owned by another party . Since each party may have policies that the other needs to satisfy , trust is established incrementally through bilateral disclosures of credentials and requests for credentials , a characteristic that distinguishes trust negotiation from other trust establishment approaches 2 , 11 . Access control policies play a central role in protecting privacy during", "label": ["privacy", "trust negotiation", "attribute-based access control"], "stemmed_label": ["privaci", "trust negoti", "attribute-bas access control"]}
{"doc": "We propose a new unsupervised learning technique for extracting information from large text collections . We model documents as if they were generated by a two-stage stochastic process . Each author is represented by a probability distribution over topics , and each topic is represented as a probability distribution over words for that topic . The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture . The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm . We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library , and learn a model with 300 topics . We discuss in detail the interpretation of the results discovered by the system including specific topic and author models , ranking of authors by topic and topics by author , significant trends in the computer science literature between 1990 and 2002 , parsing of abstracts by topics and authors and detection of unusual papers by specific authors . An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer . INTRODUCTION With the advent of the Web and various specialized digital libraries , the automatic extraction of useful information from text has become an increasingly important research area in data mining . In this paper we discuss a new algorithm that extracts both the topics expressed in large text document collections and models how the authors of documents use those topics . The methodology is illustrated using a sample of 160,000 abstracts and 80,000 authors from the well-known CiteSeer digital library of computer science research papers (Lawrence , Giles , and Bollacker , 1999) . The algorithm uses a probabilistic model that represents topics as probability distributions over words and documents as being composed of multiple topics . A novel feature of our model is the inclusion of author models , in which authors are modeled as probability distributions over topics. The author-topic models can be used to support a variety of interactive and exploratory queries on the set of documents and authors , including analysis of topic trends over time , finding the authors who are most likely to write on a given topic , and finding the most unusual paper written by a given author . Bayesian unsupervised learning is used to fit the model to a document collection. Supervised learning techniques for automated categorization of documents into known classes or topics has received considerable attention in recent years (e.g. , Yang , 1998). For many document collections , however , neither predefined topics nor labeled documents may be available . Furthermore , there is considerable motivation to uncover hidden topic structure in large corpora , particularly in rapidly changing fields such as computer science and biology , where predefined topic categories may not accurately reflect rapidly evolving content. Automatic extraction of topics from text , via unsupervised learning , has been addressed", "label": ["gibbs sampling", "text modeling", "unsupervised learning"], "stemmed_label": ["gibb sampl", "text model", "unsupervis learn"]}
{"doc": "Speed , accuracy , and subjective satisfaction are the most common measures for evaluating the usability of search user interfaces . However , these measures do not facilitate comparisons optimally and they leave some important aspects of search user interfaces uncovered . We propose new , proportional measures to supplement the current ones . Search speed is a normalized measure for the speed of a search user interface expressed in answers per minute . Qualified search speed reveals the trade-off between speed and accuracy while immediate search accuracy addresses the need to measure success in typical web search behavior where only the first few results are interesting . The proposed measures are evaluated by applying them to raw data from two studies and comparing them to earlier measures . The evaluations indicate that they have desirable features . INTRODUCTION In order to study the usability of search user interfaces we need proper measures . In the literature , speed , accuracy and subjective satisfaction measures are common and reveal interesting details . They have , however , a few shortcomings that call for additional measures. First , comparing results even within one experiment--let alone between different experiments--is hard because the measures are not typically normalized in the research reports but multiple raw numbers (like answers found and time used) are reported . Of course , unbiased comparison between studies will always be difficult as the test setup has a big effect on the results , but the problem is compounded by the presentation of multiple task dependent measures . A good measure would be as simple as possible , yet it must not discard relevant information. Second , the current measures do not reveal the sources of speed differences . In particular , the relation between speed and accuracy may be hard to understand since the current measures for those dimensions are completely separate . For example , it is essential to know if the increase in speed is due to careless behavior or better success. Third , in the web environment , a typical goal for a search is to find just a few good enough answers to a question . This is demonstrated by studies that show that about half of the users only view one or two result pages per query 11 . Current search user interface usability measures do not capture the success of such a behavior very well. In order to address these problems , we present three new proportional , normalized usability measures . The new measures are designed for the result evaluation phase of the search process 10 where real users are involved . Search speed is a normalized speed measure expressed in answers per minute . It makes within study comparisons simple and between studies bit more feasible . Qualified search speed is a combination of speed and accuracy measures that reveals the tradeoff between speed and accuracy . It shows the source of speed differences in terms of accuracy and is also measured in answers per minute . Immediate search accuracy is a", "label": ["usability evaluation", "search user interface", "speed", "usability measure", "accuracy"], "stemmed_label": ["usabl evalu", "search user interfac", "speed", "usabl measur", "accuraci"]}
{"doc": "Valuable 3D graphical models , such as high-resolution digital scans of cultural heritage objects , may require protection to prevent piracy or misuse , while still allowing for interactive display and manipulation by a widespread audience . We have investigated techniques for protecting 3D graphics content , and we have developed a remote rendering system suitable for sharing archives of 3D models while protecting the 3D geometry from unauthorized extraction . The system consists of a 3D viewer client that includes low-resolution versions of the 3D models , and a rendering server that renders and returns images of high-resolution models according to client requests . The server implements a number of defenses to guard against 3D reconstruction attacks , such as monitoring and limiting request streams , and slightly perturbing and distorting the rendered images . We consider several possible types of reconstruction attacks on such a rendering server , and we examine how these attacks can be defended against without excessively compromising the interactive experience for non-malicious users . Protecting digital information from theft and misuse , a subset of the digital rights management problem , has been the subject of much research and many attempted practical solutions . Efforts to protect software , databases , digital images , digital music files , and other content are ubiquitous , and data security is a primary concern in the design of modern computing systems and processes . However, there have been few technological solutions to specifically protect interactive 3D graphics content. The demand for protecting 3D graphical models is significant . Contemporary 3D digitization technologies allow for the reliable and efficient creation of accurate 3D models of many physical objects, and a number of sizable archives of such objects have been created. The Stanford Digital Michelangelo Project Levoy et al . 2000 , for example , has created a high-resolution digital archive of 10 large statues of Michelangelo , including the David . These statues represent the artistic patrimony of Italy's cultural institutions , and the contract with the Italian authorities permits the distribution of the 3D models only to established scholars for non-commercial use. Though all parties involved would like the models to be widely available for constructive purposes , were the digital 3D model of the David to be distributed in an unprotected fashion , it would soon be pirated , and simulated marble replicas would be manufactured outside the provisions of the parties authorizing the creation of the model. Digital 3D archives of archaeological artifacts are another example of 3D models often requiring piracy protection . Curators of such artifact collections are increasingly turning to 3D digitization as a way to preserve and widen scholarly usage of their holdings , by allowing virtual display and object examination over the Internet , for example . However , the owners and maintainers of the artifacts often desire to maintain strict control over the use of the 3D data and to guard against theft . An example of such a collection is Stanford Digital Forma Urbis Project 2004 , in which over", "label": ["digital rights management", "remote rendering", "security", "3d models"], "stemmed_label": ["digit right manag", "remot render", "secur", "3d model"]}
{"doc": "In order to enable the widespread use of robots in home and office environments , systems with natural interaction capabilities have to be developed . A prerequisite for natural interaction is the robot's ability to automatically recognize when and how long a person's attention is directed towards it for communication . As in open environments several persons can be present simultaneously , the detection of the communication partner is of particular importance . In this paper we present an attention system for a mobile robot which enables the robot to shift its attention to the person of interest and to maintain attention during interaction . Our approach is based on a method for multi-modal person tracking which uses a pan-tilt camera for face recognition , two microphones for sound source localization , and a laser range finder for leg detection . Shifting of attention is realized by turning the camera into the direction of the person which is currently speaking . From the orientation of the head it is decided whether the speaker addresses the robot . The performance of the proposed approach is demonstrated with an evaluation . In addition , qualitative results from the performance of the robot at the exhibition part of the ICVS'03 are provided . INTRODUCTION A prerequisite for the successful application of mobile service robots in home and office environments is the development of systems with natural human-robot-interfaces . Much research focuses Figure 1: Even in crowded situations (here at the ICVS'03) the mobile robot BIRON is able to robustly track persons and shift its attention to the speaker. on the communication process itself , e.g . speaker-independent speech recognition or robust dialog systems . In typical tests of such human-machine interfaces , the presence and position of the communication partner is known beforehand as the user either wears a close-talking microphone or stands at a designated position . On a mobile robot that operates in an environment where several people are moving around , it is not always obvious for the robot which of the surrounding persons wants to interact with it . Therefore, it is necessary to develop techniques that allow a mobile robot to automatically recognize when and how long a user's attention is directed towards it for communication. For this purpose some fundamental abilities of the robot are required . First of all , it must be able to detect persons in its vicinity and to track their movements over time in order to differentiate between persons . In previous work , we have demonstrated how tracking of persons can be accomplished using a laser range finder and a pan-tilt color camera 6 . As speech is the most important means of communication for humans , we extended this framework to incorporate sound source information for multi-modal person tracking and attention control. This enables a mobile robot to detect and localize sound sources in the robot's surroundings and , therfore , to observe humans and to shift its attention to a person that is likely to communicate with the robot . The", "label": ["multi-modal person tracking", "attention", "human-robot-interaction"], "stemmed_label": ["multi-mod person track", "attent", "human-robot-interact"]}
{"doc": "In this paper , we describe a framework for a personalization system to systematically induce desired emotion and attention related states and promote information processing in viewers of online advertising and e-commerce product information . Psychological Customization entails personalization of the way of presenting information (user interface , visual layouts , modalities , structures) per user to create desired transient psychological effects and states , such as emotion , attention , involvement , presence , persuasion and learning . Conceptual foundations and empiric evidence for the approach are presented . INTRODUCTION Advertising and presentation of product information is done both to inform people about new products and services and to persuade them into buying them . Persuasion can be thought of as influencing peoples attitudes and behavior . Advertising done in a mass medium , such as television or magazines can be segmented to desired audiences . However , there is a possibility to personalize or mass customize advertising in the internet and for instance in mobile phones . Similarly , in the internet also product information of various items for sale can be personalized to desired users . These two areas are introduced here together as they represent interesting opportunities for personalization. Consequently , personalization may turn out to be an important driver for future commercial applications and services in a one-to-one world in which automatic and intelligent systems tailor the interactions of users , contexts and systems in real-time . This paper describes the foundations of information personalization systems that may facilitate desired psychological states in individual users of internet based advertising and product information presentation in e-commerce thereby creating psychologically targeted messages for users of such systems . It is preliminarily hypothesized that such personalization may be one way to more efficient persuasion. When perceiving information via media and communications technologies users have a feeling of presence . In presence , the mediated information becomes the focused object of perception, while the immediate , external context , including the technological device , fades into the background 8 , 36 , 37 . Empirical studies show that information experienced in presence has real psychological effects on perceivers , such as emotional responses based on the events described or cognitive processing and learning from the events see 51 . It is likely that perceivers of advertisements and product information experience presence that may lead to various psychological effects . For instance , an attitude may be hold with greater confidence the stronger the presence experience. Personalization and customization entails the automatic or semiautomatic adaptation of information per user in an intelligent way with information technology see 33 , 68 . One may also vary the form of information (modality for instance) per user profile, which may systematically produce , amplify , or shade different psychological effects 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 . Media- and communication technologies as special cases of information technology may be considered as consisting of three layers 6 . At the bottom is a physical layer that includes", "label": ["personalization emotion", "persuasion", "advertising", "e-commerce"], "stemmed_label": ["person emot", "persuas", "advertis", "e-commerc"]}
{"doc": "Today , watermarking techniques have been extended from the multimedia context to relational databases so as to protect the ownership of data even after the data are published or distributed . However , all existing watermarking schemes for relational databases are secret key based , thus require a secret key to be presented in proof of ownership . This means that the ownership can only be proven once to the public (e.g. , to the court) . After that , the secret key is known to the public and the embedded watermark can be easily destroyed by malicious users . Moreover , most of the existing techniques introduce distortions to the underlying data in the watermarking process , either by modifying least significant bits or exchanging categorical values . The distortions inevitably reduce the value of the data . In this paper , we propose a watermarking scheme by which the ownership of data can be publicly proven by anyone , as many times as necessary . The proposed scheme is distortion-free , thus suitable for watermarking any type of data without fear of error constraints . The proposed scheme is robust against typical database attacks including tuple/attribute insertion/deletion , ran-dom/selective value modification , data frame-up , and additive attacks INTRODUCTION Ownership protection of digital products after dissemination has long been a concern due to the high value of these assets and the low cost of copying them (i.e. , piracy problem) . With the fast development of information technology , an increasing number of digital products are distributed through the internet . The piracy problem has become one of the most devastating threats to networking systems and electronic business . In recent years , realizing that \"the law does not now provide sufficient protection to the comprehensive and commercially and publicly useful databases that are at the heart of the information economy\" 12 , people have joined together to fight against theft and misuse of databases published online (e.g., parametric specifications , surveys , and life sciences data) 32 , 4 . To address this concern and to fight against data piracy , watermarking techniques have been introduced , first in the multimedia context and now in relational database literature , so that the ownership of the data can be asserted based on the detection of watermark . The use of watermark should not affect the usefulness of data , and it must be difficult for a pirate to invalidate watermark detection without rendering the data much less useful . Watermarking thus deters illegal copying by providing a means for establishing the original ownership of a redistributed copy 1 . In recent years , researchers have developed a variety of watermarking techniques for protecting the ownership of relational databases 1 , 28 , 26 , 29 , 13 , 19 , 20 , 2 (see Section 5 for more on related work) . One common feature of these techniques is that they are secret key based , where ownership is proven through the knowledge of a secret key that is used", "label": ["public verifiability", "certificate", "relational database", "watermark", "ownership protection"], "stemmed_label": ["public verifi", "certif", "relat databas", "watermark", "ownership protect"]}
{"doc": "A person working with diverse information sources--with possibly different formats and information models--may recognize and wish to express conceptual structures that are not explicitly present in those sources . Rather than replicate the portions of interest and recast them into a single , combined data source , we leave base information where it is and superimpose a conceptual model that is appropriate to the task at hand . This superimposed model can be distinct from the model(s) employed by the sources in the base layer . An application that superimposes a new conceptual model over diverse sources , with varying capabilities , needs to accommodate the various types of information and differing access protocols for the base information sources . The Superimposed Pluggable Architecture for Contexts and Excerpts (SPARCE) defines a collection of architectural abstractions , placed between superimposed and base applications , to demarcate and revisit information elements inside base sources and provide access to content and context for elements inside these sources . SPARCE accommodates new base information types without altering existing superimposed applications . In this paper , we briefly introduce several superimposed applications that we have built , and describe the conceptual model each superimposes . We then focus on the use of context in superimposed applications . We describe how SPARCE supports context and excerpts . We demonstrate how SPARCE facilitates building superimposed applications by describing its use in building our two , quite diverse applications . Introduction When a physician prepares for rounds in a hospital intensive care unit , she often creates a quick synopsis of important problems , with relevant lab tests or observations, for each patient , as shown in Figure 1 . The information is largely copied from elsewhere , e.g. , from the patient medical record , or the laboratory system . Although the underlying data sources use various information structures , including dictated free text , tabular results and formatted reports , the physician may organize the selected information items into the simple cells or groups as shown in Figure 1 (without concern for the format or information model of the base sources) . Each row contains information about a single patient , with the four columns containing patient identifying information , (a subset of) the patient's current problems , (a subset of) recent lab results or other reports , and notes (including a \"To Do\" list for the patient) . While the information elements selected for this synopsis will generally suffice for the task at hand (patient rounds) , the physician may need to view an element (such as a problem or a lab result) in the original source Gorman 2000 , Ash 2001 . However , this paper artefact obviously provides no means of automatically returning to the original context of an information element. In an ICU , we have observed a clinician actively working with a potentially diverse set of underlying information sources as she prepares to visit a patient , selecting bits of information from the various information sources , organizing them to suit the current", "label": ["superimposed information", "sparce .", "excerpts", "conceptual modelling", "software architecture", "context"], "stemmed_label": ["superimpos inform", "sparc .", "excerpt", "conceptu model", "softwar architectur", "context"]}
{"doc": "Along with the development of multimedia and wireless networking technologies , mobile multimedia applications are playing more important roles in information access . Quality of Service (QoS) is a critical issue in providing guaranteed service in a low bandwidth wireless environment . To provide Bluetooth-IP services with differentiated quality requirements , a QoS-centric cascading mechanism is proposed in this paper . This innovative mechanism , composed of intra-piconet resource allocation , inter-piconet handoff and Bluetooth-IP access modules , is based on the Bluetooth Network Encapsulation Protocol (BNEP) operation scenario . From our simulations the handoff connection time for a Bluetooth device is up to 11.84 s and the maximum average transmission delay is up to 4e-05 s when seven devices join a piconet simultaneously . Increasing the queue length for the Bluetooth-IP access system will decrease the traffic loss rate by 0.02 per 1000 IP packets at the expense of a small delay performance . Introduction Wireless communications have evolved rapidly over the past few years . Much attention has been given to research and development in wireless networking and personal mobile computing 10,17 . The number of computing and telecommunications devices is increasing and consequently , portable computing and communications devices like cellular phones, personal digital assistants , tablet PCs and home appliances are used widely . Wireless communication technologies will offer the subscriber greater flexibility and capability than ever before 14 . In February 1998 , mobile telephony and computing leaders Ericsson , Nokia , IBM , Toshiba , and Intel formed a Special Interest Group (SIG) to create a standard radio interface named Bluetooth 13 . The main aim of Bluetooth has been the development of a wireless replacement for cables between electronic devices via a universal radio link in the globally available and unlicensed 2.4 GHz Industrial Scientific and Medical (ISM) frequency band 9 . Bluetooth technologies have the potential to ensure that the best services , system resources and quality are delivered and used efficiently . However , global services will embrace all types of networks . Therefore , bluetooth-based service networks will interconnect with IPv4/v6 existing networks to provide wide area network connectivity and Internet access to and between, individuals and devices 7 . In Reference 2 , the BLUEPAC (BLUEtooth Public ACcess) concepts presented ideas for enabling mobile Bluetooth devices to access local area networks in public areas , such as airports , train stations and supermarkets . The Bluetooth specification defined how Bluetooth-enabled devices (BT) can access IP network services using the IETF Point-to-Point Protocol (PPP) and the Bluetooth Network Encapsulation Protocol (BNEP) 12,19,20 . By mapping IP addresses on the corresponding BT addresses (BD_ADDR) , common access across networks is enabled 3 . This means that devices from different networks are allowed to discover and use one another's services without the need for service translation or user interaction . To support communications between all Bluetooth-based home appliances and the existing IP world , IPv6 over Bluetooth (6overBT) technology was proposed 8 . The 6overBT technology suggested that no additional link layer or encapsulation", "label": ["handoff", "quality of service", "bluetooth-ip access system", "bnep protocol", "resource allocation"], "stemmed_label": ["handoff", "qualiti of servic", "bluetooth-ip access system", "bnep protocol", "resourc alloc"]}
{"doc": "This paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online . Most question answering systems use a wide variety of linguistic resources . We focus instead on the redundancy available in large corpora as an important resource . We use this redundancy to simplify the query rewrites that we need to use , and to support answer mining from returned snippets . Our system performs quite well given the simplicity of the techniques being utilized . Experimental results show that question answering accuracy can be greatly improved by analyzing more and more matching passages . Simple passage ranking and n-gram extraction techniques work well in our system making it efficient to use with many backend retrieval engines . INTRODUCTION Question answering has recently received attention from the information retrieval , information extraction , machine learning, and natural language processing communities 1 3 19 20 The goal of a question answering system is to retrieve `answers' to questions rather than full documents or even best-matching passages as most information retrieval systems currently do . The TREC Question Answering Track which has motivated much of the recent work in the field focuses on fact-based , short-answer questions such as \"Who killed Abraham Lincoln?\" or \"How tall is Mount Everest?\" . In this paper we focus on this kind of question answering task , although the techniques we propose are more broadly applicable. The design of our question answering system is motivated by recent observations in natural language processing that , for many applications , significant improvements in accuracy can be attained simply by increasing the amount of data used for learning. Following the same guiding principle we take advantage of the tremendous data resource that the Web provides as the backbone of our question answering system . Many groups working on question answering have used a variety of linguistic resources part-of-speech tagging , syntactic parsing , semantic relations, named entity extraction , dictionaries , WordNet , etc . (e.g., 2 8 11 12 13 15 16 ).We chose instead to focus on the Web as gigantic data repository with tremendous redundancy that can be exploited for question answering . The Web , which is home to billions of pages of electronic text , is orders of magnitude larger than the TREC QA document collection , which consists of fewer than 1 million documents . This is a resource that can be usefully exploited for question answering . We view our approach as complimentary to more linguistic approaches , but have chosen to see how far we can get initially by focusing on data per se as a key resource available to drive our system design. Automatic QA from a single , small information source is extremely challenging , since there is likely to be only one answer in the source to any user's question . Given a source , such as the TREC corpus , that contains only a relatively small number of formulations of answers to a query , we", "label": ["rewrite query", "n-gram extraction techniques", "automatic qa", "experimentation", "information extraction", "algorithms", "question answering system", "redundancy in large corpora", "facilitates answer mining", "natural language processing", "information retrieval", "machine learning", "trec qa", "simple passage ranking"], "stemmed_label": ["rewrit queri", "n-gram extract techniqu", "automat qa", "experiment", "inform extract", "algorithm", "question answer system", "redund in larg corpora", "facilit answer mine", "natur languag process", "inform retriev", "machin learn", "trec qa", "simpl passag rank"]}
{"doc": "Programming languages are a part of the core of computer science . Courses on programming languages are typically offered to junior or senior students , and textbooks are based on this assumption . However , our computer science curriculum offers the programming languages course in the first year . This unusual situation led us to design it from an untypical approach . In this paper , we first analyze and classify proposals for the programming languages course into different pure and hybrid approaches . Then , we describe a course for freshmen based on four pure approaches , and justify the main choices made . Finally , we identify the software used for laboratories and outline our experience after teaching it for seven years . INTRODUCTION The topic of programming languages is a part of the core of computer science . It played a relevant role in all the curricula recommendations delivered by the ACM or the IEEE-CS since the first Curriculum'68 2 . Recent joint curricular recommendations of the ACM and the IEEE-CS identified several \"areas\" which structure the body of knowledge of the discipline. The list of areas has grown since the first proposal made by the Denning Report 4 up to 14 in the latest version , Computing Curricula 2001 12 . Programming languages has always been one of these areas. Internationally reputed curricular recommendations are a valuable tool for the design of particular curricula . However , each country has specific features that constrain the way of organizing their studies . In Spain , the curriculum of a discipline offered by a university is the result of a trade-off . On the one hand , the university must at least offer a number of credits of the core subject matters established by the Government . On the other hand , the university may offer supplementary credits of the core as well as mandatory and optional courses defined according to the profile of the University and the faculty . Any proposal of a new curriculum follows a well-established process: (1) the curriculum is designed by a Center after consulting the departments involved; (2) it must be approved by the University Council; (3) the Universities Council of the Nation must deliver a (positive) report; and (4) the curriculum is published in the Official Bulletin of the Nation . This scheme has a number of advantages , e.g . a minimum degree of coherence among all the universities is guaranteed . However , it also has a number of disadvantages , e.g . the process to change a curriculum is very rigid. The Universidad Rey Juan Carlos is a young university , now seven years old . It offered studies of computer science since the very first year . The curriculum was designed by an external committee , so the teachers of computer science thereafter hired by the university did not have the opportunity to elaborate on it . The curriculum had a few weak points that would recommend a light reform , but the priorities of the new university postponed", "label": ["programming language course", "language description", "formal grammars", "laboratory component", "functional programming", "computer science", "programming methodology", "programming languages", "recursion", "curriculum", "freshmen", "topics", "programming paradigms"], "stemmed_label": ["program languag cours", "languag descript", "formal grammar", "laboratori compon", "function program", "comput scienc", "program methodolog", "program languag", "recurs", "curriculum", "freshmen", "topic", "program paradigm"]}
{"doc": "To deal with the problem of too many results returned from an E-commerce Web database in response to a user query , this paper proposes a novel approach to rank the query results . Based on the user query , we speculate how much the user cares about each attribute and assign a corresponding weight to it . Then , for each tuple in the query result , each attribute value is assigned a score according to its \"desirableness\" to the user . These attribute value scores are combined according to the attribute weights to get a final ranking score for each tuple . Tuples with the top ranking scores are presented to the user first . Our ranking method is domain independent and requires no user feedback . Experimental results demonstrate that this ranking method can effectively capture a user's preferences . INTRODUCTION With the rapid expansion of the World Wide Web , more and more Web databases are available . At the same time , the size of existing Web databases is growing rapidly . One common problem faced by Web users is that there is usually too many query results returned for a submitted query . For example , when a user submits a query to autos.yahoo.com to search for a used car within 50 miles of New York with a price between $5,000 and $10,000 , 10,483 records are returned . In order to find \"the best deal\" , the user has to go through this long list and compare the cars to each other , which is a tedious and time-consuming task. Most Web databases rank their query results in ascending or descending order according to a single attribute (e.g. , sorted by date, sorted by price , etc.) . However , many users probably consider multiple attributes simultaneously when judging the relevance or desirableness of a result . While some extensions to SQL allow the user to specify attribute weights according to their importance to him/her 21 , 26 , this approach is cumbersome and most likely hard to do for most users since they have no clear idea how to set appropriate weights for different attributes . Furthermore , the user-setting -weight approach is not applicable for categorical attributes. In this paper , we tackle the many-query-result problem for Web databases by proposing an automatic ranking method , QRRE (Query Result Ranking for E-commerce) , which can rank the query results from an E-commerce Web database without any user feedback . We focus specifically on E-commerce Web databases because they comprise a large part of today's online databases . In addition , most E-commerce customers are ordinary users who may not know how to precisely express their interests by formulating database queries . The carDB Web database is used in the following examples to illustrate the intuitions on which QRRE is based. Example 1: Consider a used Car-selling Web database D with a single table carDB in which the car instances are stored as tuples with attributes: Make , Model , Year , Price ,", "label": ["many query result problem", "rank the query results", "query result ranking", "qrre", "algorithms", "experimentation", "attribute value", "attribute weight assignment", "query result ranking", "attribute preference", "design", "pir", "e-commerce web databases", "human factors", "e-commerce"], "stemmed_label": ["mani queri result problem", "rank the queri result", "queri result rank", "qrre", "algorithm", "experiment", "attribut valu", "attribut weight assign", "queri result rank", "attribut prefer", "design", "pir", "e-commerc web databas", "human factor", "e-commerc"]}
{"doc": "The heterogeneous Web exacerbates IR problems and short user queries make them worse . The contents of web documents are not enough to find good answer documents . Link information and URL information compensates for the insufficiencies of content information . However , static combination of multiple evidences may lower the retrieval performance . We need different strategies to find target documents according to a query type . We can classify user queries as three categories , the topic relevance task , the homepage finding task , and the service finding task . In this paper , a user query classification scheme is proposed . This scheme uses the difference of distribution , mutual information , the usage rate as anchor texts , and the POS information for the classification . After we classified a user query , we apply different algorithms and information for the better results . For the topic relevance task , we emphasize the content information , on the other hand , for the homepage finding task , we emphasize the Link information and the URL information . We could get the best performance when our proposed classification method with the OKAPI scoring algorithm was used . INTRODUCTION The Web is rich with various sources of information . It contains the contents of documents , web directories , multi-media data , user profiles and so on . The massive and heterogeneous web document collections as well as the unpredictable querying behaviors of typical web searchers exacerbate Information Retrieval (IR) problems . Retrieval approaches based on the single source of evidence suffer from weakness that can hurt the retrieval performance in certain situations 5 . For example , content-based IR approaches have a difficulty in dealing with the diversity in vocabulary and the quality of web documents , while link-based approaches can suffer from an incomplete or noisy link structure . Combining multiple evidences compensates for the weakness of a single evidence 17 . Fusion IR studies have repeatedly shown that combining multiple sources of evidence can improve retrieval performance 5 17 . However , previous studies did not consider a user query in combining evidences 5 7 10 17 . Not only documents in the Web but also users' queries are diverse . For example , for user query `Mutual Information' , if we count on link information too highly , well-known site that has `mutual funds' and `information' as index terms gets the higher rank . For user query `Britney's Fan Club' , if we use content information too highly , yahoo or lycos's web directory pages get the higher rank , instead of the Britney's fan club site . Like these examples , combining content information and link information is not always good . We have to use different strategies to meet the need of a user . User queries can be classified as three categories according to their intent 4 . topic relevance task (informational) homepage finding task (navigational) service finding task (transactional) The topic relevance task is a traditional ad hoc retrieval task where", "label": ["url information", "web document", "url", "improvement", "frequency", "task", "information", "model", "rate", "ir", "combination of multiple evidences", "link information", "query", "query classification"], "stemmed_label": ["url inform", "web document", "url", "improv", "frequenc", "task", "inform", "model", "rate", "ir", "combin of multipl evid", "link inform", "queri", "queri classif"]}
{"doc": "In our research on superimposed information management , we have developed applications where information elements in the superimposed layer serve to annotate , comment , restructure , and combine selections from one or more existing documents in the base layer . Base documents tend to be unstructured or semi-structured (HTML pages , Excel spreadsheets , and so on) with marks delimiting selections . Selections in the base layer can be programmatically accessed via marks to retrieve content and context . The applications we have built to date allow creation of new marks and new superimposed elements (that use marks) , but they have been browse-oriented and tend to expose the line between superimposed and base layers . Here , we present a new access capability , called bi-level queries , that allows an application or user to query over both layers as a whole . Bi-level queries provide an alternative style of data integration where only relevant portions of a base document are mediated (not the whole document) and the superimposed layer can add information not present in the base layer . We discuss our framework for superimposed information management , an initial implementation of a bi-level query system with an XML Query interface , and suggest mechanisms to improve scalability and performance . INTRODUCTION You are conducting background research for a paper you are writing . You have found relevant information in a variety of sources: HTML pages on the web , PDF documents on the web and on your SIGMOD anthology of CDs , Excel spreadsheets and Word documents from your past work in a related area , and so on. You identify relevant portions of the documents and add annotations with clarifications , questions , and conclusions . As you collect information , you frequently reorganize the information you have collected thus far (and your added annotations) to reflect your perspective . You intentionally keep your information structure loose so you can easily move things around . When you have collected sufficient information , you import it , along with your comments , in to a word-processor document . As you write your paper in your word-processor , you revisit your sources to see information in its context . Also , as you write your paper you reorganize its contents , including the imported information , to suit the flow . Occasionally , you search the imported annotations , selections , and the context of the selections. You mix some of the imported information with other information in the paper and transform the mixture to suit presentation needs. Most researchers will be familiar with manual approaches to the scenario we have just described . Providing computer support for this scenario requires a toolset with the following capabilities: 1 . Select portions of documents of many kinds (PDF , HTML, etc.) in many locations (web , CD , local file system , etc.) , and record the selections. 2 . Create and associate annotations (of varying structure) with document selections. 3 . Group and link document selections and annotations, reorganize", "label": ["bi-level queries", "implementation", "system", "superimposed information management", "sparce", "superimposed", "document", "management", "ridpad", "query", "information", "information integration", "metaxpath", "hyperlink"], "stemmed_label": ["bi-level queri", "implement", "system", "superimpos inform manag", "sparc", "superimpos", "document", "manag", "ridpad", "queri", "inform", "inform integr", "metaxpath", "hyperlink"]}
{"doc": "Most of the theoretical work on sampling has addressed the inversion of general traffic properties such as flow size distribution , average flow size , or total number of flows . In this paper , we make a step towards understanding the impact of packet sampling on individual flow properties . We study how to detect and rank the largest flows on a link . To this end , we develop an analytical model that we validate on real traces from two networks . First we study a blind ranking method where only the number of sampled packets from each flow is known . Then , we propose a new method , protocol-aware ranking , where we make use of the packet sequence number (when available in transport header) to infer the number of non-sampled packets from a flow , and hence to improve the ranking . Surprisingly , our analytical and experimental results indicate that a high sampling rate (10% and even more depending on the number of top flows to be ranked) is required for a correct blind ranking of the largest flows . The sampling rate can be reduced by an order of magnitude if one just aims at detecting these flows or by using the protocol-aware method . INTRODUCTION The list of the top users or applications is one of the most useful statistics to be extracted from network traffic. Network operators use the knowledge of the most popular destinations to identify emerging markets and applications or to locate where to setup new Points of Presence . Content delivery networks use the popularity of sites to define caching and replication strategies . In traffic engineering , the identification of heavy hitters in the network can be used to treat and route them differently across the network 20 , 17, 10 . Keeping track of the network prefixes that generate most traffic is also of great importance for anomaly detection . A variation in the pattern of the most common applications may be used as a warning sign and trigger careful inspection of the packet streams. However , the ability to identify the top users in a packet stream is limited by the network monitoring technology. Capturing and processing all packets on high speed links still remains a challenge for today's network equipment 16 , 9 . In this context , a common solution is to sample the packet stream to reduce the load on the monitoring system and to simplify the task of sorting the list of items . The underlying assumption in this approach is that the sampling process does not alter the properties of the data distribution. Sampled traffic data is then used to infer properties of the original data (this operation is called inversion) . The inversion of sampled traffic is , however , an error-prone procedure that often requires a deep study of the data distribution to evaluate how the sampling rate impacts the accuracy of the metric of interest . Although the inversion may be simple for aggregate link statistics (e.g. , to", "label": ["largest flow detection and ranking", "validation with real traces", "packet sampling", "performance evaluation"], "stemmed_label": ["largest flow detect and rank", "valid with real trace", "packet sampl", "perform evalu"]}
{"doc": "Web navigation plays an important role in exploring public interconnected data sources such as life science data . A navigational query in the life science graph produces a result graph which is a layered directed acyclic graph (DAG) . Traversing the result paths in this graph reaches a target object set (TOS) . The challenge for ranking the target objects is to provide recommendations that reflect the relative importance of the retrieved object , as well as its relevance to the specific query posed by the scientist . We present a metric layered graph PageRank (lgPR) to rank target objects based on the link structure of the result graph . LgPR is a modification of PageRank; it avoids random jumps to respect the path structure of the result graph . We also outline a metric layered graph ObjectRank (lgOR) which extends the metric ObjectRank to layered graphs . We then present an initial evaluation of lgPR . We perform experiments on a real-world graph of life sciences objects from NCBI and report on the ranking distribution produced by lgPR . We compare lgPR with PageRank . In order to understand the characteristics of lgPR , an expert compared the Top K target objects (publications in the PubMed source) produced by lgPR and a word-based ranking method that uses text features extracted from an external source (such as Entrez Gene) to rank publications . INTRODUCTION The last few years have seen an explosion in the number of public Web accessible data sources , Web services and semantic Web applications . While this has occurred in many domains , biologists have taken the lead in making life science data public , and biologists spend a considerable amount of time navigating through the contents of these sources , to obtain information that is critical to their research. Providing meaningful answers to queries on life science data sources poses some unique challenges . First , information about a scientific entity , e.g. , genes , proteins , sequences and publications , may be available in a large number of autonomous sources and several sources may provide different descriptions of some entity such as a protein . Second, the links between scientific objects (links between data entries in the different sources) are important in this domain since they capture significant knowledge about the relationship and interactions between these objects . Third , interconnected data entries can be modeled as a large complex graph . Queries could be expressed as regular expression navigational queries and can more richly express a user's needs, compared to simpler keyword based queries. Consider the following navigational query: Retrieve publications related to the gene 'tnf ' that are reached by traversing one intermediate (protein or sequence) entry . This query expresses the scientist's need to expand a search for gene related publications beyond those publications whose text directly addresses the 'tnf' gene , while still limiting the search to publications that are closely linked to gene entries. Consider gene sources OMIM Gene and Entrez Gene , protein sources NCBI Protein and SwissProt ,", "label": ["navigational query", "link analysis", "pagerank", "ranking"], "stemmed_label": ["navig queri", "link analysi", "pagerank", "rank"]}
{"doc": "Vertical search is a promising direction as it leverages domain-specific knowledge and can provide more precise information for users . In this paper , we study the Web object-ranking problem , one of the key issues in building a vertical search engine . More specifically , we focus on this problem in cases when objects lack relationships between different Web communities , and take high-quality photo search as the test bed for this investigation . We proposed two score fusion methods that can automatically integrate as many Web communities (Web forums) with rating information as possible . The proposed fusion methods leverage the hidden links discovered by a duplicate photo detection algorithm , and aims at minimizing score differences of duplicate photos in different forums . Both intermediate results and user studies show the proposed fusion methods are practical and efficient solutions to Web object ranking in cases we have described . Though the experiments were conducted on high-quality photo ranking , the proposed algorithms are also applicable to other ranking problems , such as movie ranking and music ranking INTRODUCTION Despite numerous refinements and optimizations , general purpose search engines still fail to find relevant results for many queries . As a new trend , vertical search has shown promise because it can leverage domain-specific knowledge and is more effective in connecting users with the information they want. There are many vertical search engines, including some for paper search (e.g . Libra 21 , Citeseer 7 and Google Scholar 4 ) , product search (e.g . Froogle 5 ) , movie search 6 , image search 1 , 8 , video search 6 , local search 2 , as well as news search 3 . We believe the vertical search engine trend will continue to grow. Essentially , building vertical search engines includes data crawling , information extraction , object identification and integration , and object-level Web information retrieval (or Web object ranking) 20 , among which ranking is one of the most important factors . This is because it deals with the core problem of how to combine and rank objects coming from multiple communities. Although object-level ranking has been well studied in building vertical search engines , there are still some kinds of vertical domains in which objects cannot be effectively ranked . For example , algorithms that evolved from PageRank 22 , PopRank 21 and LinkFusion 27 were proposed to rank objects coming from multiple communities , but can only work on well-defined graphs of heterogeneous data. \"Well-defined\" means that like objects (e.g . authors in paper search) can be identified in multiple communities (e.g. conferences) . This allows heterogeneous objects to be well linked to form a graph through leveraging all the relationships (e.g . cited-by , authored-by and published-by) among the multiple communities. However , this assumption does not always stand for some domains . High-quality photo search , movie search and news search are exceptions . For example , a photograph forum website usually includes three kinds of objects: photos , authors and reviewers. Yet", "label": ["image search", "ranking", "web objects"], "stemmed_label": ["imag search", "rank", "web object"]}
{"doc": "While users disseminate various information in the open and widely distributed environment of the Semantic Web , determination of who shares access to particular information is at the center of looming privacy concerns . We propose a real-world -oriented information sharing system that uses social networks . The system automatically obtains users' social relationships by mining various external sources . It also enables users to analyze their social networks to provide awareness of the information dissemination process . Users can determine who has access to particular information based on the social relationships and network analysis . INTRODUCTION With the current development of tools and sites that enable users to create Web content , users have become able to easily disseminate various information . For example , users create Weblogs , which are diary-like sites that include various public and private information . Furthermore , the past year has witnessed the emergence of social networking sites that allow users to maintain an online network of friends or associates for social or business purposes . Therein , data related to millions of people and their relationships are publicly available on the Web. Although these tools and sites enable users to easily disseminate information on the Web , users sometimes have difficulty in sharing information with the right people and frequently have privacy concerns because it is difficult to determine who has access to particular information on such applications . Some tools and applications provide control over information access . For example , Friendster , a huge social networking site , offers several levels of control from \"public information\" to \"only for friends\" . However , it provides only limited support for access control. An appropriate information sharing system that enables all users to control the dissemination of their information is needed to use tools and sites such as Weblog , Wiki , and social networking services fully as an infrastructure of disseminating and sharing information . In the absence of such a system , a user would feel unsafe and would therefore be discouraged from disseminating information. How can we realize such an information sharing system on the Web? One clue exists in the information sharing processes of the real world . Information availability is often closely guarded and shared only with the people of one's social relationships . Confidential project documents which have limited distribution within a division of company , might be made accessible to other colleagues who are concerned with the project . Private family photographs might be shared not only with relatives , but also with close friends . A professor might access a private research report of her student. We find that social relationships play an important role in the process of disseminating and receiving information . This paper presents a real-world oriented information sharing system using social networks . It enables users to control the information dissemination process within social networks. The remainder of this paper is organized as follows: section 2 describes the proposed information sharing system using social networks . In section 3 , we describe", "label": ["social network", "information sharing"], "stemmed_label": ["social network", "inform share"]}
{"doc": "Enterprises in the public and private sectors have been making their large spatial data archives available over the Internet . However , interactive work with such large volumes of online spatial data is a challenging task . We propose two efficient approaches to remote access to large spatial data . First , we introduce a client-server architecture where the work is distributed between the server and the individual clients for spatial query evaluation , data visualization , and data management . We enable the minimization of the requirements for system resources on the client side while maximizing system responsiveness as well as the number of connections one server can handle concurrently . Second , for prolonged periods of access to large online data , we introduce APPOINT (an Approach for Peer-to-Peer Offloading the INTernet) . This is a centralized peer-to-peer approach that helps Internet users transfer large volumes of online data efficiently . In APPOINT , active clients of the client-server architecture act on the server's behalf and communicate with each other to decrease network latency , improve service bandwidth , and resolve server congestions . INTRODUCTION In recent years , enterprises in the public and private sectors have provided access to large volumes of spatial data over the Internet . Interactive work with such large volumes of online spatial data is a challenging task . We have been developing an interactive browser for accessing spatial online databases: the SAND (Spatial and Non-spatial Data) Internet Browser . Users of this browser can interactively and visually manipulate spatial data remotely . Unfortunately, interactive remote access to spatial data slows to a crawl without proper data access mechanisms . We developed two separate methods for improving the system performance , together , form a dynamic network infrastructure that is highly scalable and provides a satisfactory user experience for interactions with large volumes of online spatial data. The core functionality responsible for the actual database operations is performed by the server-based SAND system. SAND is a spatial database system developed at the University of Maryland 12 . The client-side SAND Internet Browser provides a graphical user interface to the facilities of SAND over the Internet . Users specify queries by choosing the desired selection conditions from a variety of menus and dialog boxes. SAND Internet Browser is Java-based , which makes it deployable across many platforms . In addition , since Java has often been installed on target computers beforehand , our clients can be deployed on these systems with little or no need for any additional software installation or customiza-tion . The system can start being utilized immediately without any prior setup which can be extremely beneficial in time-sensitive usage scenarios such as emergencies. There are two ways to deploy SAND . First , any standard Web browser can be used to retrieve and run the client piece (SAND Internet Browser) as a Java application or an applet. This way , users across various platforms can continuously access large spatial data on a remote location with little or 1 5 no need for any", "label": ["gis", "client/server", "peer-to-peer", "internet"], "stemmed_label": ["gi", "client/serv", "peer-to-p", "internet"]}
{"doc": "An increasing amount of heterogeneous information about scientific research is becoming available on-line . This potentially allows users to explore the information from multiple perspectives and derive insights and not just raw data about a topic of interest . However , most current scientific information search systems lag behind this trend; being text-based , they are fundamentally incapable of dealing with multimedia data . An even more important limitation is that their information environments are information-centric and therefore are not suitable if insights are desired . Towards this goal , in this paper , we describe the design of a system , called ResearchExplorer , which facilitates exploring multimedia scientific data to gain insights . This is accomplished by providing an interaction environment for insights where users can explore multimedia scientific information sources . The multimedia information is united around the notion of research event and can be accessed in a unified way . Experiments are conducted to show how ResearchExplorer works and how it cardinally differs from other search systems . INTRODUCTION Current web search engines and bibliography systems are information-centric . Before searching for information , users need to construct a query typically , by using some keywords to represent the information they want . After the query is issued , the system retrieves all information relevant to the query . The results from such queries are usually presented to users by listing all relevant hits . Thus , with these information-centric systems , users can find information such as a person's homepage , a paper , a research project's web page , and so on . However , when users want to know the following types of things , they are unable to find answers easily with current search systems: 1) Evolution of a field 2) People working in the field 3) A person's contribution to the field 4) Classical papers (or readings) in the field 5) Conferences/journals in the field 6) How the research of a person or an organization (group, dept , university , etc) has evolved. The reasons why current information-centric search systems have difficulty to help users to find answers to questions above are due to the limitations of their information environments. First , some issues result from their data modeling . For example , to answer the question of \"evolution of a field\" , the most important information components , which are time and location , need to be captured and appropriately presented or utilized . However , in typical bibliography systems such information is rigidly utilized (if at all available) in the time-stamping sense. Second , many important issues arise due to the presentation methods utilized by such systems . For example , even though users can find all papers of a person with some systems , it is not easy for users to observe the trend if the results are just listed sequentially . As an alternative , presenting results in a visual form can make trend easier to identify. Third , some of the questions listed above can not be answered", "label": ["event", "research event", "multimedia data", "spatio-temporal data", "exploration", "interaction environment", "insight"], "stemmed_label": ["event", "research event", "multimedia data", "spatio-tempor data", "explor", "interact environ", "insight"]}
{"doc": "Cognitive information complexity measure is based on cognitive informatics , which helps in comprehending the software characteristics . For any complexity measure to be robust , Weyuker properties must be satisfied to qualify as good and comprehensive one . In this paper , an attempt has also been made to evaluate cognitive information complexity measure in terms of nine Weyuker properties , through examples . It has been found that all the nine properties have been satisfied by cognitive information complexity measure and hence establishes cognitive information complexity measure based on information contained in the software as a robust and well-structured one . Introduction Many well known software complexity measures have been proposed such as McCabe's cyclomatic number 8 , Halstead programming effort 5 , Oviedo's data flow complexity measures 9 , Basili's measure 3 4 , Wang's cognitive complexity measure 11 and others 7 . All the reported complexity measures are supposed to cover the correctness , effectiveness and clarity of software and also to provide good estimate of these parameters. Out of the numerous proposed measures , selecting a particular complexity measure is again a problem , as every measure has its own advantages and disadvantages . There is an ongoing effort to find such a comprehensive complexity measure , which addresses most of the parameters of software . Weyuker 14 has suggested nine properties , which are used to determine the effectiveness of various software complexity measures . A good complexity measure should satisfy most of the Weyuker's properties . A new complexity measure based on weighted information count of a software and cognitive weights has been developed by Kushwaha and Misra 2 . In this paper an effort has been made to estimate this cognitive information complexity measure as robust and comprehensive one by evaluating this against the nine Weyuker's properties. Cognitive Weights of a Software Basic control structures BCS such as sequence , branch and iteration 10 13 are the basic logic building blocks of any software and the cognitive weights (W c ) of a software 11 is the extent of difficulty or relative time and effort for comprehending a given software modeled by a number of BCS's . These cognitive weights for BCS's measure the complexity of logical structures of the software . Either all the BCS's are in a linear layout or some BCS's are embedded in others . For the former case , we sum the weights of all the BCS's and for the latter , cognitive weights of inner BCS's are multiplied with the weight of external BCS's. Cognitive Information Complexity Measure (CICM) Since software represents computational information and is a mathematical entity , the amount of information contained in the software is a function of the identifiers that hold the information and the operators that perform the operations on the information i.e. Information = f (Identifiers , Operators) Identifiers are variable names , defined constants and other labels in a software . Therefore information contained in one line of code is the number of all operators and operands in that line", "label": ["cognitive weight", "cognitive information complexity measure", "basic control structures", "cognitive information complexity unit", "weighted information count"], "stemmed_label": ["cognit weight", "cognit inform complex measur", "basic control structur", "cognit inform complex unit", "weight inform count"]}
{"doc": "The emergence of Bluetooth as a default radio interface allows handheld devices to be rapidly interconnected into ad hoc networks . Bluetooth allows large numbers of piconets to form a scatternet using designated nodes that participate in multiple piconets . A unit that participates in multiple piconets can serve as a bridge and forwards traffic between neighbouring piconets . Since a Bluetooth unit can transmit or receive in only one piconet at a time , a bridging unit has to share its time among the different piconets . To schedule communication with bridging nodes one must take into account their availability in the different piconets , which represents a difficult , scatternet wide coordination problem and can be an important performance bottleneck in building scatternets . In this paper we propose the Pseudo-Random Coordinated Scatternet Scheduling (PCSS) algorithm to perform the scheduling of both intra and inter-piconet communication . In this algorithm Bluetooth nodes assign meeting points with their peers such that the sequence of meeting points follows a pseudo random process that is different for each pair of nodes . The uniqueness of the pseudo random sequence guarantees that the meeting points with different peers of the node will collide only occasionally . This removes the need for explicit information exchange between peer devices , which is a major advantage of the algorithm . The lack of explicit signaling between Bluetooth nodes makes it easy to deploy the PCSS algorithm in Bluetooth devices , while conformance to the current Bluetooth specification is also maintained . To assess the performance of the algorithm we define two reference case schedulers and perform simulations in a number of scenarios where we compare the performance of PCSS to the performance of the reference schedulers . INTRODUCTION Short range radio technologies enable users to rapidly interconnect handheld electronic devices such as cellular phones , palm devices or notebook computers . The emergence of Bluetooth 1 as default radio interface in these devices provides an opportunity to turn them from stand-alone tools into networked equipment . Building Bluetooth ad hoc networks also represents , however , a number of new challenges , partly stemming from the fact that Bluetooth was originally developed for single hop wireless connections . In this paper we study the scheduling problems of inter-piconet communication and propose a lightweight scheduling algorithm that Bluetooth nodes can employ to perform the scheduling of both intra and inter-piconet communication. Bluetooth is a short range radio technology operating in the unlicensed ISM (Industrial-Scientific-Medical) band using a frequency hopping scheme . Bluetooth (BT) units are organized into piconets. There is one Bluetooth device in each piconet that acts as the master , which can have any number of slaves out of which up to seven can be active simultaneously . The communication within a piconet is organized by the master which polls each slave according to some polling scheme . A slave is only allowed to transmit in a slave-to -master slot if it has been polled by the master in the previous master-to-slave slot . In Section", "label": ["checkpoint", "total utilization", "piconets", "threshold", "scatternet", "pcss algorithm", "bluetooth", "slaves", "inter-piconet communication", "scheduling", "intensity", "network access point", "bridging unit"], "stemmed_label": ["checkpoint", "total util", "piconet", "threshold", "scatternet", "pcss algorithm", "bluetooth", "slave", "inter-piconet commun", "schedul", "intens", "network access point", "bridg unit"]}
{"doc": "From experience with wireless sensor networks it has become apparent that dynamic reprogramming of the sensor nodes is a useful feature . The resource constraints in terms of energy , memory , and processing power make sensor network reprogramming a challenging task . Many different mechanisms for reprogramming sensor nodes have been developed ranging from full image replacement to virtual machines . We have implemented an in-situ run-time dynamic linker and loader that use the standard ELF object file format . We show that run-time dynamic linking is an effective method for reprogramming even resource constrained wireless sensor nodes . To evaluate our dynamic linking mechanism we have implemented an application-specific virtual machine and a Java virtual machine and compare the energy cost of the different linking and execution models . We measure the energy consumption and execution time overhead on real hardware to quantify the energy costs for dynamic linking . Our results suggest that while in general the overhead of a virtual machine is high , a combination of native code and virtual machine code provide good energy efficiency . Dynamic run-time linking can be used to update the native code , even in heterogeneous networks . Introduction Wireless sensor networks consist of a collection of programmable radio-equipped embedded systems . The behavior of a wireless sensor network is encoded in software running on the wireless sensor network nodes . The software in deployed wireless sensor network systems often needs to be changed , both to update the system with new functionality and to correct software bugs . For this reason dynamically reprogramming of wireless sensor network is an important feature . Furthermore , when developing software for wireless sensor networks , being able to update the software of a running sensor network greatly helps to shorten the development time. The limitations of communication bandwidth , the limited energy of the sensor nodes , the limited sensor node memory which typically is on the order of a few thousand bytes large, the absence of memory mapping hardware , and the limited processing power make reprogramming of sensor network nodes challenging. Many different methods for reprogramming sensor nodes have been developed , including full system image replacement 14 , 16 , approaches based on binary differences 15, 17 , 31 , virtual machines 18 , 19 , 20 , and loadable native code modules in the first versions of Contiki 5 and SOS 12 . These methods are either inefficient in terms of energy or require non-standard data formats and tools. The primary contribution of this paper is that we investigate the use of standard mechanisms and file formats for reprogramming sensor network nodes . We show that in-situ dynamic run-time linking and loading of native code using the ELF file format , which is a standard feature on many operating systems for PC computers and workstations , is feasible even for resource-constrained sensor nodes . Our secondary contribution is that we measure and quantify the energy costs of dynamic linking and execution of native code and compare it to the", "label": ["wireless sensor networks", "embedded systems", "dynamic linking", "operating systems", "virtual machines"], "stemmed_label": ["wireless sensor network", "embed system", "dynam link", "oper system", "virtual machin"]}
{"doc": "Sensor network computing can be characterized as resource-constrained distributed computing using unreliable , low bandwidth communication . This combination of characteristics poses significant software development and maintenance challenges . Effective and efficient debugging tools for sensor network are thus critical . Existent development tools , such as TOSSIM , EmStar , ATEMU and Avrora , provide useful debugging support , but not with the fidelity , scale and functionality that we believe are sufficient to meet the needs of the next generation of applications . In this paper , we propose a debugger , called S2DB , based on a distributed full system sensor network simulator with high fidelity and scalable performance , DiSenS . By exploiting the potential of DiSenS as a scalable full system simulator , S2DB extends conventional debugging methods by adding novel device level , program source level , group level , and network level debugging abstractions . The performance evaluation shows that all these debugging features introduce overhead that is generally less than 10% into the simulator and thus making S2DB an efficient and effective debugging tool for sensor networks . INTRODUCTION Sensor networks , comprised of tiny resource-constrained devices connected by short range radios and powered by batteries , provide an innovative way to implement pervasive and non-intrusive envi-ronmental instrumentation and (potentially) actuation . The resource-constrained nature of sensor network devices poses significant software development and maintenance challenges . To prolong battery life and promote miniaturization , most devices have little memory, use low-power and unreliable radios , and run long duty cycles . In addition to these per-device constraints , by definition sensor networks are also distributed systems , with all of the concomitant synchronization and consistency concerns that distributed coordination implies. For these reasons , effective debugging support is critical . A number of sensor network development systems 2 , 18 , 3 , 17 , 13 , 6 provide debugging support for individual devices and/or the complete network . However , they all have their limitations . Some rely on hardware support , subject to the same resource constraints that as the programs on which they operate . Some only monitor the network radio traffic . And most importantly , as networks scale , these tools become difficult to apply to the details of collections of interacting sensor nodes. In this paper , we present a new approach that is based on scalable full system sensor network simulation with enhanced debugging features . Our debugging tool is called S 2 DB (where S 2 stands for Simulation and Sensor network) . The goal of S 2 DB is to adapt conventional debugging methods to sensor network applications so that we can have better control of hardware details and debug the complete sensor network in a coordinated way . Our approach relies upon four principle innovations in the area of debugging resource constrained devices. At the single device level , we introduce the concept of debugging point a generalized notion of break point , watch point, and state interrogation that permits state display from", "label": ["sensor network", "simulation", "debugging"], "stemmed_label": ["sensor network", "simul", "debug"]}
{"doc": "Computing and maintaining network structures for efficient data aggregation incurs high overhead for dynamic events where the set of nodes sensing an event changes with time . Moreover , structured approaches are sensitive to the waiting-time which is used by nodes to wait for packets from their children before forwarding the packet to the sink . Although structure-less approaches can address these issues , the performance does not scale well with the network size . We propose a semi-structured approach that uses a structure-less technique locally followed by Dynamic Forwarding on an implicitly constructed packet forwarding structure to support network scalability . The structure , ToD , is composed of multiple shortest path trees . After performing local aggregation , nodes dynamically decide the forwarding tree based on the location of the sources . The key principle behind ToD is that adjacent nodes in a graph will have low stretch in one of these trees in ToD , thus resulting in early aggregation of packets . Based on simulations on a 2000 nodes network and real experiments on a 105 nodes Mica2-based network , we conclude that efficient aggregation in large scale networks can be achieved by our semi-structured approach . Introduction Data aggregation is an effective technique for conserving communication energy in sensor networks . In sensor networks , the communication cost is often several orders of magnitude larger than the computation cost . Due to inherent redundancy in raw data collected from sensors , in-network data aggregation can often reduce the communication cost by eliminating redundancy and forwarding only the extracted information from the raw data . As reducing consumption of communication energy extends the network lifetime , it is critical for sensor networks to support in-network data aggregation . Various data aggregation approaches have been proposed for data gathering applications and event-based applications. These approaches make use of cluster based structures 1 , 2 or tree based structures 38 . In data gathering applications, such as environment and habitat monitoring 912 , nodes periodically report the sensed data to the sink . As the traffic pattern is unchanging , these structure-based approaches incur low maintenance overhead and are therefore suitable for such applications . However , in event-based applications, such as intrusion detection 13 , 14 and biological hazard detection 15 , the source nodes are not known in advance. Therefore the approaches that use fixed structures can not efficiently aggregate data , while the approaches that change the structure dynamically incur high maintenance overhead 4 , 5 . The goal of this paper is to design a scalable and efficient data aggregation protocol that incurs low maintenance overhead and is suited for event-based applications. Constructing an optimal structure for data aggregation for various aggregation functions has been proven to be an NP-hard problem 16 , 17 . Although heuristics can be used to construct structures for data aggregation , another problem associated with the convergecast traffic pattern , where nodes transmit their packets to the cluster-head or parent in cluster or tree structures , results in low performance", "label": ["tod", "structure-free", "anycasting", "data aggregation"], "stemmed_label": ["tod", "structure-fre", "anycast", "data aggreg"]}
{"doc": "Mining frequent structural patterns from graph databases is an interesting problem with broad applications . Most of the previous studies focus on pruning unfruitful search subspaces effectively , but few of them address the mining on large , disk-based databases . As many graph databases in applications cannot be held into main memory , scalable mining of large , disk-based graph databases remains a challenging problem . In this paper , we develop an effective index structure , ADI (for adjacency index) , to support mining various graph patterns over large databases that cannot be held into main memory . The index is simple and efficient to build . Moreover , the new index structure can be easily adopted in various existing graph pattern mining algorithms . As an example , we adapt the well-known gSpan algorithm by using the ADI structure . The experimental results show that the new index structure enables the scalable graph pattern mining over large databases . In one set of the experiments , the new disk-based method can mine graph databases with one million graphs , while the original gSpan algorithm can only handle databases of up to 300 thousand graphs . Moreover , our new method is faster than gSpan when both can run in main memory . INTRODUCTION Mining frequent graph patterns is an interesting research problem with broad applications , including mining structural patterns from chemical compound databases , plan databases , XML documents , web logs , citation networks, and so forth . Several efficient algorithms have been proposed in the previous studies 2 , 5 , 6 , 8 , 11 , 9 , ranging from mining graph patterns , with and without constraints, to mining closed graph patterns. Most of the existing methods assume implicitly or explic-itly that the databases are not very large , and the graphs in the database are relatively simple . That is , either the databases or the major part of them can fit into main memory , and the number of possible labels in the graphs 6 is small . For example , 11 reports the performance of gSpan, an efficient frequent graph pattern mining algorithm , on data sets of size up to 320 KB , using a computer with 448 MB main memory. Clearly , the graph database and the projected databases can be easily accommodated into main memory. Under the large main memory assumption , the computation is CPU-bounded instead of I/O-bounded . Then , the algorithms focus on effective heuristics to prune the search space . Few of them address the concern of handling large graph databases that cannot be held in main memory. While the previous studies have made excellent progress in mining graph databases of moderate size , mining large, disk-based graph databases remains a challenging problem. When mining a graph database that cannot fit into main memory , the algorithms have to scan the database and navigate the graphs repeatedly . The computation becomes I/O-bounded . For example , we obtain the executable of gSpan from the", "label": ["index", "edge table", "graph mining", "subgraph mine", "frequent graph pattern mining", "adjacency list representation", "graph database", "dfs code", "adi index structure", "frequent graph pattern", "gspan algorithm", "disk bases databases", "graph databases", "memory based databases"], "stemmed_label": ["index", "edg tabl", "graph mine", "subgraph mine", "frequent graph pattern mine", "adjac list represent", "graph databas", "df code", "adi index structur", "frequent graph pattern", "gspan algorithm", "disk base databas", "graph databas", "memori base databas"]}
{"doc": "The IP Multimedia Subsystem (IMS) defined by Third Generation Partnership Projects (3GPP and 3GPP2) is a technology designed to provide robust multimedia services across roaming boundaries and over diverse access technologies with promising features like quality-of-service (QoS) , reliability and security . The IMS defines an overlay service architecture that merges the paradigms and technologies of the Internet with the cellular and fixed telecommunication worlds . Its architecture enables the efficient provision of an open set of potentially highly integrated multimedia services , combining web browsing , email , instant messaging , presence , VoIP , video conferencing , application sharing , telephony , unified messaging , multimedia content delivery , etc . on top of possibly different network technologies . As such IMS enables various business models for providing seamless business and consumer multimedia applications . In this communication converged world , the challenging issues are security , quality of service (QoS) and management & administration . In this paper our focus is to manage secure access to multimedia services and applications based on SIP and HTTP on top of IP Multimedia Subsystem (IMS) . These services include presence , video conferencing , messaging , video broadcasting , and push to talk etc . We will utilize Generic Bootstrapping Architecture (GBA) model to authenticate multimedia applications before accessing these multimedia services offered by IMS operators . We will make enhancement in GBA model to access these services securely by introducing Authentication Proxy (AP) which is responsible to implement Transport Layer Security (TLS) for HTTP and SIP communication . This research work is part of Secure Service Provisioning (SSP) Framework for IP Multimedia System at Fokus Fraunhofer IMS 3Gb Testbed . Introduction With the emergence of mobile multimedia services , such as unified messaging , click to dial , across network multiparty conferencing and seamless multimedia streaming services , the convergence of networks (i.e . fixedmobile convergence and voicedata integration) has started , leading to an overall Internet Telecommunications convergence . In prospect of these global trends , the mobile communications world has defined within the evolution of cellular systems an All-IP Network vision which integrates cellular networks and the Internet . This is the IP Multimedia System (IMS) 1 , namely overlay architecture for the provision of multimedia services , such as VoIP (Voice over Internet Protocol) and videoconferencing on top of globally emerging 3G (Third Generation) broadband packet networks . The IP Multimedia System (IMS) which is standardized by Third Generation Partnership Project (3GPP & 3GGP2) in releases 5 is an overlay network on top of GPRS/UMTS (General Packet Radio Systems/Universal Mobile Telecommunication Systems) networks and extended by ETSI TISPAN 2 for fixed line access network within the Next Generation Network (NGN) architecture. The IMS provides all IP Service Delivery Platform (SDP) for mobile multimedia services provisioning e.g . VoIP , Video-telephony , Multimedia conferencing , Mobile Content , Push-to-Talk etc . and it is based on IETF protocols like SIP for session control , Diameter for AAA (Authentication , Authorization , and Auditing) and SDP (Service Delivery Protocol) ,", "label": ["tls tunnel end points", "generic authentication architecture", "glms/xdms", "general bootstrapping architecture", "transport layer security", "network authentication function", "signalling protocols", "generic bootstrapping architecture", "authentication proxy", "gba", "diameter proxy", "transport layer security", "ip multimedia system", "authentication proxy", "tls", "ip multimedia subsystem", "ims platform", "fokus ims testbed", "naf", "ap", "security and privacy"], "stemmed_label": ["tl tunnel end point", "gener authent architectur", "glms/xdm", "gener bootstrap architectur", "transport layer secur", "network authent function", "signal protocol", "gener bootstrap architectur", "authent proxi", "gba", "diamet proxi", "transport layer secur", "ip multimedia system", "authent proxi", "tl", "ip multimedia subsystem", "im platform", "foku im testb", "naf", "ap", "secur and privaci"]}
{"doc": "In-network aggregation is an essential primitive for performing queries on sensor network data . However , most aggregation algorithms assume that all intermediate nodes are trusted . In contrast , the standard threat model in sensor network security assumes that an attacker may control a fraction of the nodes , which may misbehave in an arbitrary (Byzantine) manner . We present the first algorithm for provably secure hierarchical in-network data aggregation . Our algorithm is guaranteed to detect any manipulation of the aggregate by the adversary beyond what is achievable through direct injection of data values at compromised nodes . In other words , the adversary can never gain any advantage from misrepresenting intermediate aggregation computations . Our algorithm incurs only O(log2n) node congestion , supports arbitrary tree-based aggregator topologies and retains its resistance against aggregation manipulation in the presence of arbitrary numbers of malicious nodes . The main algorithm is based on performing the SUM aggregation securely by first forcing the adversary to commit to its choice of intermediate aggregation results , and then having the sensor nodes independently verify that their contributions to the aggregate are correctly incorporated . We show how to reduce secure MEDIAN , COUNT , and AVERAGE to this primitive . INTRODUCTION Wireless sensor networks are increasingly deployed in security-critical applications such as factory monitoring , environmental monitoring , burglar alarms and fire alarms . The sensor nodes for these applications are typically deployed in unsecured locations and are not made tamper-proof due to cost considerations . Hence , an adversary could undetectably take control of one or more sensor nodes and launch active attacks to subvert correct network operations. Such environments pose a particularly challenging set of constraints for the protocol designer: sensor network protocols must be highly energy efficient while being able to function securely in the presence of possible malicious nodes within the network. In this paper we focus on the particular problem of securely and efficiently performing aggregate queries (such as MEDIAN , SUM and AVERAGE ) on sensor networks . In-network data aggregation is an efficient primitive for reducing the total message complexity of aggregate sensor queries . For example , in-network aggregation of the SUM function is performed by having each intermediate node forward a single message containing the sum of the sensor readings of all the nodes downstream from it , rather than forwarding each downstream message one-by-one to the base station . The energy savings of performing in-network aggregation have been shown to be significant and are crucial for energy-constrained sensor networks 9 , 11 , 20 . Unfortunately , most in-network aggregation schemes assume that all sensor nodes are trusted 12 , 20 . An adversary controlling just a few aggregator nodes could potentially cause the sensor network to return arbitrary results , thus completely subverting the function of the network to the adversary's own purposes. Despite the importance of the problem and a significant amount of work on the area , the known approaches to secure aggregation either require strong assumptions about network topology or adversary", "label": ["algorithm", "secure aggregation", "commitment forest", "in-network data aggregation", "commitment tree", "sensor networks", "secure hierarchical data aggregation protocol", "sensor network", "aggregation commit", "result checking", "query dissemination", "congestion complexity", "data aggregation"], "stemmed_label": ["algorithm", "secur aggreg", "commit forest", "in-network data aggreg", "commit tree", "sensor network", "secur hierarch data aggreg protocol", "sensor network", "aggreg commit", "result check", "queri dissemin", "congest complex", "data aggreg"]}
{"doc": "The use of middleware eases the development of distributed applications by abstracting the intricacies (communication and coordination among software components) of the distributed network environment . In wireless sensor networks , this is even trickier because of their specific issues such as addressing , mobility , number of sensors and energy-limited nodes . This paper describes SensorBus , a message-oriented middleware (MOM) model for wireless sensor networks based on the publish-subscribe paradigm and that allows the free exchange of the communication mechanism among sensor nodes allowing as result the capability of using more than one communication mechanism to address the requirements of larger number of applications . We intend to provide a platform which addresses the main characteristics of wireless sensor networks and also allows the development of energy-efficient applications . SensorBus incorporates constraint and query languages which will aid the development of interactive applications . It intends with the utilization of filters reduces data movement minimizing the energy consumption of nodes . INTRODUCTION Recent advances in wireless networking technology , low-power digital circuits , sensing materials and Micro Electro-Mechanical Systems (MEMS) opened up the possibility of building small sensor devices capable of data processing , remote sensing and wireless communication . When several small sensors are scattered and linked over an area we may call this arrangement a \"Sensor Network\" . These networks can be used for collecting and analyzing data from the physical environment . More specifically, sensor networks are comprised of hundreds or even thousands of heterogeneous sensor nodes exchanging information to perform distributed sensing and collaborative data processing 1 . From a functional perspective sensor networks behave like distributed systems with many different types of sensor nodes. Given the diversity of node functionality and the size of these networks it is important for a user to be able to program and manage the distributed applications that perform the information gathering . A programmer may develop these applications using operating system primitives . This kind of procedure , however, brings another level of complexity to the programmer , in which he not only has to deal with low-level primitives but he will also have to treat issues concerning communication and coordination among software components distributed over the network . A much friendlier approach is the utilization of a middleware in order to provide higher-level primitives to hide issues concerning the distributed environment. Traditional middleware is not suited to this task because of the characteristics of wireless networks . For example , conventional middleware designed for wired networks raises exceptions when they do not find a specific component , but this situation is much more like the standard than the exception in wireless environments . The lower bandwidth available for wireless networks requires optimizing the transport of data and this is not considered in conventional middleware . The coordination primitives of these middleware products do not take into account the frequent disconnections that happen in wireless networks. Another problem is the size and computing requirements of these middleware products; they are often too large and too heavy to be running", "label": ["message service", "publish-subscribe paradigm", "message-oriented middleware model", "environmental monitoring applications", "application filters", "context service", "middleware", "constraint and query languages", "design pattern", "wireless sensor networks", "application service", "wireless sensor network"], "stemmed_label": ["messag servic", "publish-subscrib paradigm", "message-ori middlewar model", "environment monitor applic", "applic filter", "context servic", "middlewar", "constraint and queri languag", "design pattern", "wireless sensor network", "applic servic", "wireless sensor network"]}
{"doc": "We investigate the design space of sensor network broadcast authentication . We show that prior approaches can be organized based on a taxonomy of seven fundamental proprieties , such that each approach can satisfy at most six of the seven proprieties . An empirical study of the design space reveals possibilities of new approaches , which we present in the following two new authentication protocols : RPT and LEA . Based on this taxonomy , we offer guidance in selecting the most appropriate protocol based on an application's desired proprieties . Finally , we pose the open challenge for the research community to devise a protocol simultaneously providing all seven properties . INTRODUCTION Due to the nature of wireless communication in sensor networks, attackers can easily inject malicious data messages or alter the content of legitimate messages during multihop forwarding . Sensor network applications thus need to rely on authentication mechanisms to ensure that data from a valid source was not altered in transit . Authentication is thus arguably the most important security primitive in sensor network communication . Source authentication ensures a receiver that the message originates from the claimed sender , and data authentication ensures that the data from that sender was unchanged (thus also providing message integrity). When we use the term authentication we mean both source and data authentication. Broadcast authentication is a challenging problem . Furthermore, it is of central importance as broadcasts are used in many applications . For example , routing tree construction , network query , software updates , time synchronization , and network management all rely on broadcast . Without an efficient broadcast authentication algorithm , the base station would have to resort to per-node unicast messages , which does not scale to large networks . The practical-ity of many secure sensor network applications thus hinges on the presence of an efficient algorithm for broadcast authentication. In point-to-point authentication , authentication can be achieved through purely symmetric means: the sender and receiver would share a secret key used to compute a cryptographic message authentication code (MAC) over each message 15 , 23 . When a message with a valid MAC is received , the receiver can be assured that the message originated from the sender . Researchers showed that MACs can be efficiently implemented on resource-constrained sensor network nodes 31 , and find that computing a MAC function requires on the order of 1ms on the computation-constrained Berkeley mote platform 11 , 14 . Authentication of broadcast messages in sensor networks is much harder than point-to-point authentication 1 . The symmetric approach used in point-to-point authentication is not secure in broadcast settings , where receivers are mutually untrusted . If all nodes share one secret key , any compromised receiver can forge messages from the sender. In fact , authenticated broadcast requires an asymmetric mechanism 1 . The traditional approach for asymmetric mechanisms is to use digital signatures , for example the RSA signature 34 . Unfortunately , asymmetric cryptographic mechanisms have high computation , communication , and storage overhead , making their", "label": ["sensor network", "broadcast authentication", "taxonomy"], "stemmed_label": ["sensor network", "broadcast authent", "taxonomi"]}
{"doc": "Many methods for classification and gene selection with microarray data have been developed . These methods usually give a ranking of genes . Evaluating the statistical significance of the gene ranking is important for understanding the results and for further biological investigations , but this question has not been well addressed for machine learning methods in existing works . Here , we address this problem by formulating it in the framework of hypothesis testing and propose a solution based on resampling . The proposed r-test methods convert gene ranking results into position p-values to evaluate the significance of genes . The methods are tested on three real microarray data sets and three simulation data sets with support vector machines as the method of classification and gene selection . The obtained position p-values help to determine the number of genes to be selected and enable scientists to analyze selection results by sophisticated multivariate methods under the same statistical inference paradigm as for simple hypothesis testing methods . INTRODUCTION AN important application of DNA microarray technologies in functional genomics is to classify samples according to their gene expression profiles , e.g. , to classify cancer versus normal samples or to classify different types or subtypes of cancer . Selecting genes that are informative for the classification is one key issue for understanding the biology behind the classification and an important step toward discovering those genes responsible for the distinction . For this purpose , researchers have applied a number of test statistics or discriminant criteria to find genes that are differentially expressed between the investigated classes 1 , 2 , 3 , 4 , 5 , 6 , 7 . This category of gene selection methods is usually referred to as the filtering method since the gene selection step usually plays the role of filtering the genes before doing classification with some other methods. Another category of methods is the so-called wrapper methods , which use the classification performance itself as the criterion for selecting the genes and genes are usually selected in a recursive fashion 8 , 9 , 10 , 11 , 12 . A representative method of this category is SVM-RFE based on support vector machines (SVM) , which uses linear SVM to classify the samples and ranks the contribution of the genes in the classifier by their squared weights 10 . All these selection methods produce rankings of the genes . When a test statistic , such as the t-test , F-test , or bootstrap test , is used as the criterion , the ranking is attached by p-values derived from the null distribution of the test statistic , which reflects the probability of a gene showing the observed difference between the classes simply due to chance . Such p-values give biologists a clear understanding of the information that the genes probably contain . The availability of the p-value makes it possible to investigate the microarray data under the solid framework of statistical inference and many theoretical works have been built based on the extension of the concept of p-value,", "label": ["significance of gene ranking", "gene selection", "microarray data analysis", "classification"], "stemmed_label": ["signific of gene rank", "gene select", "microarray data analysi", "classif"]}
{"doc": "The contour tree , an abstraction of a scalar field that encodes the nesting relationships of isosurfaces , can be used to accelerate isosurface extraction , to identify important isovalues for volume-rendering transfer functions , and to guide exploratory visualization through a flexible isosurface interface . Many real-world data sets produce unmanageably large contour trees which require meaningful simplification . We define local geometric measures for individual contours , such as surface area and contained volume , and provide an algorithm to compute these measures in a contour tree . We then use these geometric measures to simplify the contour trees , suppressing minor topological features of the data . We combine this with a flexible isosurface interface to allow users to explore individual contours of a dataset interactively . Introduction Isosurfaces , slicing , and volume rendering are the three main techniques for visualizing three-dimensional scalar fields on a two-dimensional display . A recent survey Brodlie and Wood 2001 describes the maturation of these techniques since the mid 1980s . For example , improved understanding of isosurfaces has produced robust definitions of watertight surfaces and efficient extraction methods . We believe that the same improved understanding and structuring leads to new interfaces that give the user better methods to select isosurfaces of interest and that provide a rich framework for data-guided exploration of scalar fields. Although key ideas in this paper apply to both isosurfaces and volume rendering , the immediate application is to isosurface rendering . An isosurface shows the surface for a fixed value (the isovalue ) of the scalar field and is the 3D analogue of equal-height contour lines on a topographic map . The contour tree represents the nesting relationships of connected components of isosurfaces, which we call contours , and is thus a topological abstraction of a scalar field . Since genus changes to surfaces do not affect the nesting relationship , they are not represented in the contour tree . Our contribution is to combine the flexible isosurface interface Carr and Snoeyink 2003 with online contour tree simplification guided by geometric properties of contours to produce a tool for interactive exploration of large noisy experimentally-sampled data sets. An additional contribution is to draw attention to other potential applications of simplified contour trees , such as detail-preserving denoising , automated segmentation , and atlasing. Figure 1 shows a comparison between a conventional isosurface and a flexible isosurface extracted from the same data set after contour tree simplification . On the left , the outermost surface (the skull) occludes other surfaces , making it difficult to study structures inside the head . Moreover , the contour tree for this data set has over 1 million edges , making it impractical as a visual representation. Figure 2: The topographic map (2-d scalar field) , surface rendering , and contour tree for a volcanic crater lake with a central island . A: a maximum on the crater edge; B: maximum of island in the lake; F: lake surface; C and D: saddle points. On the right is a flexible", "label": ["isosurfaces", "topological simplification", "contour trees"], "stemmed_label": ["isosurfac", "topolog simplif", "contour tree"]}
{"doc": "This paper focuses on defending against compromised nodes' dropping of legitimate reports and investigates the misbehavior of a maliciously packet-dropping node in sensor networks . We present a resilient packet-forwarding scheme using Neighbor Watch System (NWS) , specifically designed for hop-by-hop reliable delivery in face of malicious nodes that drop relaying packets , as well as faulty nodes that fail to relay packets . Unlike previous work with multipath data forwarding , our scheme basically employs single-path data forwarding , which consumes less power than multipath schemes . As the packet is forwarded along the single-path toward the base station , our scheme , however , converts into multipath data forwarding at the location where NWS detects relaying nodes' misbehavior . Simulation experiments show that , with the help of NWS , our forwarding scheme achieves a high success ratio in face of a large number of packet-dropping nodes , and effectively adjusts its forwarding style , depending on the number of packet-dropping nodes en-route to the base station . INTRODUCTION Wireless sensor networks consist of hundreds or even thousands of small devices each with sensing , processing , and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. SASN'06 , October 30 , 2006 , Alexandria , Virginia , USA. Copyright 2006 ACM 1-59593-554-1/06/0010 ... $ 5.00. communicating capabilities to monitor the real-world environment . They are envisioned to play an important role in a wide variety of areas ranging from critical military-surveillance applications to forest fire monitoring and the building security monitoring in the near future . In such a network , a large number of sensor nodes are distributed to monitor a vast field where the operational conditions are harsh or even hostile . To operate in such environments , security is an important aspect for sensor networks and security mechanisms should be provided against various attacks such as node capture , physical tampering , eavesdropping, denial of service , etc 23 , 33 , 38 . Previous research efforts against outsider attacks in key-management schemes 4 , 13 , 32 and secure node-to-node communication mechanisms 24 , 32 in sensor networks are well-defined. Those security protections , however , break down when even a single legitimate node is compromised. It turns out to be relatively easy to compromise a legitimate node 14 , which is to extract all the security information from the captured node and to make malicious code running for the attacker's purpose. Even a small number of compromised nodes can pose severe security threats on the entire part of the network, launching several attacks such as dropping legitimate reports , injecting bogus sensing reports , advertising inconsistent routing information", "label": ["neighbor watch system", "legitimate node", "reliable delivery", "packet-dropping attacks", "aggregation protocols", "malicious node", "robustness", "critical area", "single-path forwarding", "sensor network security", "cluster key", "secure ad-hoc network routing protocol", "secure routing", "degree of multipath"], "stemmed_label": ["neighbor watch system", "legitim node", "reliabl deliveri", "packet-drop attack", "aggreg protocol", "malici node", "robust", "critic area", "single-path forward", "sensor network secur", "cluster key", "secur ad-hoc network rout protocol", "secur rout", "degre of multipath"]}
{"doc": "In this paper we introduce the intermediate rank or higher rank lattice rule for the general case when the number of quadrature points is n t m , where m is a composite integer , t is the rank of the rule , n is an integer such that (n , m) = 1 . Our emphasis is the applications of higher rank lattice rules to a class of option pricing problems . The higher rank lattice rules are good candidates for applications to finance based on the following reasons: the higher rank lattice rule has better asymptotic convergence rate than the conventional good lattice rule does and searching higher rank lattice points is much faster than that of good lattice points for the same number of quadrature points; furthermore , numerical tests for application to option pricing problems showed that the higher rank lattice rules are not worse than the conventional good lattice rule on average . Introduction It is well known in scientific computation that Monte Carlo (MC) simulation method is the main method to deal with high dimensional ( 4) problems . The main drawback for this method is that it converges slowly with convergence rate O( 1 N ) , where N is the number of points (or samples or simulations) , even after using various variance reduction methods . To speed it up , researchers use quasi-random or low-discrepancy point sets , instead of using pseudo-random point sets . This is the so called quasi-Monte Carlo (QMC) method. There are two classes of low-discrepancy sequences (LDS) . The first one is constructive LDS , such as Halton's sequence , Sobol's sequence , Faure's sequence , and Nieder-reiter's (t , m , s)-nets and (t , s)-sequence . This kind of LDS has convergence rate O( (log N ) s N ) , where s is the dimension of the problem , N is , again , the number of points. The second class is the integration lattice points , for example , good lattice points (GLP) . This type of LDS has convergence rate O( (log N ) s N ) , where &gt; 1 is a parameter related to the smoothness of the integrand , s and N are the same as above . The monograph by Niederreiter 1 gives very detailed information on constructive LDS and good lattice points , while the monograph by Hua and Wang 2 and Sloan and Joe 3 describe good lattice rules in detail. Unlike the constructive sequences , the construction of good lattice points is not constructive in the sense that they could be found only by computer searches (except in the 2- dimensional case , where good lattice points can be constructed by using the Fibonacci numbers) . Such searches are usually very time consuming , especially when the number of points is large or the dimension is high , or both. Therefore , to develop algorithms which can be used in finding good lattice points fast is of practical importance. This paper discusses the applications of the", "label": ["monte carlo and quasi-monte carlo methods", "simulation of multivariate integrations", "lattice rules", "option pricing"], "stemmed_label": ["mont carlo and quasi-mont carlo method", "simul of multivari integr", "lattic rule", "option price"]}
{"doc": "The way current search engines work leaves a large amount of information available in the World Wide Web outside their catalogues . This is due to the fact that crawlers work by following hyperlinks and a few other references and ignore HTML forms . In this paper , we propose a search engine prototype that can retrieve information behind HTML forms by automatically generating queries for them . We describe the architecture , some implementation details and an experiment that proves that the information is not in fact indexed by current search engines . INTRODUCTION The gigantic growth in content present in the World Wide Web has turned search engines into fundamental tools when the objective is searching for information . A study in 2000 11 discovered that they are the most used source for finding answers to questions , positioning themselves above books, for example. However , a great deal of relevant information is still hidden from general-purpose search engines like AlltheWeb.com or Google . This part of the Web , known as the Hidden Web 7 , the Invisible Web 6 , 9 or the Deep Web 1 is growing constantly , even more than the visible Web , to which we are accustomed 6 . This happens because the crawler (the program that is responsible for autonomous navigating the web , fetching pages) used by current search engines cannot reach this information . There are many reasons for this to occur . The Internet's own dynamics , for example , ends up making the index of search engines obsolete because even the quickest crawlers only manage to access only a small fraction each day of the total information available on the Web. The cost of interpretation of some types of files , as for example Macromedia Flash animations , compressed files , and programs (executable files) could be high , not compensating for the indexing of the little , or frequently absent , textual content . For this reason , that content is also not indexed for the majority of search engines. Dynamic pages also cause some problems for indexing. There are no technical problems , since this type of page generates ordinary HTML as responses for its requests . However , they can cause some challenges for the crawlers , called spider traps 8 , which can cause , for example , the crawler to visit the same page an infinite number of times . Therefore, some search engines opt not to index this type of content. Finally , there are some sites that store their content in databases and utilize HTML forms as an access interface. This is certainly the major barrier in the exploration of the hidden Web and the problem that has fewer implemented solutions . Nowadays none of the commercial search engines that we use explore this content , which is called the Truly Invisible Web 9 . Two fundamental reasons make crawling the hidden Web a non-trivial task 7 . First is the issue of scale . Another study shows that the hidden", "label": ["implementation", "architecture", "label extraction", "experimentation", "html form", "smartcrawl", "web crawler", "hidden web content", "information retrieval", "search engine", "search engine", "extraction algorithm", "hidden web"], "stemmed_label": ["implement", "architectur", "label extract", "experiment", "html form", "smartcrawl", "web crawler", "hidden web content", "inform retriev", "search engin", "search engin", "extract algorithm", "hidden web"]}
{"doc": "Braille and audio feedback based systems have vastly improved the lives of the visually impaired across a wide majority of the globe . However , more than 13 million visually impaired people in the Indian sub-continent could not benefit much from such systems . This was primarily due to the difference in the technology required for Indian languages compared to those corresponding to other popular languages of the world . In this paper , we describe the Sparsha toolset . The contribution made by this research has enabled the visually impaired to read and write in Indian vernaculars with the help of a computer . INTRODUCTION The advent of computer systems has opened up many avenues for the visually impaired . They have benefited immensely from computer based systems like automatic text-to-Braille translation systems and audio feedback based virtual environments . Automatic text-to-Braille translation systems are widely available for languages like English , French , Spanish , Portuguese , and Swedish 7 , 26 , 18, 16 . Similarly audio feedback based interfaces like screen readers are available for English and other languages ref c , 8 , 20 . These technologies have enabled the visually impaired to communicate effectively with other sighted people and also harness the power of the Internet. However , most of these technologies remained unusable to the large visually impaired population in the Indian sub-continent 17 . This crisis can be attributed to primarily two reasons . First , the languages in the mentioned region differ widely from other popular languages in the world , like English. These languages or vernaculars also use relatively complex scripts for writing . Hence , the technologies used for English and other such languages cannot be easily extended to these languages . Secondly, the development of these technologies for Indian languages , right from scratch , is not trivial as the various Indian languages also differ significantly amongst themselves. The Sparsha toolset uses a number of innovative techniques to overcome the above mentioned challenges and provides a unified framework for a large number of popular Indian languages . Each of the tools of Sparsha will be discussed in detail in the following sections . Apart from English the languages supported by Sparsha include Hindi , Bengali , Assamese , Marathi , Gujarati , Oriya , Telugu and Kannada . The motivation for this work is to enable the visually impaired to read and write in all Indian languages . The toolset set has been named Sparsha since the word \"Sparsha\" means \"touch\" in Hindi , something which is closely associated with how Braille is read. BHARATI BRAILLE TRANSLITERATION Bharati Braille is a standard for writing text in Indian languages using the six dot format of Braille . It uses a single script to represent all Indian languages . This is done by assigning the same Braille cell to characters in different languages that are phonetically equivalent. In other words , the same combination of dots in a cell may represent different characters in each of the different Indian languages. However ,", "label": ["audio feedback", "indian languages", "braille", "visual impairment"], "stemmed_label": ["audio feedback", "indian languag", "braill", "visual impair"]}
{"doc": "This paper describes StyleCam , an approach for authoring 3D viewing experiences that incorporate stylistic elements that are not available in typical 3D viewers . A key aspect of StyleCam is that it allows the author to significantly tailor what the user sees and when they see it . The resulting viewing experience can approach the visual richness and pacing of highly authored visual content such as television commercials or feature films . At the same time , StyleCam allows for a satisfying level of interactivity while avoiding the problems inherent in using unconstrained camera models . The main components of StyleCam are camera surfaces which spatially constrain the viewing camera; animation clips that allow for visually appealing transitions between different camera surfaces; and a simple , unified , interaction technique that permits the user to seamlessly and continuously move between spatial-control of the camera and temporal-control of the animated transitions . Further , the user's focus of attention is always kept on the content , and not on extraneous interface widgets . In addition to describing the conceptual model of StyleCam , its current implementation , and an example authored experience , we also present the results of an evaluation involving real users . INTRODUCTION Computer graphics has reached the stage where 3D models can be created and rendered , often in real time on commodity hardware , at a fidelity that is almost indistinguishable from the real thing . As such , it should be feasible at the consumer level to use 3D models rather than 2D images to represent or showcase various physical artifacts . Indeed , as an example , many product manufacturers' websites are beginning to supply not only professionally produced 2D images of their products , but also ways to view their products in 3D . Unfortunately , the visual and interactive experience provided by these 3D viewers currently fall short of the slick , professionally produced 2D images of the same items . For example , the quality of 2D imagery in an automobile's sales brochure typically provides a richer and more compelling presentation of that automobile to the user than the interactive 3D experiences provided on the manufacturer's website . If these 3D viewers are to replace , or at the very least be at par with , the 2D imagery , eliminating this r, viewpoint in the scene difference in quality is critical. The reasons for the poor quality of these 3D viewers fall roughly into two categories . First , 2D imagery is usually produced by professional artists and photographers who are skilled at using this well-established artform to convey information , feelings , or experiences , whereas creators of 3D models do not necessarily have the same established skills and are working in an evolving medium . Howeve this problem will work itself out as the medium matures. The second issue is more troublesome . In creating 2D images a photographer can carefully control most of the elements that make up the shot including lighting and viewpoint , in an attempt", "label": ["3d viewers", "camera controls", "3d navigation", "3d visualization", "interaction techniques"], "stemmed_label": ["3d viewer", "camera control", "3d navig", "3d visual", "interact techniqu"]}
{"doc": "Tactile displays are now becoming available in a form that can be easily used in a user interface . This paper describes a new form of tactile output . Tactons , or tactile icons , are structured , abstract messages that can be used to communicate messages non-visually . A range of different parameters can be used for Tacton construction including : frequency , amplitude and duration of a tactile pulse , plus other parameters such as rhythm and location . Tactons have the potential to improve interaction in a range of different areas , particularly where the visual display is overloaded , limited in size or not available , such as interfaces for blind people or in mobile and wearable devices . . This paper describes Tactons , the parameters used to construct them and some possible ways to design them . Examples of where Tactons might prove useful in user interfaces are given . Introduction The area of haptic (touch-based) human computer interaction (HCI) has grown rapidly over the last few years . A range of new applications has become possible now that touch can be used as an interaction technique (Wall et al., 2002) . However , most current haptic devices have scant provision for tactile stimulation , being primarily pro-grammable , constrained motion force-feedback devices for kinaesthetic display . The cutaneous (skin-based) component is ignored even though it is a key part of our experience of touch (van Erp , 2002) . It is , for example, important for recognising texture , and detecting slip, compliance and direction of edges . As Tan (1997) says \"In the general area of human-computer interfaces .. . the tactual sense is still underutilised compared with vision and audition\" . One reason for this is that , until recently, the technology for tactile displays was limited. Tactile displays are not new but they have not received much attention from HCI researchers as they are often engineering prototypes or designed for very specific applications (Kaczmarek et al. , 1991) . They have been used in areas such as tele-operation or displays for blind people to provide sensory substitution where one sense is used to receive information normally received by another (Kaczmarek et al.) . Most of the development of these devices has taken place in robotics or engineering labs and has focused on the challenges inherent in building low cost , high-resolution devices with realistic size, power and safety performance . Little research has gone into how they might actually be used at the user interface. Devices are now available that allow the use of tactile displays so the time is right to think about how they might be used to improve interaction. In this paper the concept of Tactons , or tactile icons , is introduced as a new communication method to complement graphical and auditory feedback at the user interface . Tactons are structured , abstract messages that can be used to communicate messages non-visually . Conveying structured messages through touch will be very useful in areas such as wearable computing", "label": ["tactile displays", "multimodal interaction", "tactons", "non-visual cues"], "stemmed_label": ["tactil display", "multimod interact", "tacton", "non-visu cue"]}
{"doc": "Wireless link losses result in poor TCP throughput since losses are perceived as congestion by TCP , resulting in source throttling . In order to mitigate this effect , 3G wireless link designers have augmented their system with extensive local retransmission mechanisms . In addition , in order to increase throughput , intelligent channel state based scheduling have also been introduced . While these mechanisms have reduced the impact of losses on TCP throughput and improved the channel utilization , these gains have come at the expense of increased delay and rate variability . In this paper , we comprehensively evaluate the impact of variable rate and variable delay on long-lived TCP performance . We propose a model to explain and predict TCP's throughput over a link with variable rate and/or delay . We also propose a network-based solution called Ack Regulator that mitigates the effect of variable rate and/or delay without significantly increasing the round trip time , while improving TCP performance by up to 40% . INTRODUCTION Third generation wide-area wireless networks are currently being deployed in the United States in the form of 3G1X technology 10 with speeds up to 144Kbps . Data-only enhancements to 3G1X have already been standardized in the 3G1X-EVDO standard (also called High Data Rate or HDR) with speeds up to 2Mbps 6 . UMTS 24 is the third generation wireless technology in Europe and Asia with deploy-ments planned this year . As these 3G networks provide pervasive internet access , good performance of TCP over these wireless links will be critical for end user satisfaction. While the performance of TCP has been studied extensively over wireless links 3 , 4 , 15 , 20 , most attention has been paid to the impact of wireless channel losses on TCP. Losses are perceived as congestion by TCP , resulting in source throttling and very low net throughput. In order to mitigate the effects of losses , 3G wireless link designers have augmented their system with extensive local retransmission mechanisms . For example , link layer retransmission protocols such as RLP and RLC are used in 3G1X 22 and UMTS 21 , respectively . These mechanisms ensure packet loss probability of less than 1% on the wireless link , thereby mitigating the adverse impact of loss on TCP. While these mechanisms mitigate losses , they also increase delay variability . For example , as we shall see in Section 3, ping latencies vary between 179ms to over 1 second in a 3G1X system. In addition , in order to increase throughput , intelligent channel state based scheduling have also been introduced. Channel state based scheduling 7 refers to scheduling techniques which take the quality of wireless channel into account while scheduling data packets of different users at the base station. The intuition behind this approach is that since the channel quality varies asynchronously with time due to fading , it is preferable to give priority to a user with better channel quality at each scheduling epoch. While strict priority could lead to starvation of users with", "label": ["algorithm", "architecture", "tcp", "wireless communication", "performance evaluation", "3g wireless links", "prediction model", "design", "link and rate variation", "3g wireless", "simulation result", "congestion solution", "network"], "stemmed_label": ["algorithm", "architectur", "tcp", "wireless commun", "perform evalu", "3g wireless link", "predict model", "design", "link and rate variat", "3g wireless", "simul result", "congest solut", "network"]}
{"doc": "Students in a sophomore-level database fundamentals course were taught SQL and database concepts using both Oracle and SQL Server . Previous offerings of the class had used one or the other database . Classroom experiences suggest that students were able to handle learning SQL in the dual environment , and , in fact , benefited from this approach by better understanding ANSI-standard versus database-specific SQL and implementation differences in the two database systems . INTRODUCTION A problem arises in many technology classes . The instructor wishes to teach principles and concepts of a technology . To give students hands-on experience putting those theories to work , a specific product that implements that technology is selected for a lab component . Suddenly the students are learning more about the specific product than they are about the technology concepts. They may or may not realize what is specific to that product and what is general to the technology . Students may even start referring to the course as a VB course , a PHP course , or an Oracle course when what you wanted to teach was programming , web scripting , or database principles. This paper presents the experiences from a database fundamentals course that used both Oracle and SQL Server so that students would better understand ANSI-standard SQL . Though each database is ANSI SQL compliant , there are definite differences in implementation (Gorman , 2001; Gulutzan , 2002) . By learning each implementation and how each departs from ANSI-standard SQL , students can be better prepared to work with any database and better understand general concepts of databases and SQL . The paper discusses the observed results from this approach and how well the approach met learning objectives. COURSE CONTEXT AND LEARNING OBJECTIVES CPT 272 , Database Fundamentals , is a sophomore-level database programming and design class taught primarily to computer technology majors in their fourth semester . Students will have previously taken a freshman-level course that introduces them to databases as a tool for learning general information system development terms and concepts . The freshman-level course uses Microsoft Access because it is easy to use for quickly developing a small personal information system . That course also introduces both SQL and Query By Example methods for querying a database as well basic database design concepts , which are applied for simple data models. Students then move into two programming courses , the second of which uses single-table SQL statements for providing data to (formerly) Visual Basic or (currently) web-programming applications . So by the time the students take the Database Fundamentals course they have a concept of what a database is and how it is used as the back-end for programming . The Database Fundamentals course is the course where students learn SQL in depth , database concepts , and basic database design . It does not teach stored procedure programming , triggers , or enterprise or distributed database design , which are covered in more advanced courses. The learning objectives for the Database Fundamentals course are: To understand the", "label": ["sql", "sql server", "training vs. education", "database systems", "oracle", "database", "educational fundamentals", "student feedbacks", "ansi-standard sql", "teaching in it", "dual environment", "sql language", "course design", "practical results"], "stemmed_label": ["sql", "sql server", "train vs. educ", "databas system", "oracl", "databas", "educ fundament", "student feedback", "ansi-standard sql", "teach in it", "dual environ", "sql languag", "cours design", "practic result"]}
{"doc": "We present a model , based on the maximum entropy method , for analyzing various measures of retrieval performance such as average precision , R-precision , and precision-at-cutoffs . Our methodology treats the value of such a measure as a constraint on the distribution of relevant documents in an unknown list , and the maximum entropy distribution can be determined subject to these constraints . For good measures of overall performance (such as average precision) , the resulting maximum entropy distributions are highly correlated with actual distributions of relevant documents in lists as demonstrated through TREC data; for poor measures of overall performance , the correlation is weaker . As such , the maximum entropy method can be used to quantify the overall quality of a retrieval measure . Furthermore , for good measures of overall performance (such as average precision) , we show that the corresponding maximum entropy distributions can be used to accurately infer precision-recall curves and the values of other measures of performance , and we demonstrate that the quality of these inferences far exceeds that predicted by simple retrieval measure correlation , as demonstrated through TREC data . INTRODUCTION The efficacy of retrieval systems is evaluated by a number of performance measures such as average precision , R-precision , and precisions at standard cutoffs . Broadly speaking , these measures can be classified as either system-oriented measures of overall performance (e.g. , average precision and R-precision) or user-oriented measures of specific performance (e.g. , precision-at-cutoff 10) 3 , 12 , 5 . Different measures evaluate different aspects of retrieval performance , and much thought and analysis has been devoted to analyzing the quality of various different performance measures 10 , 2, 17 . We consider the problem of analyzing the quality of various measures of retrieval performance and propose a model based on the maximum entropy method for evaluating the quality of a performance measure . While measures such as average precision at relevant documents , R-precision , and 11pt average precision are known to be good measures of overall performance , other measures such as precisions at specific cutoffs are not . Our goal in this work is to develop a model within which one can numerically assess the overall quality of a given measure based on the reduction in uncertainty of a system's performance one gains by learning the value of the measure . As such , our evaluation model is primarily concerned with assessing the relative merits of system-oriented measures , but it can be applied to other classes of measures as well. We begin with the premise that the quality of a list of documents retrieved in response to a given query is strictly a function of the sequence of relevant and non-relevant documents retrieved within that list (as well as R , the total number of relevant documents for the given query) . Most standard measures of retrieval performance satisfy this premise. Our thesis is then that given the assessed value of a \"good\" overall measure of performance , one's uncertainty about the", "label": ["average precision", "evaluation", "maximum entropy"], "stemmed_label": ["averag precis", "evalu", "maximum entropi"]}
{"doc": "Recognition of motion streams such as data streams generated by different sign languages or various captured human body motions requires a high performance similarity measure . The motion streams have multiple attributes , and motion patterns in the streams can have different lengths from those of isolated motion patterns and different attributes can have different temporal shifts and variations . To address these issues , this paper proposes a similarity measure based on singular value decomposition (SVD) of motion matrices . Eigenvector differences weighed by the corresponding eigenvalues are considered for the proposed similarity measure . Experiments with general hand gestures and human motion streams show that the proposed similarity measure gives good performance for recognizing motion patterns in the motion streams in real time . INTRODUCTION Motion streams can be generated by continuously performed sign language words 14 or captured human body motions such as various dances . Captured human motions can be applied to the movie and computer game industries by reconstructing various motions from video sequences 10 or images 15 or from motions captured by motion capture systems 4 . Recognizing motion patterns in the streams with unsupervised methods requires no training process , and is very convenient when new motions are expected to be added to the known pattern pools . A similarity measure with good performance is thus necessary for segmenting and recognizing the motion streams . Such a similarity measure needs to address some new challenges posed by real world Work supported partially by the National Science Foundation under Grant No . 0237954 for the project CAREER: Animation Databases. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. Copyright 200X ACM X-XXXXX-XX-X/XX/XX ... $ 5.00. motion streams: first , the motion patterns have dozens of attributes , and similar patterns can have different lengths due to different motion durations; second , different attributes of similar motions have different variations and different temporal shifts due to motion variations; and finally , motion streams are continuous , and there are no obvious \"pauses\" between neighboring motions in a stream . A good similarity measure not only needs to capture the similarity of complete motion patterns , but also needs to capture the differences between complete motion patterns and incomplete motion patterns or sub-patterns in order to segment a stream for motion recognition. As the main contribution of this paper , we propose a similarity measure to address the above issues . The proposed similarity measure is defined based on singular value decomposition of the motion matrices . The first few eigenvectors are compared for capturing the similarity of two matrices, and the inner products of the eigenvectors are given different weights for", "label": ["motion stream", "segmentation", "data streams", "eigenvector", "singular value decomposition", "gesture", "recognition", "eigenvalue", "similarity measure", "pattern recognition"], "stemmed_label": ["motion stream", "segment", "data stream", "eigenvector", "singular valu decomposit", "gestur", "recognit", "eigenvalu", "similar measur", "pattern recognit"]}
{"doc": "This paper presents a formalization for Topic Maps (TM) . We first simplify TMRM , the current ISO standard proposal for a TM reference model and then characterize topic map instances . After defining a minimal merging operator for maps we propose a formal foundation for a TM query language . This path expression language allows us to navigate through given topic maps and to extract information . We also show how such a language can be the basis for a more industrial version of a query language and how it may serve as foundation for a constraint language to define TM-based ontologies . Introduction Topic Maps (TM (Pepper 1999)) , a knowledge representation technology alternative to RDF (O . Lassila and K . Swick 1993) , have seen some industrial adoption since 2001 . Concurrently , the TM community is taking various efforts to define a more fundamental , more formal model to capture the essence of what Topic Maps are (Newcomb , Hunting , Algermissen & Durusau 2003 , Kipp 2003 , Garshol 2004-07-22 , Bogachev n.d.) . While the degree of formality and the extent of TM machinery varies , all models tend to abstract away from the sets of concepts defined in (Pepper 2000) and use assertions (and topics) as their primitives. After giving an overview over the current state of affairs , we start with an attempt to conceptually simplify the TMRM (Newcomb et al . 2003) model . From that , a mathematically more rigorous formalization of TMs follows in section 4 . Based on maps and elementary map composition we define a path expression language using a postfix notation . While low-level , it forms the basis for querying and constraining topic maps as we point out in section 6 . The last section closes with future research directions. Related Work Historically , Topic Maps , being a relatively new technology , had some deficits in rigor in terms of a defining model . This may be due to the fact that it was more Paradoxically , the standardization efforts started out with the syntax (XTM) with only little , informal description of the individual constructs . TMDM (formerly known as SAM) was supposed to fill this role by precisely defining how XTM instances are to be deserialized into a data structure . This is done by mapping the syntax into an infoset model (comparable to DOM) whereby UML diagrams help to illustrate the intended structure as well as the constraints put on it . While such an approach to model definition has a certain appeal for (Java) developers , its given complexity puts it well outside the reach for a more mathematical formalization. In parallel a fraction within the TM community ar-gued that the TM paradigm can be interpreted on a much more fundamental level if one considers assertions as the basic building blocks , abstracting from the TAO-level which mainly sees topics with their names , occurrences and involvements in associations. This group has developed several generations of the TMRM (Newcomb et al", "label": ["semantic web", "topic maps", "knowledge engineering"], "stemmed_label": ["semant web", "topic map", "knowledg engin"]}
{"doc": "The slowing pace of commodity microprocessor performance improvements combined with ever-increasing chip power demands has become of utmost concern to computational scientists . As a result , the high performance computing community is examining alternative architectures that address the limitations of modern cache-based designs . In this work , we examine the potential of using the forthcoming STI Cell processor as a building block for future high-end computing systems . Our work contains several novel contributions . First , we introduce a performance model for Cell and apply it to several key scientific computing kernels: dense matrix multiply , sparse matrix vector multiply , stencil computations , and 1D/2D FFTs . The difficulty of programming Cell , which requires assembly level intrinsics for the best performance , makes this model useful as an initial step in algorithm design and evaluation . Next , we validate the accuracy of our model by comparing results against published hardware results , as well as our own implementations on the Cell full system simulator . Additionally , we compare Cell performance to benchmarks run on leading superscalar (AMD Opteron) , VLIW (Intel Itanium2) , and vector (Cray X1E) architectures . Our work also explores several different mappings of the kernels and demonstrates a simple and effective programming model for Cell's unique architecture . Finally , we propose modest microarchitectural modifications that could significantly increase the efficiency of double-precision calculations . Overall results demonstrate the tremendous potential of the Cell architecture for scientific computations in terms of both raw performance and power efficiency . INTRODUCTION Over the last decade the HPC community has moved towards machines composed of commodity microprocessors as a strategy for tracking the tremendous growth in processor performance in that market . As frequency scaling slows, and the power requirements of these mainstream processors continues to grow , the HPC community is looking for alternative architectures that provide high performance on scientific applications , yet have a healthy market outside the scientific community . In this work , we examine the potential of the forthcoming STI Cell processor as a building block for future high-end computing systems , by investigating performance across several key scientific computing kernels: dense matrix multiply , sparse matrix vector multiply , stencil computations on regular grids , as well as 1D and 2D FFTs. Cell combines the considerable floating point resources required for demanding numerical algorithms with a power-efficient software-controlled memory hierarchy . Despite its radical departure from previous mainstream/commodity processor designs , Cell is particularly compelling because it will be produced at such high volumes that it will be cost-competitive with commodity CPUs . The current implementation of Cell is most often noted for its extremely high performance single-precision (SP) arithmetic , which is widely considered insufficient for the majority of scientific applications . Although Cell's peak double precision performance is still impressive relative to its commodity peers (~14.6 Gflop/s@3.2GHz) , we explore how modest hardware changes could significantly improve performance for computationally intensive DP applications. This paper presents several novel results. We present quantitative performance data for", "label": ["gemm", "fft", "cell processor", "three level memory", "spmv", "stencil", "sparse matrix"], "stemmed_label": ["gemm", "fft", "cell processor", "three level memori", "spmv", "stencil", "spars matrix"]}
{"doc": "Component Based Development aims at constructing software through the inter-relationship between pre-existing components . However , these components should be bound to a specific application domain in order to be effectively reused . Reusable domain components and their related documentation are usually stored in a great variety of data sources . Thus , a possible solution for accessing this information is to use a software layer that integrates different component information sources . We present a component information integration data layer , based on mediators . Through mediators , domain ontology acts as a technique/formalism for specifying ontological commitments or agreements between component users and providers , enabling more accurate software component information search . INTRODUCTION Component Based Development (CBD) 1 aims at constructing software through the inter-relationship between pre-existing components , thus reducing the complexity , as well as the cost of software development , through the reuse of exhaustively tested components . Building new solutions by combining components should improve quality and support rapid development , leading to a shorter time-to-market . At the same time , nimble adaptation to changing requirements can be achieved by investing only in key changes of a component-based solution , rather than undertaking a major release change . For these reasons , component technology is expected by many to be the cornerstone of software production in the years to come. According to Jacobson , Griss and Jonsson 1 , the effectiveness of component reuse depends on the connectiveness among them and their binding to specific application domains . The connectiveness is one of the most discussed problems in CBD 6 , 12 . Approaches that deal with component interfaces (one of premises for connection between components) focus on their capability to provide and request services . Although this interface aspect is important in a CBD approach , other problems arise when trying to connect components . The connectiveness also depends on the execution environment , the heterogeneity of components , the distance between them and the architecture that controls their connections 1 , 5 , 12 . The architecture that governs the connections between components also depends on the application domain . Therefore , reuse possibilities increase when components are bound to domain concepts . As stated by Krueger 1 , while retrieving reusable software components , it is advantageous to use a terminology that is familiar to the domain . This approach diverges from other software component retrieval proposals , such as the Agora System 6 that bases the search only on the component interfaces , covering solely the component connectiveness problem , and the RIG initiative 10 that presents an approach for domain repository integration with minor user transparency and without web access. Suppose that , in a typical component retrieval scenario , a software developer wants to find software components to use in the construction of an application under development . If he does not know any other specialized service that provides information about components , the natural search space will be the Internet. Now , consider that this developer has", "label": ["domain engineering", "software classification and identification", "component repositories", "component based engineering"], "stemmed_label": ["domain engin", "softwar classif and identif", "compon repositori", "compon base engin"]}
{"doc": "We present enhancements for UDDI / DAML-S registries allowing cooperative discovery and selection of Web services with a focus on personalization . To find the most useful service in each instance of a request , not only explicit parameters of the request have to be matched against the service offers . Also user preferences or implicit assumptions of a user with respect to common knowledge in a certain domain have to be considered to improve the quality of service provisioning . In the area of Web services the notion of service ontologies together with cooperative answering techniques can take a lot of this responsibility . However , without quality assessments for the relaxation of service requests and queries a personalized service discovery and selection is virtually impossible . This paper focuses on assessing the semantic meaning of query relaxation plans over multiple conceptual views of the service ontology , each one representing a soft query constraint of the user request . Our focus is on the question what constitutes a minimum amount of necessary relaxation to answer each individual request in a cooperative manner . Incorporating such assessments as early as possible we propose to integrate ontology-based discovery directly into UDDI directories or query facilities in service provisioning portals . Using the quality assessments presented here , this integration promises to propel today's Web services towards an intuitive user-centered service provisioning . Categories and Subject Descriptors INTRODUCTION Web services are expected to provide an open platform not only for electronic B2B interaction , but also for the provisioning of so-called user-centered services , i.e . B2C services that can provide useful information and a variety of service offers to support users in a modern mobile lifestyle . Though the capabilities of such services are still relatively simple , their sophistication will grow with the improvement of (wireless) networks , bandwidths , and client device capabilities . However , finding the adequate service for subsequent use of each individual user becomes a more and more demanding problem . Given the convergence of networks in forthcoming (mobile) environments and the evolving innovative business models for third party service deployment (e.g . NTT DoCoMo's i-mode service certification/licensing model for mobile service portals A href=\"193.html#10\" 19 ) the variety of services is even expected to grow . Making an informed choice of the `right' service will therefore include matching individual users' preferences or dislikes against the concepts and capabilities of the services offered. Usually the interaction process for Web services consists of three distinct phases: a discovery of possible services , the selection of the most useful , and the subsequent execution . In understanding what a service actually offers the first two phases are crucial and the general acceptance of user-centered services will depend on the solutions of still demanding problems in interaction like cooperative querying . As sh A href=\"193.html#9\" own in 4 and 5 A href=\"193.html#10\" the discovery and selection processes of user-centered Web services involves a high degree of respect for user preferences to be flexible enough for real world use .", "label": ["selection of the most useful", "web service definition language", "web services", "tree-shaped clipping of ontologies", "subsequent execution", "semantic web", "user profiling", "the generalization throughout ontologies", "ontology resembles common knowledge", "universal description discovery and integration", "discovery of possible services", "generalization hierarchy of concepts", "cooperative service discovery", "personalization", "preference-based service provisioning", "domain-specific understanding of concepts", "relaxing multiple ontologies"], "stemmed_label": ["select of the most use", "web servic definit languag", "web servic", "tree-shap clip of ontolog", "subsequ execut", "semant web", "user profil", "the gener throughout ontolog", "ontolog resembl common knowledg", "univers descript discoveri and integr", "discoveri of possibl servic", "gener hierarchi of concept", "cooper servic discoveri", "person", "preference-bas servic provis", "domain-specif understand of concept", "relax multipl ontolog"]}
{"doc": "Word prediction can be used for enhancing the communication ability of persons with speech and language impair-ments . In this work , we explore two methods of adapting a language model to the topic of conversation , and apply these methods to the prediction of fringe words . INTRODUCTION Alternative and Augmentative Communication (AAC) is the field of research concerned with finding ways to help those with speech difficulties communicate more easily and completely . Today there are approximately 2 million people in the United States with some form of communication difficulty . One means to help ease communication is the use of an electronic communication device , which may have synthetic speech as output . However , one issue in using an AAC device is communication rate . Whereas speaking rate is estimated at 180 words per minute (wpm) , many AAC users' communication rates are lower than 15 wpm 3 , 7, 16 . Thus one goal of developers is to find ways to increase the rate of communication , by making AAC devices easier to use and more intelligent. Some researchers have attempted to speed communication rate by providing quick access to the core vocabulary the relatively small set of frequently used words . Methods for doing this include abbreviation expansion and iconic methods such as semantic compaction 1 . In contrast , in this work we attempt to speed access to the much larger set of words often called fringe vocabulary . This set is of interest because although each individual word occurs less frequently , the set of fringe words on the whole is very significant . Suppose that the user wants to enter \"I want a home in the country.\" After typing , \"I want a h\" , they might see something like shown below . The system has created a prediction window containing the five words that it thinks the user may be trying to type . In this example , the user can press F5 to complete the word \"home\" and the system will enter the word with a space afterwards . So in this example, the user needed 2 keystrokes to enter what would normally take 5 keystrokes. It is difficult to judge how much word prediction can speed communication rate . Much of this determination is dependent on the accuracy of the prediction method , the characteristics of the user , such as their physical and cognitive abilities , and the characteristics of the user interface , such as where the prediction list is displayed and how a word in the list is selected . Here , the prediction method is evaluated separately from the rest of a word prediction system by simulating what a user would type in a conversation if he/she were taking full advantage of the prediction list . This theoretical evaluation measures the percentage of keystrokes that were saved by word prediction over typing out every character. In this paper we first describe related work and give some background in statistical approaches to word prediction . We present", "label": ["core vocabulary", "identify current topic of conversation", "aac", "language modeling", "accuracy of prediction method", "fringe vocabulary", "prediction of fringe words", "conversations in the switchboard corpus", "word prediction", "immediate prediction", "decrease probability of unrelated words", "increase probability of related words", "prediction window size", "communication rate", "construct a new language model", "topic modeling"], "stemmed_label": ["core vocabulari", "identifi current topic of convers", "aac", "languag model", "accuraci of predict method", "fring vocabulari", "predict of fring word", "convers in the switchboard corpu", "word predict", "immedi predict", "decreas probabl of unrel word", "increas probabl of relat word", "predict window size", "commun rate", "construct a new languag model", "topic model"]}
{"doc": "In this paper we introduce a probabilistic framework to exploit hierarchy , structure sharing and duration information for topic transition detection in videos . Our probabilistic detection framework is a combination of a shot classification step and a detection phase using hierarchical probabilistic models . We consider two models in this paper: the extended Hierarchical Hidden Markov Model (HHMM) and the Coxian Switching Hidden semi-Markov Model (S-HSMM) because they allow the natural decomposition of semantics in videos , including shared structures , to be modeled directly , and thus enabling efficient inference and reducing the sample complexity in learning . Additionally , the S-HSMM allows the duration information to be incorporated , consequently the modeling of long-term dependencies in videos is enriched through both hierarchical and duration modeling . Furthermore , the use of the Coxian distribution in the S-HSMM makes it tractable to deal with long sequences in video . Our experimentation of the proposed framework on twelve educational and training videos shows that both models outperform the baseline cases (flat HMM and HSMM) and performances reported in earlier work in topic detection . The superior performance of the S-HSMM over the HHMM verifies our belief that duration information is an important factor in video content modeling . INTRODUCTION The ultimate goal of the video segmentation problem is to characterize the temporal dynamics of the video whereby it can be segmented into coherent units , possibly at different levels of abstraction . Seeking abstract units to move beyond the shots has thus been an active topic of much recent research . While the problem of shot transition is largely solved at a satisfactory level 7 , the `abstract units' or scene detection problem is much harder , partially due to the following three challenges identified in 29 : (a) the variety in directional styles , (b) the semantic relationship of neighbouring scenes , and (c) the knowledge of the viewer about the world. While the last aspect is beyond the scope of this work , the first two clearly imply that effective modeling of high-level semantics requires the domain knowledge (directional style) and the modeling of long-term , multiple-scale correlations of the video dynamics (neighboring semantic relationship). Modeling temporal correlations over a long period is generally a challenging problem . As we shall review in the subsequent section , this problem is usually solved in a specific domain setting so that the expert knowledge about the domain can be utilised . While organization of content in generic videos (e.g. , movies) is too diverse to be fully char-acterized by statistical models , the hierarchy of semantic structure in the class of education-oriented videos is more defined , exposing strong temporal correlation in time , and thus make it more desirable to probabilistic modeling . In this paper , we concentrate on this video genre and develop an effective framework to segment these videos into topi-cally correlated units . This problem is an important step to enable abstraction , summarization , and browsing of educational content a rich class of film genre", "label": ["domain knowledge", "topic transition detection", "a variety in directional styles", "semantic relationship of neighborhood scenes", "coxian switching hidden semi-markov model", "natural hierarchical organization of videos", "model educational video content", "extended hierarchical hidden markov model", "unified and coherent probabilistic framework", "educational videos", "shot-based semantic classification", "their semantically shared substructures", "topic transition detection", "probabilistic framework", "coxian switching hidden semi-markov model", "coxian", "hierarchical markov (semi-markov) models", "typical durations of important semantics", "modeling temporal correlation", "hierarchical hidden markov model"], "stemmed_label": ["domain knowledg", "topic transit detect", "a varieti in direct style", "semant relationship of neighborhood scene", "coxian switch hidden semi-markov model", "natur hierarch organ of video", "model educ video content", "extend hierarch hidden markov model", "unifi and coher probabilist framework", "educ video", "shot-bas semant classif", "their semant share substructur", "topic transit detect", "probabilist framework", "coxian switch hidden semi-markov model", "coxian", "hierarch markov (semi-markov) model", "typic durat of import semant", "model tempor correl", "hierarch hidden markov model"]}
{"doc": "Most existing web video search engines index videos by file names , URLs , and surrounding texts . These types of video metadata roughly describe the whole video in an abstract level without taking the rich content , such as semantic content descriptions and speech within the video , into consideration . Therefore the relevance ranking of the video search results is not satisfactory as the details of video contents are ignored . In this paper we propose a novel relevance ranking approach for Web-based video search using both video metadata and the rich content contained in the videos . To leverage real content into ranking , the videos are segmented into shots , which are smaller and more semantic-meaningful retrievable units , and then more detailed information of video content such as semantic descriptions and speech of each shots are used to improve the retrieval and ranking performance . With video metadata and content information of shots , we developed an integrated ranking approach , which achieves improved ranking performance . We also introduce machine learning into the ranking system , and compare them with IRmodel (information retrieval model) based method . The evaluation results demonstrate the effectiveness of the proposed ranking methods . INTRODUCTION Multimedia search has become an active research field due to the rapid increase of online-available content and new practical applications . Search technology is considered the key to navigating the Internet's growing media (video , audio and image) collections . Google Yahoo , Blinkx and other search companies have provided elementary video search engines . However, existing video search engines are all based on the text information related to the video which can be retrieved from web pages , such as file names , URLs , and surrounding texts . These types of textual information can be considered as \"metadata\" of the video since they only roughly describe the video . There is no doubt that text searching is the most efficient way to retrieve information (even when searching for videos) , because it well matches the manner of human thinking . However , only using metadata is far form people's expectation in video searching , because even the best case scenario , the metadata is only the highly concentrated overview of a video , with many losses on details. In general , a video consists of many shots and sub-events with a temporal main thread . The video should be segmented into smaller retrievable units that are directly related to what users perceive as meaningful . Much research has concentrated on segmenting video streams into \"shots\" using low level visual features 1 . Each segment has its own scenes and meanings . In many cases , when users query a video , they intend to find some desired clips in the video instead of viewing it thoroughly . However , this can seldom be achieved by searching the surrounding text which is related to the whole video. Much content information can be used to search videos and shots . In content-based video retrieval systems ,", "label": ["video index, relevance ranking", "content-based relevance ranking", "video retrieval", "metadata", "learning based ranking", "neutral network based ranking", "relevance ranking", "content information", "content-based approach", "ranking method", "integrated ranking", "video metadata", "ir-model", "segmented", "content-based ranking", "machine learning model", "video segmentation", "ir model based ranking", "video search", "video search"], "stemmed_label": ["video index, relev rank", "content-bas relev rank", "video retriev", "metadata", "learn base rank", "neutral network base rank", "relev rank", "content inform", "content-bas approach", "rank method", "integr rank", "video metadata", "ir-model", "segment", "content-bas rank", "machin learn model", "video segment", "ir model base rank", "video search", "video search"]}
{"doc": "The growing importance of access control has led to the definition of numerous languages for specifying policies . Since these languages are based on different foundations , language users and designers would benefit from formal means to compare them . We present a set of properties that examine the behavior of policies under enlarged requests , policy growth , and policy decomposition . They therefore suggest whether policies written in these languages are easier or harder to reason about under various circumstances . We then evaluate multiple policy languages , including XACML and Lithium , using these properties . INTRODUCTION Access-control policies should not be write-only . Because they govern both the containment and availability of critical information , they must be highly amenable to analysis by both humans and by reasoning software such as verifiers. An access-control policy dictates a function from requests for access to decisions about whether or not to grant access . The competing requirements of expressive power and computational speed makes the design of policy languages a delicate balancing act . Contemporary policy languages have largely followed one of two routes . Some are based on logics, restricting first-order logic (e.g. , Lithium 9 ) or augmenting Datalog (e.g. , Cassandra 2 ) . Others are custom languages such as XACML 12 and EPAL 13 , which behave roughly by rule-evaluation and do not depend on theorem-proving capabilities to determine a response to a query. The custom language approach often produces fairly limited languages . For example , to express hierarchical role-based access-control (RBAC) 14 in XACML requires a fairly cumbersome encoding 1 . On the other hand , its more direct request evaluation strategy suggests that policies written in XACML are more transparent than policies written in languages based on first-order logic (as we motivate in Section 2). How , then , do we distinguish different policy languages? Studies of complexity and expressive power may ensure tractable verification and the ability to capture certain policies, but do not directly classify the ease of reasoning about policies in a language . In this paper we take a step towards formalizing reasonability properties that make languages more amenable to reasoning . We then apply these properties to actual policy languages. Such properties are useful even when verification is computationally tractable because they provide a guide to where and how to edit a policy for a desired effect. Concretely , our properties study three main questions: how decisions change as requests include more information, how decisions change as policies grow , and how amenable policies are to compositional reasoning . The last of these is especially important for two reasons . First , organizations in-creasingly have different divisions creating policy fragments that must be combined into a whole while preserving the intent of each division; second , to mirror these use cases , and to scale better as policies grow in size , it becomes important for analysis and verification tools to function modularly. These properties codify our observations made while writing and studying policies for non-trivial systems .", "label": ["common features", "access control", "lithium", "modularity", "reasonability property", "policy decomposition", "properties", "access control", "policy combinator", "xacml", "comtemporary policy", "access-control policy", "policy language property", "first order logic", "xacml", "multiple policy language", "policy language", "policy", "security", "formalize", "policy languague"], "stemmed_label": ["common featur", "access control", "lithium", "modular", "reason properti", "polici decomposit", "properti", "access control", "polici combin", "xacml", "comtemporari polici", "access-control polici", "polici languag properti", "first order logic", "xacml", "multipl polici languag", "polici languag", "polici", "secur", "formal", "polici languagu"]}
{"doc": "In a wide range of business areas dealing with text data streams , including CRM , knowledge management , and Web monitoring services , it is an important issue to discover topic trends and analyze their dynamics in real-time.Specifically we consider the following three tasks in topic trend analysis: 1)Topic Structure Identification; identifying what kinds of main topics exist and how important they are , 2)Topic Emergence Detection; detecting the emergence of a new topic and recognizing how it grows , 3)Topic Characterization ; identifying the characteristics for each of main topics . For real topic analysis systems , we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way , and be dealt with in a single framework . This paper proposes a new topic analysis framework which satisfies this requirement from a unifying viewpoint that a topic structure is modeled using a finite mixture model and that any change of a topic trend is tracked by learning the finite mixture model dynamically.In this framework we propose the usage of a time-stamp based discounting learning algorithm in order to realize real-time topic structure identification .This enables tracking the topic structure adaptively by forgetting out-of-date statistics.Further we apply the theory of dynamic model selection to detecting changes of main components in the finite mixture model in order to realize topic emergence detection.We demonstrate the effectiveness of our framework using real data collected at a help desk to show that we are able to track dynamics of topic trends in a timely fashion . INTRODUCTION In a wide range of business areas dealing with text streams, including CRM , knowledge management , and Web monitoring services , it is an important issue to discover topic trends and analyze their dynamics in real-time.For example , it is desired in the CRM area to grasp a new trend of topics in customers' claims every day and to track a new topic as soon as it emerges.A topic is here defined as a seminal event or activity.Specifically we consider the following three tasks in topic analysis: 1) Topic Structure Identification; learning a topic structure in a text stream , in other words , identifying what kinds of main topics exist and how important they are. 2) Topic Emergence Detection; detecting the emergence of a new topic and recognizing how rapidly it grows , similarly, detecting the disappearance of an existing topic. 3) Topic Characterization; identifying the characteristics for each of main topics. For real topic analysis systems , we may require that these three tasks be performed in an on-line fashion rather than in a retrospective way , and be dealt with in a single framework. The main purpose of this paper is to propose a new topic analysis framework that satisfies the requirement as above, and to demonstrate its effectiveness through its experimental evaluations for real data sets. Our framework is designed from a unifying viewpoint that a topic structure in a text stream is modeled using a finite mixture model (a model of the form of", "label": ["finite mixture model", "crm", "time-stamp based discounting learning algorithm", "topic structure identification", "topic characterization", "topic detection and tracking", "time-stamp based learning algorithm", "topic structure identification", "topic emergence detection", "text mining", "topic emergence detection", "tracking dynamics", "dynamic model selection", "data mining", "information gain", "topic trends", "topic characterization", "text data streams", "model selection", "topic trend", "topic analysis"], "stemmed_label": ["finit mixtur model", "crm", "time-stamp base discount learn algorithm", "topic structur identif", "topic character", "topic detect and track", "time-stamp base learn algorithm", "topic structur identif", "topic emerg detect", "text mine", "topic emerg detect", "track dynam", "dynam model select", "data mine", "inform gain", "topic trend", "topic character", "text data stream", "model select", "topic trend", "topic analysi"]}
{"doc": "This paper shows the importance that management plays in the protection of information and in the planning to handle a security breach when a theft of information happens . Recent thefts of information that have hit major companies have caused concern . These thefts were caused by companies' inability to determine risks associated with the protection of their data and these companies lack of planning to properly manage a security breach when it occurs . It is becoming necessary , if not mandatory , for organizations to perform ongoing risk analysis to protect their systems . Organizations need to realize that the theft of information is a management issue as well as a technology one , and that these recent security breaches were mainly caused by business decisions by management and not a lack of technology . INTRODUCTION After counter-terrorism and counter-intelligence , cyber crime is the third highest priority for the U.S . Federal Bureau 4 . With the rise of the theft of information and the lure of big profits for this stolen information , it is necessary for information systems to have the ability to protect this valuable asset . It is estimated that a credit card number unsupported by any other documentation is worth $10 , and a credit history report retails for $60 2 . Recent breaches of information systems that have lead to thefts of information have shown that management practices and not technology was part of the issue and in some cases the primary cause of the theft of the information . With each of these thefts, there is a third party committing a crime , but in each case , risk analysis could have been used to avoid or to help mitigate the theft . It is becoming a necessity that companies examine their business practices and company policies to avoid risks associated with information stealing . The solution to information stealing does not reside in technology alone but also requires an understanding by management of the business and the risks associated with it . This paper examines the theft of information from different companies in order to explain the short coming of management practices that lead to the theft . . CASE STUDIES In May of 2005 , Citigroup lost computer tapes that were being sent to the credit bureau via UPS that included Social Security numbers and payment history information for 3.9 million customers . After this event , this New York based company has decided that it will start sending its data to the credit bureau electronically using encryption 8 . Citigroup should have learned a lesson from Time Warner who lost a shipment of backup tapes that contained personal information of 600,000 employees that was being sent to an offsite data storage company in March of 2005 9 . But the question remains , why was Citigroup sending sensitive information unsecured? Why did they not encrypt the data in the first place , and why did they realize that these tapes could get lost or stolen as evident to what", "label": ["security breach", "risk analysis", "information security", "business practises and policy", "information system", "cases of information theft", "privacy", "management issue", "information security management", "theft of information", "human factor", "data protection procedure", "security management", "information security", "cyber crime", "confidential information", "incident response plan", "encryption", "data protection", "personal information"], "stemmed_label": ["secur breach", "risk analysi", "inform secur", "busi practis and polici", "inform system", "case of inform theft", "privaci", "manag issu", "inform secur manag", "theft of inform", "human factor", "data protect procedur", "secur manag", "inform secur", "cyber crime", "confidenti inform", "incid respons plan", "encrypt", "data protect", "person inform"]}
{"doc": "Information seeking and management practices are an integral aspect of people's daily work . However , we still have little understanding of collaboration in the information seeking process . Through a survey of collaborative information seeking practices of academic researchers , we found that researchers reported that (1) the lack of expertise is the primary reason that they collaborate when seeking information; (2) traditional methods , including face-to-face , phone , and email are the preferred communication mediums for collaboration; and (3) collaborative information seeking activities are usually successful and more useful than individually seeking information . These results begin to highlight the important role that collaborative information seeking plays in daily work . INTRODUCTION Information seeking and management practices are an integral aspect of people's daily work . In organizational work , information is vital for making decisions and coordinating activities . Therefore , organizations have developed a wide variety of processes and technologies to support their workers' information seeking activities . Much of this support has been for the individual information seeker; in most organizations , information seeking has been traditionally viewed as an individual activity 1 , 2 . Yet , collaboration is becoming an increasingly important component of work in organizations . Multidisciplinary teams are a common feature of modern organizations 3, 4 . To successfully accomplish their work , team members must collaborate with each other efficiently and effectively. One important aspect of the team's work is seeking information 5 . Yet , we have little understanding of collaborative information seeking practices 6 , 7 . Therefore , to help team members work together effectively and to design information systems that support their work, we must understand the collaborative information seeking practices of team members. To examine collaborative information seeking (CIS) practices , we conducted a survey of academic researchers in a small technology-focused research university. Researchers have traditionally collaborated with each other on research projects because of the often cross-disciplinary nature of the work . This collaboration has increased in recent years as information and communication technologies have improved . Although the survey asked a variety of questions , in this paper , we focus on three particular areas of interest: What triggers are most likely to lead to CIS activities? When engaging in CIS , what media or channel of communication is most likely used to collaborate? How successful are these CIS activities? In a previous study , we identified three triggers that cause team members to collaborate when seeking information. These triggers are (1) lack of expertise (2) complex information need and (3) information not easily accessible 8 . In this study , we were interested in identifying which of these triggers researchers reported to be the most important reason for them to collaborate when seeking information . We also wanted to identify what were the primary mechanisms of collaboration (e.g. , e-mail , face-to-face , etc.) . We were also interested in determining the Permission to make digital or hard copies of all or part of this work for personal or classroom use", "label": ["academic researchers", "communication media", "information seeking", "group work", "survey", "collaboration", "collaborative information seeking"], "stemmed_label": ["academ research", "commun media", "inform seek", "group work", "survey", "collabor", "collabor inform seek"]}
{"doc": "A transactional agent is a mobile agent which manipulates objects in multiple computers by autonomously finding a way to visit the computers . The transactional agent commits only if its commitment condition like atomicity is satisfied in presence of faults of computers . On leaving a computer , an agent creates a surrogate agent which holds objects manipulated . A surrogate can recreate a new incarnation of the agent if the agent itself is faulty . If a destination computer is faulty , the transactional agent finds another operational computer to visit . After visiting computers , a transactional agent makes a destination on commitment according to its commitment condition . We discuss design and implementation of the transactional agent which is tolerant of computer faults . INTRODUCTION A transaction manipulates multiple objects distributed in computers through methods . Objects are encapsulations of data and methods for manipulating the data . A transaction is modeled to be a sequence of methods which satisfies the ACID (atomicity , consistency , isolation , and dura-bility ) properties 8 , 9 . Huge number and various types of peer computers are interconnected in peer-to-peer (P2P) networks 3 . Personal computers easily get faulty not only by crash but also by hackers and intrusions . A mobile agent can autonomously escape from faulty computers by moving to another operational computer . Mobile agents 5 , 19 are programs which move to remote computers and then locally manipulate objects on the computers. An ACID transaction initiates a subtransaction on each database server , which is realized in mobile agents 16 , 9, 13 . In this paper , a transactional agent is a mobile agent which autonomously decides in which order the agent visits computers in presence of computer faults , and locally manipulates objects in a current computer with not only atomicity but also other types of commitment conditions like at-least-one condition 6 . After manipulating all or some objects in computers , an agent makes a decision on commit or abort. For example , an agent atomically commits only if all objects in the computers are successfully manipulated 4 . An agent commits if objects in at least one of the computers are successfully manipulated . In addition , an agent negotiates with another agent which would like to manipulate a same object in a conflicting manner . Through the negotiation, each agent autonomously makes a decision on whether the agent holds or releases the objects 6 , 14 . If an agent leaves a computer , objects locked by the agent are automatically released . Hence , once leaving a computer, an agent cannot abort . An agent creates a surrogate agent on leaving a computer . A surrogate agent still holds locks on objects in a computer on behalf of the agent after the agent leaves. A transactional agent autonomously finds another destination computer if a destination computer is faulty. An agent and surrogate are faulty if the current computer is faulty . Some surrogate of the agent which exists on another computer", "label": ["fault-tolerant agent", "transactional agent", "transaction", "acid transaction", "surrogate agent", "mobile agent", "fault-tolerant", "fault-tolerant", "computer fault", "mobile agent", "transaction processing"], "stemmed_label": ["fault-toler agent", "transact agent", "transact", "acid transact", "surrog agent", "mobil agent", "fault-toler", "fault-toler", "comput fault", "mobil agent", "transact process"]}
{"doc": "Users' cross-lingual queries to a digital library system might be short and not included in a common translation dictionary (unknown terms) . In this paper , we investigate the feasibility of exploiting the Web as the corpus source to translate unknown query terms for cross-language information retrieval (CLIR) in digital libraries . We propose a Web-based term translation approach to determine effective translations for unknown query terms by mining bilingual search-result pages obtained from a real Web search engine . This approach can enhance the construction of a domain-specific bilingual lexicon and benefit CLIR services in a digital library that only has monolingual document collections . Very promising results have been obtained in generating effective translation equivalents for many unknown terms , including proper nouns , technical terms and Web query terms . INTRODUCTION With the development of digital library technologies , large amounts of library content and cultural heritage material are being digitized all over the world . As digital library systems become commonly constructed and digitized content becomes widely accessible on the Web , digital libraries that cross language and regional boundaries will be in increasingly high demand globally. Unfortunately , most of existing digital library systems only provide monolingual content and search support in certain target languages . To facilitate a cross-language information retrieval (CLIR) service in digital library systems , it is important to develop a powerful query translation engine . This must be able to automatically translate users' queries from multiple source languages to the target languages that the systems accept. Conventional approaches to CLIR incorporate parallel texts 16 as the corpus . These texts contain bilingual sentences , from which word or phrase translations can be extracted with appropriate sentence alignment methods 7 . The basic assumption of such an approach is that queries may be long so query expansion methods can be used to enrich query terms not covered in parallel texts. However , this approach presents some fundamental difficulties for digital libraries that wish to support practical CLIR services . First, since most existing digital libraries contain only monolingual text collections , there is no bilingual corpus for cross-lingual training. Second , real queries are often short , diverse and dynamic so that only a subset of translations can be extracted through the corpora in limited domains . How to efficiently construct a domain-specific translation dictionary for each text collection has become a major challenge for practical CLIR services in digital libraries . In this paper , we propose a Web-based approach to deal with this problem . We intend to exploit the Web as the corpus to find effective translations automatically for query terms not included in a dictionary (unknown terms) . Besides , to speedup online translation process of unknown terms , we extract possible key terms from the document set in digital libraries and try to obtain their translations in advance. For some language pairs , such as Chinese and English , as well as Japanese and English , the Web offers rich texts in a mixture of languages . Many", "label": ["information search and retrieval", "web mining", "term translation", "translation dictionary", "context vector analysis", "unknown cross-lingual queries", "web-based term translation approach", "cross-language information retrieval", "bilingual lexicon construction", "digital library", "pat-tree based local maxima algorithm", "clir services", "term extraction", "digital libraries"], "stemmed_label": ["inform search and retriev", "web mine", "term translat", "translat dictionari", "context vector analysi", "unknown cross-lingu queri", "web-bas term translat approach", "cross-languag inform retriev", "bilingu lexicon construct", "digit librari", "pat-tre base local maxima algorithm", "clir servic", "term extract", "digit librari"]}
{"doc": "A type-indexed function is a function that is defined for each member of some family of types . Haskell's type class mechanism provides collections of open type-indexed functions , in which the indexing family can be extended by defining a new type class instance but the collection of functions is fixed . The purpose of this paper is to present TypeCase: a design pattern that allows the definition of closed type-indexed functions , in which the index family is fixed but the collection of functions is extensible . It is inspired by Cheney and Hinze's work on lightweight approaches to generic programming . We generalise their techniques as a design pattern . Furthermore , we show that type-indexed functions with type-indexed types , and consequently generic functions with generic types , can also be encoded in a lightweight manner , thereby overcoming one of the main limitations of the lightweight approaches . Introduction A type-indexed function is a function that is defined for each member of a family of types . One of the most popular mechanisms implementing this notion is the Haskell 31 type class system . A type class consists of a collection of related type-indexed functions; the family of index types is the set of instances of the type class. Type classes provide just one possible interpretation of the notion of type-indexed functions . In particular , they assume an open-world perspective: the family of index types is extensible , by defining a new type class instance for that type , but the collection of type-indexed functions is fixed in the type class interface so needs to be known in advance . For some applications -- particularly when providing a framework for generic programming -- the family of index types is fixed (albeit large) and the collection of type-indexed functions is not known in advance , so a closed-world perspective would make more sense. The original concept of a design pattern has its origins in Christopher Alexander's work in architecture , but it has been picked up with enthusiasm by the object-oriented programming community. The idea of design patterns is to capture , abstract and record beneficial recurring patterns in software design . Sometimes those patterns can be captured formally , as programming language constructs or software library fragments . Often , however , the appropriate abstraction cannot be directly stated , either because of a lack of expressiveness in the language , or because there is inherent ambiguity in the pattern -- Alexander describes a pattern as a solution `you can use . . . a million times over , without ever doing it the same way twice' 1 . In this case , one must resort to an informal description . Even if the abstraction itself can be captured formally, one might argue that a complete description of the pattern includes necessarily informal information: a name , motivation , examples, consequences , implementation trade-offs , and so on. In this paper , we present a technique that allows the definition of closed type-indexed functions , as opposed", "label": ["generic programming", "type-indexed functions", "type classes"], "stemmed_label": ["gener program", "type-index function", "type class"]}
{"doc": "The research field of Intelligent Service Robots , which has become more and more popular over the last years , covers a wide range of applications from climbing machines for cleaning large storefronts to robotic assistance for disabled or elderly people . When developing service robot software , it is a challenging problem to design the robot architecture by carefully considering user needs and requirements , implement robot application components based on the architecture , and integrate these components in a systematic and comprehensive way for maintainability and reusability . Furthermore , it becomes more difficult to communicate among development teams and with others when many engineers from different teams participate in developing the service robot . To solve these problems , we applied the COMET design method , which uses the industry-standard UML notation , to developing the software of an intelligent service robot for the elderly , called T-Rot , under development at Center for Intelligent Robotics (CIR) . In this paper , we discuss our experiences with the project in which we successfully addressed these problems and developed the autonomous navigation system of the robot with the COMET/UML method . INTRODUCTION Robots have been used in several new applications . In recent years , both academic and commercial research has been focusing on the development of a new generation of robots in the emerging field of service robots . Service robots are individually designed to perform tasks in a specific environment for working with or assisting humans and must be able to perform services semi- or fully automatically 1 . Examples of service robots are those used for inspection , maintenance , housekeeping , office automation and aiding senior citizens or physically challenged individuals 2 . A number of commercialized service robots have recently been introduced such as vacuum cleaning robots , home security robots, robots for lawn mowing , entertainment robots , and guide robots 3 , 4 . In this context , Public Service Robot (PSR) systems have been developed for indoor service tasks at Korea Institute of Science and Technology (KIST) 5 , 6 . The PSR is an intelligent service robot , which has various capabilities such as navigation, manipulation , etc . Up to now , three versions of the PSR systems, that is , PSR-1 , PSR-2 , and a guide robot Jinny have been built. The worldwide aging population and health care costs of aged people are rapidly growing and are set to become a major problem in the coming decades . This phenomenon could lead to a huge market for service robots assisting with the care and support of the disabled and elderly in the future 8 . As a result , a new project is under development at Center for Intelligent Robotics (CIR) at KIST , i.e . the intelligent service robot for the elderly, called T-Rot. In our service robot applications , it is essential to not only consider and develop a well-defined robot software architecture, but also to develop and integrate robot application components in a systematic and", "label": ["software engineering", "object-oriented analysis and design methods", "service robot development", "uml"], "stemmed_label": ["softwar engin", "object-ori analysi and design method", "servic robot develop", "uml"]}
{"doc": "This paper presents a unified utility framework for resource selection of distributed text information retrieval . This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases . With the estimated relevance information , resource selection can be made by explicitly optimizing the goals of different applications . Specifically , when used for database recommendation , the selection is optimized for the goal of high-recall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval , the selection targets the high-precision goal (high precision in the final merged list of documents) . This new model provides a more solid framework for distributed information retrieval . Empirical studies show that it is at least as effective as other state-of-the-art algorithms . INTRODUCTION Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing . Distributed information retrieval , also known as federated search 1,4,7,11,14,22 is different from ad-hoc information retrieval as it addresses the cases when documents cannot be acquired and stored in a single database . For example , \"Hidden Web\" contents (also called \"invisible\" or \"deep\" Web contents) are information on the Web that cannot be accessed by the conventional search engines. Hidden web contents have been estimated to be 2-50 19 times larger than the contents that can be searched by conventional search engines . Therefore , it is very important to search this type of valuable information. The architecture of distributed search solution is highly influenced by different environmental characteristics . In a small local area network such as small company environments , the information providers may cooperate to provide corpus statistics or use the same type of search engines . Early distributed information retrieval research focused on this type of cooperative environments 1,8 . On the other side , in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required . Even if they are willing to cooperate in these environments , it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required. Many applications fall into the latter type of uncooperative environments such as the Mind project 16 which integrates non-cooperating digital libraries or the QProber system 9 which supports browsing and searching of uncooperative hidden Web databases . In this paper , we focus mainly on uncooperative environments that contain multiple types of independent search engines. There are three important sub-problems in distributed information retrieval . First , information about the contents of each individual database must be acquired (resource representation) 1,8,21 . Second , given a query , a set of resources must be selected to do the search (resource", "label": ["resource selection", "distributed information retrieval"], "stemmed_label": ["resourc select", "distribut inform retriev"]}
{"doc": "The presence of \"unwanted\" (or background) traffic in the Internet is a well-known fact . In principle any network that has been engineered without taking its presence into account might experience troubles during periods of massive exposure to unwanted traffic , e.g . during large-scale infections . A concrete example was provided by the spreading of Code-Red-II in 2001 , which caused several routers crashes worldwide . Similar events might take place in 3G networks as well , with further potential complications arising from their high functional complexity and the scarcity of radio resources . For example , under certain hypothetical network configuration settings unwanted traffic , and specifically scanning traffic from infected Mobile Stations , can cause large-scale wastage of logical resources , and in extreme cases even starvation . Unwanted traffic is present nowdays also in GPRS/UMTS , mainly due to the widespread use of 3G connect cards for laptops . We urge the research community and network operators to consider the issue of 3G robustness to unwanted traffic as a prominent research area . INTRODUCTION Public wide-area wireless networks are now migrating to third-generation systems (3G) , designed to support packet-switched data services and Internet access . Several UMTS networks became operational since 2003 while early GPRS deployments date back to 2000 . Since then , the growing popularity of 3G terminals and services has extended the coverage of Internet wireless access to the geographic area, and 3G networks are becoming key components of the global Internet . In a recent CCR contribution Keshav 17 foresees that cell phones will become the dominant component of future Internet population , while Kleinrock expects this role to be played by \"small pervasive devices ubiquitously embedded in the physical world\" (quoted from 14 , . 112 ). Both scenarios underlay that the main access mode in the future Internet will be wide-area wireless . Currently deployed 3G networks , along with their future evolutions , are in pole-position face to concurrent technologies (e.g . WIMAX) to provide such access connectivity in the large-scale. Generally speaking , the 3G network being essentially a mixture of two paradigms , namely mobile cellular and IP , it is exposed to the security and reliability issues affecting each component , plus the new risks emerging from their combination . The 3G environment inherits from the cellular paradigm a number of features like terminal personalization and geolocalization that make privacy and information security particularly critical . When coupled with the IP world, markedly the \"openess\" of its applications and accessibility, the concerns of privacy and security from the user perspective become even more critical than in legacy 2G networks. Because of that - and of some \"lessons learned\" from past mistakes in 2G security 5 - privacy and information security aspects have received a thorough treatment in the 3G specifications (see 7 for an exhaustive overview) . Nevertheless, the specific topic of 3G network security in relation to the robustness and availability of the network infrastructure itself has not received adequate attention by the research community to", "label": ["unwanted traffic", "cellular networks", "3g"], "stemmed_label": ["unwant traffic", "cellular network", "3g"]}
{"doc": "The tools used to search and find Learning Objects in different systems do not provide a meaningful and scalable way to rank or recommend learning material . This work propose and detail the use of Contextual Attention Metadata , gathered from the different tools used in the lifecycle of the Learning Object , to create ranking and recommending metrics to improve the user experience . Four types of metrics are detailed: Link Analysis Ranking , Similarity Recommendation , Personalized Ranking and Contextual Recommendation . While designed for Learning Objects , it is shown that these metrics could also be applied to rank and recommend other types of reusable components like software libraries . INTRODUCTION One of the main reasons to capture and analyze the information about the interaction between a user and a tool is to improve the user experience . For example , a online library system could record the subject of the books that a client has bought before in order to recommend him new books about a similar subject the next time he/she logs in , saving him/her the hassle to search for them 1 . A news web site could record the topic of the news articles that a user normally read in order to filter out news that do not interest such user 2 . A collaborative browser could use the information recollected from the browsing patterns of a given community to improve the rank of different pages on the searches of an individual user , member of that community 3 . The generic name of Attention Metadata 4 has been applied to describe the information about these interactions. When the information stored does not only contain the reference to the user and the action that it performs , but also register when the action took place , through which tool the action was performed , what others thing was doing the user at the same time, what is the profile of the user performing the action , to what community he/she belongs , etc , it leads to an improved and more useful form of record , called Contextualized Attention Metadata 5 (CAM) . AttentionXML 6 and its extensions 5 are an effort to standardize the way in which CAM is stored . This standardization will lead to the opportunity to share attention records between different applications . For example , a second generation of an Attention-Sharing online library could know which news topics the user is interested in and it could recommend him/her books related to those topics. The authors believe that one group of applications that could greatly benefit from CAM information is the search and find of Learning Objects . These applications have suffered from an under-par performance compared to similar applications in other fields 7 8 . The main reason for this is the lack of a meaningful and scalable way to rank or recommend the objects to the users. Currently , two main methods are used to rank (not even recommend) Learning Objects: Manual Rating or Metadata Content Rating . In", "label": ["learning objects", "ranking", "attention metadata", "recommending"], "stemmed_label": ["learn object", "rank", "attent metadata", "recommend"]}
{"doc": "Software systems evolve over time due to changes in requirements , optimization of code , fixes for security and reliability bugs etc . Code churn , which measures the changes made to a component over a period of time , quantifies the extent of this change . We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn . Using statistical regression models , we show that while absolute measures of code churn are poor predictors of defect density , our set of relative measures of code churn is highly predictive of defect density . A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density . Furthermore , our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent . INTRODUCTION A \"reliability chasm\" often separates the quality of a software product observed in its pre-release testing in a software development shop and its post-release use in the field . That is, true field reliability , as measured by the number of failures found by customers over a period of time , cannot be measured before a product has been completed and delivered to a customer . Because true reliability information is available late in the process, corrective actions tend to be expensive 3 . Clearly , software organizations can benefit in many ways from an early warning system concerning potential post-release defects in their product to guide corrective actions to the quality of the software. We use code churn to predict the defect density in software systems . Code churn is a measure of the amount of code change taking place within a software unit over time . It is easily extracted from a system's change history , as recorded automatically by a version control system . Most version control systems use a file comparison utility (such as diff) to automatically estimate how many lines were added , deleted and changed by a programmer to create a new version of a file from an old version . These differences are the basis of churn measures. We create and validate a set of relative code churn measures as early indicators of system defect density . Relative churn measures are normalized values of the various measures obtained during the churn process . Some of the normalization parameters are total lines of code , file churn , file count etc . Munson et al . 17 use a similar relative approach towards establishing a baseline while studying code churn . Studies have shown that absolute measures like LOC are poor predictors of pre- and post release faults 7 in industrial software systems . In general , process measures based on change history have been found be better indicators of fault rates than product metrics of code 9 . In an evolving system it", "label": ["principal component analysis", "relative code churn", "defect density", "fault-proneness", "multiple regression"], "stemmed_label": ["princip compon analysi", "rel code churn", "defect densiti", "fault-pron", "multipl regress"]}
{"doc": "With the underlying W-CDMA technique in 3G networks , resource management is a very significant issue as it can directly influence the system capacity and also lead to system QoS . However , the resource can be dynamically managed in order to maintain the QoS according to the SLA . In this paper , CBR is used as part of an intelligent-based agent management system . It uses information from previously managed situations to maintain the QoS in order to meet the SLA . The results illustrate the performance of an agent in traffic pattern recognition in order to identify the specific type of problem and finally propose the right solution . INTRODUCTION The third generation (3G) cellular system has been developed to satisfy increasing customer demands for higher bit-rate access in order to provide wireless Internet access anytime and anywhere . In addition , 3G networks will integrate different type of services like voice , data , and video. With W-CDMA , all users share the same spectrum and use codes to identify themselves . Hence the whole bandwidth can be reused in every cell . The system is considered a soft capacity system as all users simultaneously transmit so increasing the interference seen by others . The system capacity is , therefore , limited by the total interference that occurs from other users (in the case of the network being uplink-capacity limited) or other base stations (in the case of the network being downlink-capacity limited) and the background noise . The benefit of this technique is therefore providing the flexible , higher bandwidth services , and maintaining the best system capacity . On the other hand , it leads to more complexity in resource management. Previous work 1 introduced the use of intelligent agents in managing the resources to meet the service level agreement (SLA) when congestion occurs . It shows that by using intelligent agents together with the assignment and admission scheme , the system environment can be monitored and the policy that is suitable for that particular situation will be selected and applied to the system. Also the quality of service (QoS) for each particular class of customer can be monitored and controlled according to the SLA . In 2 , Case-Based Reasoning (CBR) is introduced as a mean of giving the agent more \"intelligence\" . The aim of using CBR is so that the problem can be automatically solved by referring to a similar traffic pattern that the system has seen before and kept in the case library . The end solution from the previous case can then be applied immediately to give a fast and efficient response . In this paper , a wider range of traffic situations will be illustrated , which will also show the benefit of using CBR in order to identify different traffic patterns and to propose the best solution . In addition , results show the outcome of system flexibility in giving different priority patterns to customers according to the system requirements. The paper is organised as follows . The agent", "label": ["service level agreement", "intelligent agent and case-based reasoning", "3g resource management"], "stemmed_label": ["servic level agreement", "intellig agent and case-bas reason", "3g resourc manag"]}
{"doc": "Business process modeling focus on describing how activities interact with other business objects while sustaining the organization's strategy . Business objects are object-oriented representations of organizational concepts , such as resources and actors , which collaborate with one another in order to achieve business goals . These objects exhibit different behavior according to each specific collaboration context . This means the perception of a business object depends on its collaborations with other objects . Business process modeling techniques do not clearly separate the multiple collaborative aspects of a business object from its internal aspects , making it difficult to understand objects which are used in different contexts , thus hindering reuse . To cope with such issues , this paper proposes using role modeling as a separation of concerns mechanism to increase the understandability and reusability of business process models . The approach divides a business process model into a business object model and a role model . The business object models deals with specifying the structure and intrinsic behavior of business objects while the role model specifies its collaborative aspects . INTRODUCTION Representing and keeping the alignment between the multiple elements of an organization is fundamental to understand how it operates and how it can be adapted to cope with a changing business environment 5 . This requires understanding how business activities interact and are aligned with other organizational elements while supporting the operation of the business. In the past years , significant work , particularly in the area of business process modeling has been proposed , ranging from general modeling concepts to business automation languages 10 , 16, 17 , 18 . Business process modeling can be used for multiple purposes , such as facilitating human understanding and communication 29 , supporting process improvement and re-engineering through business process analysis and simulation 8 , 17 and automating the execution of business processes 1 , 22 . A business process model captures the relationships that are meaningful to the business between different organizational concepts , such as activities , the resources used by activities and the human or automated actors who perform these activities . Identifying the properties and relationships of these concepts is fundamental to help understanding and evolving the business since it facilitates the communication between stakeholders , business specialists and support system specialists. We model business concepts as classes of business objects in a consistent object-oriented glossary of business concepts from where objects can be composed , specialized and reused. However , fully characterizing the type of a business object , its properties and relationships is not straightforward . This results from a business object generally being used in different contexts and relating to several other business objects in the organization. For example , a business object modeling a Product may be brought into play in several processes , such as Manufacturing , Logistics and Selling . In each of these contexts , it relates with different activities and resources , displaying different and possibly overlapping properties and behavior that are context-dependent . This means the object acts", "label": ["business object", "business process modeling", "role modeling", "organizational engineering"], "stemmed_label": ["busi object", "busi process model", "role model", "organiz engin"]}
{"doc": "Researchers have explored the design of ambient information systems across a wide range of physical and screen-based media . This work has yielded rich examples of design approaches to the problem of presenting information about a user's world in a way that is not distracting , but is aesthetically pleasing , and tangible to varying degrees . Despite these successes , accumulating theoretical and craft knowledge has been stymied by the lack of a unified vocabulary to describe these systems and a consequent lack of a framework for understanding their design attributes . We argue that this area would significantly benefit from consensus about the design space of ambient information systems and the design attributes that define and distinguish existing approaches . We present a definition of ambient information systems and a taxonomy across four design dimensions: Information Capacity , Notification Level , Representational Fidelity , and Aesthetic Emphasis . Our analysis has uncovered four patterns of system design and points to unexplored regions of the design space , which may motivate future work in the field . INTRODUCTION From the very first formulation of Ubiquitous Computing , the idea of a calmer and more environmentally integrated way of displaying information has held intuitive appeal . Weiser called this \"calm computing\" 35 and described the area through an elegant example: a small , tangible representation of information in the world , a dangling string that would wiggle based on network traffic . When information can be conveyed via calm changes in the environment , users are more able to focus on their primary work tasks while staying aware of non-critical information that affects them . Research in this sub-domain goes by various names including \"ambient displays\" , \"peripheral displays\" , and \"notification systems\" . The breadth of the systems in these broad categories is quite large . We seek to disentangle the terminology used to describe and categorize the wide array of systems in order to provide a common language for discussing research therein. An ambient display can represent many types of data , from stock prices , to weather forecasts , to the presence or absence of colleagues . Maintaining awareness of co-located and distant work and social groups has been a long-term research thread in the area of Computer Supported Cooperative Work (CSCW) 5 , 8 . The Tangible Media Group at the MIT Media Lab , directed by Ishii, also helped shape the field of ambient computation . They coined the term \"tangible media,\" citing inspiration from Weiser's vision 35 and from Pederson and Sokoler's AROMA system 29 and developed AmbientROOM 17 and Ambient Fixtures 6 , 18 . These systems use ambient displays to make people aware of both group activity and other information such as network traffic. Recent work in Ambient Intelligence has brought techniques from Artificial Intelligence to ambient systems , spearheaded by the Disappearing Computer initiative of the European Union 31 . This research thrust seeks to imbue ambient systems with contextual knowledge about the environment . The Roomware project has resulted in smart architectural", "label": ["peripheral display", "four main design patterns", "calm computing", "symbolic sculptural display", "high throughput textual display", "notification system", "information monitor display", "ambient information system", "taxonomy", "framework to understand design attributes", "user interface", "notification systems and peripheral displays", "design guidelines", "multiple-information consolidators", "ambient display", "definition and characteristics of ambient systems", "ubiquitous computing"], "stemmed_label": ["peripher display", "four main design pattern", "calm comput", "symbol sculptur display", "high throughput textual display", "notif system", "inform monitor display", "ambient inform system", "taxonomi", "framework to understand design attribut", "user interfac", "notif system and peripher display", "design guidelin", "multiple-inform consolid", "ambient display", "definit and characterist of ambient system", "ubiquit comput"]}
{"doc": "ABSTRACT Personalized information agents can help overcome some of the limitations of communal Web information sources such as portals and search engines . Two important components of these agents are: user profiles and information filtering or gathering services . Ideally , these components can be sep-arated so that a single user profile can be leveraged for a variety of information services . Toward that end , we are building an information agent called SurfAgent;in previous studies , we have developed and tested methods for automatically learning a user profile 20 . In this paper , we evaluate alternative methods for recommending new documents to a user by generating queries from the user profile and submitting them to a popular search engine . Our study focuses on three questions: How do different algorithms for query generation perform relative to each other? Is positive relevance feedback adequate to support the task? Can a user profile be learned independent of the service? We found that three algorithms appear to excel and that using only positive feedback does degrade the results somewhat . We conclude with the results of a pilot user study for assessing interaction of the profile and the query generation mechanisms . INTRODUCTION The Web has become an indispensable source of information for many people . Based on surveys of the most popular Web sites 14 , users deal with the overwhelming amount and constantly updating nature of the information by routinely visiting hub sites (e.g. , Netscape , Yahoo , CNN) and making copious use of search engines (e.g. , AltaVista , Excite , Magellan ) . Users have derived tremendous leverage from shared information resources such as those just mentioned . Hubs or portals provide communally useful information about perennial (e.g. , financial management , child rearing) and timely (e.g. , September 11 events , stock quote) topics . Search engines satisfy specific , spontaneous information needs. As our appetite for information increases , so does its availability on the Web . Studies (e.g. , 21 , 12 ) have identified limitations with these tools for satisfying users' needs;for example , users appear to lack the motivation to learn how to formulate complex queries or to view long lists of potential matches . Meta-search engines , such as Meta-Crawler 18 , SavvySearch 6 , and NECI 11 , propose to overcome the Web coverage problem by combining the indexing power of multiple stand-alone search engines . However , because they leverage the capabilities of many search engines , they tend to generalize the search task: limiting the access to search-engine -specific advanced search capabilities and , perhaps, introducing even more noise into the return results. One promising approach to compensating for the limitations is to personalize the tools . Pretschner and Gauch divide personalization into two types: personalized access to resources and filtering/ranking 15 . For example , My Yahoo (http://my.yahoo.com) provides personalized access by allowing users to design their own Yahoo page with pertinent information;many search and meta-search engines support some customization (e.g. , types of search , return amount", "label": ["query generation", "information agents", "user modeling"], "stemmed_label": ["queri gener", "inform agent", "user model"]}
{"doc": "This paper presents a novel macroblock mode decision algorithm for inter-frame prediction based on machine learning techniques to be used as part of a very low complexity MPEG-2 to H.264 video transcoder . Since coding mode decisions take up the most resources in video transcoding , a fast macro block (MB) mode estimation would lead to reduced complexity . The proposed approach is based on the hypothesis that MB coding mode decisions in H.264 video have a correlation with the distribution of the motion compensated residual in MPEG-2 video . We use machine learning tools to exploit the correlation and derive decision trees to classify the incoming MPEG-2 MBs into one of the 11 coding modes in H.264 . The proposed approach reduces the H.264 MB mode computation process into a decision tree lookup with very low complexity . The proposed transcoder is compared with a reference transcoder comprised of a MPEG-2 decoder and an H.264 encoder . Our results show that the proposed transcoder reduces the H.264 encoding time by over 95% with negligible loss in quality and bitrate . INTRODUCTION During the past few years , technological developments , such as novel video coding algorithms , lower memory costs , and faster processors , are facilitating the design and development of highly efficient video encoding standards . Among the recent works in this area , the H.264 video encoding standard , also known as MPEG-4 AVC occupies a central place 1 . The H.264 standard is highly efficient by offering perceptually equivalent video quality at about 1/3 to 1/2 of the bitrates offered by the MPEG-2 format . However , these gains come with a significant increase in encoding and decoding complexity 2 . Though H.264 is highly efficient compared to MPEG-2 , the wide and deep penetration of MPEG-2 creates a need for co-existence of these technologies and hence creates an important need for MPEG-2 to H.264 transcoding technologies . However , given the significant differences between both encoding algorithms , the transcoding process of such systems is much more complex compared to the other heterogeneous video transcoding processes 3-6 . The main elements that require to be addressed in the design of an efficient heterogeneous MPEG-2 to H.264 transcoder are 7 : the inter-frame prediction , the transform coding and the intra-frame prediction . Each one of these elements requires to be examined and various research efforts are underway . In this paper , we focus our attention on a part of the inter-frame prediction: the macroblock mode decision , one of the most stringent tasks involved in the transcoding process. A video transcoder is comprised of a decoding stage followed by an encoding stage . The decoding stage of a transcoder can perform full decoding to the pixel level or partial decoding to the coefficient level . Partial decoding is used in compressed domain transcoding where the transform coefficients in the input format are directly transcoded to the output format . This transformation is straightforward when the input and output formats of the transcoder use the same", "label": ["h.264", "inter-frame", "machine learning", "transcoding", "mpeg-2"], "stemmed_label": ["h.264", "inter-fram", "machin learn", "transcod", "mpeg-2"]}
{"doc": "The emergence of third-generation (3G) mobile networks offers new opportunities for the effective delivery of data with rich content including multimedia messaging and video-streaming . Provided that streaming services have proved highly successful over stationary networks in the past , we anticipate that the same trend will soon take place in 3G networks . Although mobile operators currently make available pertinent services , the available resources of the underlying networks for the delivery of rich data remain in-herently constrained . At this stage and in light of large numbers of users moving fast across cells , 3G networks may not be able to warrant the needed quality-of-service requirements . The support for streaming services necessitates the presence of content or media servers properly placed over the 3G network; such servers essen-tially become the source for streaming applications . Evidently , a centralized approach in organizing streaming content might lead to highly congested media-nodes which in presence of moving users will certainly yield increased response times and jitter to user requests . In this paper , we propose a workaround that enables 3G networks to offer uninterrupted video-streaming services in the presence of a large number of users moving in high-speed . At the same time , we offer a distributed organization for the network's media-servers to better handle over-utilization . INTRODUCTION The third generation ( 3G) mobile phone system UMTS enables better quality and allows for more convenient use of multimedia messaging and video-services by offering higher bandwidth and lower latency than its GSM and GPRS predecessors 15 , 1 . UMTS furnishes upto 2Mbps rates for indoor and upto 384Kbps for outdoor environments . Clearly , much improvement in terms of allo-cated resources has been made for the handling of \"rich\" data including multimedia messages and video-services . Nevertheless , the available resources still present significant limitations for the scale up of services when very large number of clients are present in a cell of a network . Perhaps , the most daunting challenge comes from moving users who access multimedia data and video feeds with the help of their mobile phones and PDAs while traveling on either private vehicles or mass transportation means such as commuter trains and public busses . Evidently , a large number of concurrent connections soliciting data resources in a cell and being handled in real-time pose significant capacity problems for the underlying 3G network . The situation becomes even more challenging when users attempt to follow streaming sources while on the move . We consider streaming services to be of key importance as they will ultimately offer information on demand for the moving user in any geographic position and at any time . In order to facilitate video streaming over UMTS networks a number of issues have to be addressed so that users do not experience delays and discontinuities during playback . The two core aspects that require attention are the variations of the available bandwidth as users enter and leave cells as well as the effective management of handovers as roaming users attach to", "label": ["mobile multimedia services", "rate adaptation", "real-time streaming", "streaming for moving users"], "stemmed_label": ["mobil multimedia servic", "rate adapt", "real-tim stream", "stream for move user"]}
{"doc": "We address the problem of integrating objects from a source taxonomy into a master taxonomy . This problem is not only currently pervasive on the web , but also important to the emerging semantic web . A straightforward approach to automating this process would be to learn a classifier that can classify objects from the source taxonomy into categories of the master taxonomy . The key insight is that the availability of the source taxonomy data could be helpful to build better classifiers for the master taxonomy if their categorizations have some semantic overlap . In this paper , we propose a new approach , co-bootstrapping , to enhance the classification by exploiting such implicit knowledge . Our experiments with real-world web data show substantial improvements in the performance of taxonomy integration . INTRODUCTION A taxonomy , or directory or catalog , is a division of a set of objects (documents , images , products , goods , services , etc.) into a set of categories . There are a tremendous number of taxonomies on the web , and we often need to integrate objects from a source taxonomy into a master taxonomy. This problem is currently pervasive on the web , given that many websites are aggregators of information from various other websites 2 . A few examples will illustrate the scenario . A web marketplace like Amazon may want to combine goods from multiple vendors' catalogs into its own . A web portal like NCSTRL may want to combine documents from multiple libraries' directories into its own . A company may want to merge its service taxonomy with its partners' . A researcher may want to merge his/her bookmark taxonomy with his/her peers'. Singapore-MIT Alliance , an innovative engineering education and research collaboration among MIT , NUS and NTU , has a need to integrate the academic resource (courses , seminars, reports , softwares , etc.) taxonomies of these three universities. This problem is also important to the emerging semantic web 4 , where data has structures and ontologies describe the semantics of the data , thus better enabling computers and people to work in cooperation . On the semantic web , data often come from many different ontologies , and information processing across ontologies is not possible without knowing the semantic mappings between them . Since taxonomies are central components of ontologies , ontology mapping necessarily involves finding the correspondences between two taxonomies , which is often based on integrating objects from one taxonomy into the other and vice versa 10 , 14 . If all taxonomy creators and users agreed on a universal standard, taxonomy integration would not be so difficult . But the web has evolved without central editorship . Hence the correspondences between two taxonomies are inevitably noisy and fuzzy . For illustration , consider the taxonomies of two web portals Google and Yahoo : what is \"Arts/Music/Styles/\" in one may be \"Entertainment/Music/Genres/\" in the other , category \"Computers_and_Internet/Software/Freeware\" and category \"Computers/Open_Source/Software\" have similar contents but show non-trivial differences , and so on . It is", "label": ["taxonomy integration", "bootstrapping", "semantic web", "classification", "ontology mapping", "machine learning", "boosting"], "stemmed_label": ["taxonomi integr", "bootstrap", "semant web", "classif", "ontolog map", "machin learn", "boost"]}
{"doc": "Today web search engines provide the easiest way to reach information on the web . In this scenario , more than 95% of Indian language content on the web is not searchable due to multiple encodings of web pages . Most of these encodings are proprietary and hence need some kind of standardization for making the content accessible via a search engine . In this paper we present a search engine called WebKhoj which is capable of searching multi-script and multi-encoded Indian language content on the web . We describe a language focused crawler and the transcoding processes involved to achieve accessibility of Indian langauge content . In the end we report some of the experiments that were conducted along with results on Indian language web content . INTRODUCTION India is a multi-language , multi-script country with 22 official languages and 11 written script forms . About a billion people in India use these languages as their first language. English , the most common technical language , is the lingua franca of commerce , government , and the court system , but is not widely understood beyond the middle class and those who can afford formal , foreign-language education . Not only is there a large societal gap between the rich and poor , but that gap appears to be widening due the dominance of English in the society . About 5% of the population (usually the educated class) can understand English as their second language . Hindi is spoken by about 30% 5 of the population, but it is concentrated in urban areas and north-central India , and is still not only foreign but often unpopular in many other regions . Computability of Indian languages could help bridge the societal gaps in education , economy and health-care . However the research and development , availability of standards , support from operating systems and applications in these directions moved very slow due to language heterogeneity. Today this phenomenon can also be observed on the world wide web . The percentage of Indian language content is very less compared to the official languages of United Nations 7 . Even within the available content , majority is not searchable and hence not reachable due to multiple encodings used while authoring such websites . Web publishers of such content were hesitant to use any available standards such as Unicode due to very delayed support from operating systems and browsers in rendering Indic scripts . Even today Hindi is rendered properly only on Windows XP and beyond . Linux has very little support for these languages. Indian languages had barely any support till Windows 2000 operating system . This creates a major bottleneck for web publishers in these languages to get viewership. Despite all these issues , we found considerable amount of content being published on the web . However such content gets unnoticed or gets very less viewership since most of such content is not accessible through search engines due to nonstandard encodings being rendered using proprietary fonts. This paper is organized into seven sections", "label": ["web search", "indian languages", "non-standard encodings"], "stemmed_label": ["web search", "indian languag", "non-standard encod"]}
{"doc": "Some large scale topical digital libraries , such as CiteSeer , harvest online academic documents by crawling open-access archives , university and author homepages , and authors' self-submissions . While these approaches have so far built reasonable size libraries , they can suffer from having only a portion of the documents from specific publishing venues . We propose to use alternative online resources and techniques that maximally exploit other resources to build the complete document collection of any given publication venue . We investigate the feasibility of using publication metadata to guide the crawler towards authors' homepages to harvest what is missing from a digital library collection . We collect a real-world dataset from two Computer Science publishing venues , involving a total of 593 unique authors over a time frame of 1998 to 2004 . We then identify the missing papers that are not indexed by CiteSeer . Using a fully automatic heuristic-based system that has the capability of locating authors' homepages and then using focused crawling to download the desired papers , we demonstrate that it is practical to harvest using a focused crawler academic papers that are missing from our digital library . Our harvester achieves a performance with an average recall level of 0.82 overall and 0.75 for those missing documents . Evaluation of the crawler's performance based on the harvest rate shows definite advantages over other crawling approaches and consistently outperforms a defined baseline crawler on a number of measures . INTRODUCTION Digital libraries that are based on active crawling methods such as CiteSeer often have missing documents in collections of archived publications , such as ACM and IEEE . How do such digital libraries find and obtain those missing? We propose using external resources of publication metadata and focused crawlers to search the Web for those missing. The basic concept of a focused crawler (also known as a topical crawlers) 1 , is based on a crawling strategy that relevant Web pages contain more relevant links , and these relevant links should be explored first . Initially , the measure of relevancy was based on keywords matching; connectivity-based metrics were later introduced 2 . In 3 the concept of a focused crawler was formally introduced: a crawler that seeks , acquires , indexes , and maintains pages on a specific set of topics that represent a relatively narrow segment of the Web. Today , focused crawling techniques have become more important for building specialty and niche (vertical) search engines While both the sheer volume of the Web and its highly dynamic content increasingly challenge the task of document collection , digital libraries based on crawling benefit from focused crawlers since they can quickly harvest a high-quality subset of the relevant online documents. Current approaches to harvesting online academic documents normally consist of focused crawling of open-access archives, author and institution web sites and directories of authors' self-submissions . A random sample of 150 journals and conferences in Computer Science show that less than 10% have websites that are open to crawlers . Many of the top", "label": ["digital libraries", "citeseer", "focused crawler", "dblp", "harvesting", "acm"], "stemmed_label": ["digit librari", "cites", "focus crawler", "dblp", "harvest", "acm"]}
{"doc": "Hidden Web databases maintain a collection of specialised documents , which are dynamically generated in response to users' queries . However , the documents are generated by Web page templates , which contain information that is irrelevant to queries . This paper presents a Two-Phase Sampling (2PS) technique that detects templates and extracts query-related information from the sampled documents of a database . In the first phase , 2PS queries databases with terms contained in their search interface pages and the subsequently sampled documents . This process retrieves a required number of documents . In the second phase , 2PS detects Web page templates in the sampled documents in order to extract information relevant to queries . We test 2PS on a number of real-world Hidden Web databases . Experimental results demonstrate that 2PS effectively eliminates irrelevant information contained in Web page templates and generates terms and frequencies with improved accuracy . INTRODUCTION An increasing number of databases on the Web maintain a collection of documents such as archives , user manuals or news articles . These databases dynamically generate documents in response to users' queries and are referred to as Hidden Web databases 5 . As the number of databases proliferates , it has become prohibitive for specialised search services (such as search.com) to evaluate databases individually in order to answer users' queries. Current techniques such as database selection and categorisation have been employed to enhance the effectiveness of information retrieval from databases 2 , 5 , 10 , 11 , 15 . In the domain of the Hidden Web , knowledge about the contents of databases is often unavailable . Existing approaches such as in 2 , 10 , 15 acquire knowledge through sampling documents from databases . For instance , query-based sampling 2 queries databases with terms that are randomly selected from those contained in the sampled documents . The techniques in 10 , 15 sample databases with terms obtained from Web logs to retrieve additional topic terms. A major issue associated with existing techniques is that they also extract information irrelevant to queries . That is , information extracted is often found in Web page templates , which contain navigation panels , search interfaces and advertisements. Consequently , the accuracy of terms and frequencies generated from sampled documents has been reduced. In addition , approximate string matching techniques are adopted by 13 to extract information from Web pages , but this approach is limited to textual contents only . Alternatively , the approaches proposed in 3 , 4 analyse Web pages in tree-like structures. However , such an approach requires Web pages with well-conformed HTML tag trees . Furthermore , 3 discovers dynamically generated objects from Web pages , which are clustered into groups of similar structured pages based on a set of pre-defined templates , such as exception page templates and result page templates. In this paper , we propose a sampling and extraction technique, which is referred to as Two-Phase Sampling (2PS) . 2PS aims to extract information relevant to queries in order to acquire information", "label": ["hidden web databases", "search interface pages", "information extraction", "hypertext markup langauges", "hidden web databases", "2-phase sampling technique", "neighbouring adjacent tag segments", "string matching techniques", "information extraction", "web page templates", "document sampling", "query-based sampling", "irrelavant information extraction"], "stemmed_label": ["hidden web databas", "search interfac page", "inform extract", "hypertext markup langaug", "hidden web databas", "2-phase sampl techniqu", "neighbour adjac tag segment", "string match techniqu", "inform extract", "web page templat", "document sampl", "query-bas sampl", "irrelav inform extract"]}
{"doc": "In this paper , we introduce a unified approach for the adaptive control of 3G mobile networks in order to improve both quality of service (QoS) for mobile subscribers and to increase revenue for service providers . The introduced approach constantly monitors QoS measures as packet loss probability and the current number of active mobile users during operation of the network . Based on the values of the QoS measures just observed , the system parameters of the admission controller and packet scheduler are controlled by the adaptive performance management entity . Considering UMTS , we present performance curves showing that handover failure probability is improved by more than one order of magnitude . Moreover , the packet loss probability can be effectively regulated to a predefined level and provider revenue is significantly increased for all pricing policies . Introduction The third generation (3G) of mobile networks is expected to complete the worldwide globalization process of mobile communication . Since different parts of the worlds emphasize different issues , the global term 3G has regional synonyms : In the US and Japan , 3G often carries the name International Mobile Telephony 2000 (IMT2000) . In Europe, 3G has become Universal Mobile Telecommunications System (UMTS) following the ETSI perspective . The European industrial players have created the 3rd Generation Partnership Project (3GPP) 1 for the standardization of UMTS. 3G mobile networks provide the foundation for new services with high-rate data not provided by current second generation systems 26 . While the standardization of 3G is still ongoing the discussion of technical issues beyond 3G has already started 23,28 . Recently , Aretz et al . reported a vision for the future of wireless communication systems beyond 3G that consists of a combination of several optimized access systems on a common IP-based medium access and core network platform 5 . Charging and pricing are essential issues for network operations of 3G mobile networks . A primary target of differen-tiated pricing of Internet services is the prevention of system overload and an optimal resource usage according to different daytimes and different traffic intensities 12 . Among the proposed pricing proposals , flat-rate pricing 11 is the most common mode of payment today for bandwidth services. Flat-rate pricing is popular because of its minimal accounting overhead . A flat-rate encourages usage but does not offer any motivation for users to adjust their demand . Dynamic pricing models that take the state of the network into account in the price determination have been proposed as being more Corresponding author. responsive . Usage-based pricing regulates usage by imposing a fee based on the amount of data actually sent , whereas congestion-sensitive pricing uses a fee based on the current state of congestion in the network . Thus , a unified approach considering both dynamic pricing and controlling quality of service (i.e. , performance management) provides an effective tool for the operation of 3G mobile networks . However , in previous work 8,13,19,21,25 the improvement of Quality of Service (QoS) in 3G mobile networks and the optimization of mobile", "label": ["qos", "packet loss probability", "quality of service in mobile systems", "provider revenue", "performance evaluation of next generation mobile systems", "packet scheduler", "adaptive performance management", "admission control in mobile system", "pricing policy", "admission control", "3g mobile networks", "pricing and revenue optimization"], "stemmed_label": ["qo", "packet loss probabl", "qualiti of servic in mobil system", "provid revenu", "perform evalu of next gener mobil system", "packet schedul", "adapt perform manag", "admiss control in mobil system", "price polici", "admiss control", "3g mobil network", "price and revenu optim"]}
{"doc": "Facet-based component retrieval techniques have been proved to be an effective way for retrieving . These Techniques are widely adopted by component library systems , but they usually simply list out all the retrieval results without any kind of ranking . In our work , we focus on the problem that how to determine the ranks of the components retrieved by user . Factors which can influence the ranking are extracted and identified through the analysis of ER-Diagram of facet-based component library system . In this paper , a mathematical model of weighted ranking algorithm is proposed and the timing of ranks calculation is discussed . Experiment results show that this algorithm greatly improves the efficiency of component retrieval system . Motivations A high efficiency retrieval system for software component library is important for the reuse of software components . The point of high efficiency is not that the time performance in one matching or retrieving process which can be measured by how many seconds or how many milliseconds elapsed , but that the efficiency to make the component consumers be able to find what they need as soon as possible , even though the former is the basis of the latter. No matter accuracy matching or fuzzy matching , our component retrieval system usually simply lists out all the retrieval results without any kind of ranking , or at least without a systematic ranking . Users have to view the detail information of all the retrieval results one by one to find out which is the best to fit their requirements , or else they have to adjust their query conditions to retrieve again. If there are a large number of components retrieved from the component library , it could be a tough and torturous experience to find a proper component . However , it's a fact that there's a matching degree between the query conditions and retrieval results . The matching degree is just the similarity and relevancy between the query condition and its retrieval results . Only when we rank the retrieval results by the matching degree as the Web search engines can component consumers easily find what they need . They only have to compare the first several retrieval results but not all of them. According to the discussion above , it's clear that a formula to calculate the matching degree and its corresponding ranking algorithm , which can greatly improve the retrieval efficiency for software component library , are needed . In this paper , we propose a weighted ranking algorithm for facet-based component retrieval system . This algorithm has been implemented in a software component library , called DLCL , and greatly improves the efficiency of the retrieval system. Introduction to Retrieval Methods for Component Library 2.1 Existing Retrieval Methods for Component Library Retrieval of software components is a core technique of component library . Today there are lots of retrieval methods for software component library . The main are as follows 1 , 2 : (1) Specification matching method; (2) AI Based method; (3) Information", "label": ["retrieval system", "facet", "component rank", "component retrieval", "and component library", "ranking algorithm", "weighted ranking algorithm", "matching degree", "facet-based component retrieval", "component library"], "stemmed_label": ["retriev system", "facet", "compon rank", "compon retriev", "and compon librari", "rank algorithm", "weight rank algorithm", "match degre", "facet-bas compon retriev", "compon librari"]}
{"doc": "The organization of HTML into a tag tree structure , which is rendered by browsers as roughly rectangular regions with embedded text and HREF links , greatly helps surfers locate and click on links that best satisfy their information need . Can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen HREF target page w.r.t . an information need , based on information limited to the HREF source page? Such a capability would be of great interest in focused crawling and resource discovery , because it can fine-tune the priority of unvisited URLs in the crawl frontier , and reduce the number of irrelevant pages which are fetched and discarded . We show that there is indeed a great deal of usable information on a HREF source page about the relevance of the target page . This information , encoded suitably , can be exploited by a supervised apprentice which takes online lessons from a traditional focused crawler by observing a carefully designed set of features and events associated with the crawler . Once the apprentice gets a sufficient number of examples , the crawler starts consulting it to better prioritize URLs in the crawl frontier . Experiments on a dozen topics using a 482-topic taxonomy from the Open Directory (Dmoz) show that online relevance feedback can reduce false positives by 30% to 90% . Introduction Keyword search and clicking on links are the dominant modes of accessing hypertext on the Web. Support for keyword search through crawlers and search engines is very mature , but the surfing paradigm is not modeled or assisted (Note: The HTML version of this paper is best viewed using Microsoft Internet Explorer . To view the HTML version using Netscape , add the following line to your ~/.Xdefaults or ~/.Xresources file: Netscape*documentFonts.charset*adobe-fontspecific: iso-8859-1 For printing use the PDF version , as browsers may not print the mathematics properly.) Contact author , email soumen@cse.iitb.ac.in Copyright is held by the author/owner(s). WWW2002 , May 711 , 2002 , Honolulu , Hawaii , USA. ACM 1-58113-449-5/02/0005 Baseline learner Dmoz topic taxonomy Class models consisting of term stats Frontier URLS priority queue Crawler Pick best Newly fetched page u Submit page for classification If Pr(c*|u) is large enough then enqueue all outlinks v of u with priority Pr(c*|u) Crawl database Seed URLs Figure 1: A basic focused crawler controlled by one topic classifier/learner. as well . Support for surfing is limited to the basic interface provided by Web browsers , except for a few notable research prototypes. While surfing , the user typically has a topic-specific information need , and explores out from a few known relevant starting points in the Web graph (which may be query responses) to seek new pages relevant to the chosen topic/s . While deciding for or against clicking on a specific link (u , v) , humans use a variety of clues on the source page u to estimate the worth of the (unseen) target page v , including the tag tree structure of u ,", "label": ["focused crawlers", "reinforcement learning", "urls", "focused crawling", "taxonomy", "dom", "href link", "classifiers", "document object model"], "stemmed_label": ["focus crawler", "reinforc learn", "url", "focus crawl", "taxonomi", "dom", "href link", "classifi", "document object model"]}
{"doc": "Many volume filtering operations used for image enhancement , data processing or feature detection can be written in terms of three-dimensional convolutions . It is not possible to yield interactive frame rates on todays hardware when applying such convolutions on volume data using software filter routines . As modern graphics workstations have the ability to render two-dimensional convoluted images to the frame buffer , this feature can be used to accelerate the process significantly . This way generic 3D convolution can be added as a powerful tool in interactive volume visualization toolkits . Introduction Direct volume rendering is a very important technique for visualizing three dimensional data . Several fundamental different methods have been introduced 2 , 4 , 5 , 6 , 8 , 12 . Hardware-based volume texturing 9 , 14 is one of the most prominent variants for interactive visualization due to the high frame rates that can be achieved with this technique. The basic principle of texture based volume rendering is depicted in Figure 1 . According to the sampling theorem a 3D view of the volume is generated by drawing an adequate number of equidistant, semi-transparent polygons parallel to the image plane with respect to the current viewing direction (\"volume slicing\"). Filtering on the other hand is a major part of the visualization pipeline . It is broadly used for improving images , reducing noise, and enhancing detail structure . Volume rendering can benefit from filter operations , as low pass filters reduce the noise e.g . of sam-pled medical volume images and high pass filters can be used for edge extraction , visualizing prominent data features . Multiscale approaches as 13 regularly use disjunct filtering and downsampling steps and can benefit from any speedups of the filtering process. Filters can be classified as linear or non-linear . Discrete linear filters can be written as convolutions with filter kernels that completely specify the filtering operation . Non-linear filters , as for instance morphological operators , were recently used for volume analysis and visualization 7 . Segmentation and classification depend heavily on filtering operations as well . Bro-Nielson 1 already thought about using convolution hardware for accelerating the registration process. For texture based volume rendering the data set has to be loaded into special texture memory , which can be addressed by the graphics pipe very fast . The loading process itself is relatively slow , taking several seconds for big data sets even on the fastest available graphics workstations . As the data set has to be reloaded after a filter operation has been performed in software , interactive filtering will benefit a lot from convolution algorithms that directly work on the texture hardware . Additionally , we will show in the following that computing the convolution with graphics hardware is much faster than software solutions. 3D Convolution The general three-dimensional discrete convolution can be written as ~ f (x , y , z) = i 1 ,i 2 ,i 3 k(i 1 , i 2 , i 3 ) f(x + i 1 , y +", "label": ["3d convolution", "convolution", "visualization", "filtering", "volume visualization", "hardware acceleration", "volume rendering"], "stemmed_label": ["3d convolut", "convolut", "visual", "filter", "volum visual", "hardwar acceler", "volum render"]}
{"doc": "As today the amount of accessible information is overwhelming , the intelligent and personalized filtering of available information is a main challenge . Additionally , there is a growing need for the seamless mobile and multi-modal system usage throughout the whole day to meet the requirements of the modern society (\"anytime , anywhere , anyhow\") . A personal information agent that is delivering the right information at the right time by accessing , filtering and presenting information in a situation-aware matter is needed . Applying Agent-technology is promising , because the inherent capabilities of agents like autonomy , pro- and reactiveness offer an adequate approach . We developed an agent-based personal information system called PIA for collecting , filtering , and integrating information at a common point , offering access to the information by WWW , e-mail , SMS , MMS , and J2ME clients . Push and pull techniques are combined allowing the user to search explicitly for specific information on the one hand and to be informed automatically about relevant information divided in pre- , work and recreation slots on the other hand . In the core of the PIA system advanced filtering techniques are deployed through multiple filtering agent communities for content-based and collaborative filtering . Information-extracting agents are constantly gathering new relevant information from a variety of selected sources (internet , files , databases , web-services etc.) . A personal agent for each user is managing the individual information provisioning , tailored to the needs of this specific user , knowing the profile , the current situation and learning from feedback . Introduction Nowadays , desired information often remains unfound, because it is hidden in a huge amount of unnecessary and irrelevant data . On the Internet there are well maintained search engines that are highly useful if you want to do full-text keyword-search 1 , but they are not able to support you in a personalized way and typically do not offer any \"push-services\" or in other words no information will be sent to you when you are not active . Also , as they normally do not adapt themselves to mobile devices , they cannot be used throughout a whole day because you are not sitting in front of a standard browser all the time and when you return , these systems will treat you in the very same way as if you have never been there before (no personalization , no learning) . Users who are not familiar with domain-specific keywords won't be able to do successful research , because no support is offered . Predefined or auto-generated keywords for the search-domains are needed to fill that gap . As information demands are continuously increasing today and the gathering of information is time-consuming , there is a growing need for a personalized support . Labor-saving information is needed to increase productivity at work and also there is an increasing aspiration for a personalized offer of general information , specific domain knowledge , entertainment, shopping , fitness , lifestyle and health information . Existing commercial \"personalized\"", "label": ["adaptation and learning", "filtering", "recommendation systems", "agent-based deployed applications", "evolution", "intelligent and personalized filtering", "agents and complex systems", "personal information agent", "agent technology", "ubiquitous access"], "stemmed_label": ["adapt and learn", "filter", "recommend system", "agent-bas deploy applic", "evolut", "intellig and person filter", "agent and complex system", "person inform agent", "agent technolog", "ubiquit access"]}
{"doc": "In this paper we present a multilingual information retrieval system that provides access to Tourism information by exploiting the intuitiveness of natural language . In particular , we describe the knowledge representation model underlying the information retrieval system . This knowledge representation approach is based on associative networks and allows the definition of semantic relationships between domain-intrinsic information items . The network structure is used to define weighted associations between information items and augments the system with a fuzzy search strategy . This particular search strategy is performed by a constrained spreading activation algorithm that implements information retrieval on associative networks . Strictly speaking , we take the relatedness of terms into account and show , how this fuzzy search strategy yields beneficial results and , moreover , determines highly associated matches to users' queries . Thus , the combination of the associative network and the constrained spreading activation approach constitutes a search algorithm that evaluates the relatedness of terms and , therefore , provides a means for implicit query expansion . Introduction Providing easy and intuitive access to information still remains a challenge in the area of information system research and development . Moreover , as Van Rijsbergen (1979) points out , the amount of available information is increasing rapidly and offering accurate and speedy access to this information is becoming ever more difficult . This quote , although about 20 years old , is still valid nowadays if you consider the amount of information offered on the Internet . But how to address these problems? How to overcome the limitations associated with conventional search interfaces ? Furthermore , users of information retrieval systems are often computer illiterate and not familiar with the required logic for formulating appropriate queries , e.g . the burdens associated with Boolean Copyright c 2004 , Australian Computer Society , Inc . This paper appeared at First Asia-Pacific Conference on Conceptual Modelling (APCCM 2004) , Dunedin , New Zealand . Conferences in Research and Practice in Information Technology , Vol. 31 . Sven Hartmann and John Roddick , Ed . Reproduction for academic , not-for profit purposes permitted provided this text is included. logic . This goes hand in hand with the urge to understand what users really want to know from information retrieval systems. Standard information retrieval interfaces consist of check boxes , predefined option sets or selection lists forcing users to express her or his needs in a very restricted manner . Therefore , an approach leaving the means of expression in users' hands , narrows the gap between users' needs and interfaces used to express these needs . An approach addressing this particular problem is to allow query formulation in natural language . Natural language interfaces offer easy and intuitive access to information sources and users can express their information needs in their own words. Hence , we present a multilingual information retrieval system allowing for query formulation in natural language . To reduce word sense ambiguities the system operates on a restricted domain . In particular, the system provides access to tourism", "label": ["natural language information retrieval", "constrained spreading activation", "query expansion", "spreading activation", "multilingual information retrieval system", "knowledge representation model", "associative networks", "knowledge representation", "natural language query"], "stemmed_label": ["natur languag inform retriev", "constrain spread activ", "queri expans", "spread activ", "multilingu inform retriev system", "knowledg represent model", "associ network", "knowledg represent", "natur languag queri"]}
{"doc": "The dramatic increase in demand for wireless Internet access has lead to the introduction of new wireless architectures and systems including 3G , Wi-Fi and WiMAX . 3G systems such as UMTS and CDMA2000 are leaning towards an all-IP architecture for transporting IP multimedia services , mainly due to its scalability and promising capability of inter-working heterogeneous wireless access networks . During the last ten years , substantial work has been done to understand the nature of wired IP traffic and it has been proven that IP traffic exhibits self-similar properties and burstiness over a large range of time scales . Recently , because of the large deployment of new wireless architectures , researchers have focused their attention towards understanding the nature of traffic carried by different wireless architecture and early studies have shown that wireless data traffic also exhibits strong long-range dependency . Thus , the classical tele-traffic theory based on a simple Markovian process cannot be used to evaluate the performance of wireless networks . Unfortunately , the area of understanding and modeling of different kinds of wireless traffic is still immature which constitutes a problem since it is crucial to guarantee tight bound QoS parameters to heterogeneous end users of the mobile Internet . In this paper , we make several contributions to the accurate modeling of wireless IP traffic by presenting a novel analytical model that takes into account four different classes of self-similar traffic . The model consists of four queues and is based on a G/M/1 queueing system . We analyze it on the basis of priority with no preemption and find exact packet delays . To date , no closed form expressions have been presented for G/M/1 with priority . INTRODUCTION During the past decade , researchers have made significant efforts to understand the nature of Internet traffic and it has been proven that Internet traffic exhibits self-similar properties . The first study , which stimulated research on self-similar traffic , was based on measurements of Ethernet traffic at Bellcore 1 . Subsequently , the self-similar feature has been discovered in many other types of Internet traffic including studies on Transmission Control Protocol (TCP) 2 , 3 , WWW traffic 4 , VBR video 5 and Signaling System No 7 6 . Deeper studies into the characteristics of Internet traffic has discovered and investigated various properties such as self-similarity 7 , long-range dependence 8 and scaling behavior at small time-scale 9 . The references 10 , 11 provide two extensive bibliographies on self-similarity and long-range dependence research covering both theoretical and applied papers on the subject. Concurrently , over the past few years , we have witnessed a growing popularity of Third Generation Systems (3G) , which have been designed to provide high-speed data services and multimedia applications over mobile personal communication networks . The Universal Mobile Telecommunication System (UMTS) is the predominant global standard for 3G developed by Third Generation Partnership Project (3GPP) 12 . The UMTS architecture is shown in Fig . 1 . It consists of two service domains , a Circuit", "label": ["qos", "3g networks", "traffic modelling", "3g", "self-similar", "ggsn", "self-similar traffic", "wireless ip traffic", "umts", "queuing model"], "stemmed_label": ["qo", "3g network", "traffic model", "3g", "self-similar", "ggsn", "self-similar traffic", "wireless ip traffic", "umt", "queu model"]}
{"doc": "We present a foundation for a computational meta-theory of languages with bindings implemented in a computer-aided formal reasoning environment . Our theory provides the ability to reason abstractly about operators , languages , open-ended languages , classes of languages , etc . The theory is based on the ideas of higher-order abstract syntax , with an appropriate induction principle parameterized over the language (i.e . a set of operators) being used . In our approach , both the bound and free variables are treated uniformly and this uniform treatment extends naturally to variable-length bindings . The implementation is reflective , namely there is a natural mapping between the meta-language of the theorem-prover and the object language of our theory . The object language substitution operation is mapped to the meta-language substitution and does not need to be defined recursively . Our approach does not require designing a custom type theory; in this paper we describe the implementation of this foundational theory within a general-purpose type theory . This work is fully implemented in the MetaPRL theorem prover , using the pre-existing NuPRL-like Martin-Lof-style computational type theory . Based on this implementation , we lay out an outline for a framework for programming language experimentation and exploration as well as a general reflective reasoning framework . This paper also includes a short survey of the existing approaches to syntactic reflection . Introduction 1.1 Reflection Very generally , reflection is the ability of a system to be \"self-aware\" in some way . More specifically , by reflection we mean the property of a computational or formal system to be able to access and internalize some of its own properties. There are many areas of computer science where reflection plays or should play a major role . When exploring properties of programming languages (and other languages) one often realizes that languages have at least two kinds of properties -- semantic properties that have to do with the meaning of what the language's constructs express and syntactic properties of the language itself. Suppose for example that we are exploring some language that contains arithmetic operations . And in particular , in this language one can write polynomials like x 2 + 2x + 1 . In this case the number of roots of a polynomial is a semantic property since it has to do with the valuation of the polynomial . On the other hand , the degree of a polynomial could be considered an example of a syntactic property since the most natural way to define it is as a property of the expression that represents that polynomial . Of course , syntactic properties often have semantic consequences , which is what makes them especially important . In this example , the number of roots of a polynomial is bounded by its degree. Another area where reflection plays an important role is run-time code generation -- in most cases , a language that supports run-time code generation is essentially reflective , as it is capable of manipulating its own syntax . In order to reason", "label": ["system reflection", "programming language", "high order abstract syntax", "formal languages", "theorem prover", "nuprl", "meta-syntax", "metaprl theorem prover", "languages with bindings", "uniform reflection framework", "higher-order abstract syntax", "bruijn-style operations", "hoas-style operations", "nuprl-like martin-lof-style computational type theory", "higher-order abstract syntax", "type theory", "formal definition and theory", "computer aided reasoning", "meta-reasoning", "recursive definition", "reflective reasoning", "reflection", "languages with bindings", "substitution", "metaprl", "runtime code generation", "meta-language", "uniform reflection framework", "theory of syntax", "programming language experimentation"], "stemmed_label": ["system reflect", "program languag", "high order abstract syntax", "formal languag", "theorem prover", "nuprl", "meta-syntax", "metaprl theorem prover", "languag with bind", "uniform reflect framework", "higher-ord abstract syntax", "bruijn-styl oper", "hoas-styl oper", "nuprl-lik martin-lof-styl comput type theori", "higher-ord abstract syntax", "type theori", "formal definit and theori", "comput aid reason", "meta-reason", "recurs definit", "reflect reason", "reflect", "languag with bind", "substitut", "metaprl", "runtim code gener", "meta-languag", "uniform reflect framework", "theori of syntax", "program languag experiment"]}
{"doc": "Researchers with deep knowledge of scientific domains are now developing highly-adaptive and irregular (asymmetrical ) parallel computations , leading to challenges in both delivery of data for computation and mapping of processes to physical resources . Using software engineering principles , we have developed a new communications protocol and architectural style for asymmetrical parallel computations called ADaPT . Utilizing the support of architecturally-aware middleware , we show that ADaPT provides a more efficient solution in terms of message passing and load balancing than asymmetrical parallel computations using collective calls in the Message-Passing Interface (MPI) or more advanced frameworks implementing explicit load-balancing policies . Additionally , developers using ADaPT gain significant windfall from good practices in software engineering , including implementation-level support of architectural artifacts and separation of computational loci from communication protocols INTRODUCTION In recent years , as the cost-to-performance ratio of consumer hardware has continued to decrease , computational clusters consisting of fast networks and commodity hardware have become a common sight in research laboratories . A Copyright is held by the author/owner. ICSE'06 , May 2028 , 2006 , Shanghai , China. ACM 1-59593-085-X/06/0005. growing number of physicists , biologists , chemists , and computer scientists have developed highly-adaptive and irregular parallel applications that are characterized by computational intensity , loosely-synchronous parallelism and dynamic computation . Because the computation time of each parallel process varies significantly for this class of computation , we shall refer to them as asymmetrical parallel computations . Adaptive mesh refinements for the simulation of crack growth , combinatorial search applications used in artificial intelligence , and partial differential equation field solvers 2 are examples of asymmetrical computations. While supercomputing platforms available to us continue to increase performance , our ability to build software capable of matching theoretical limits is lacking 8 . At the same time , researchers with significant depth of knowledge in a scientific domain but with limited software experience are confounded by the interface bloat of libraries such the Message-Passing Interface (MPI) , which has 12 different routines for point-to-point communications alone 5 . Would-be practitioners of high-performance computing are introduced early to the mantra of optimization . The myth that high-level concepts inherent to software engineering principles , such as \"separation of concerns,\" result in inefficiencies at the performance level has caused these researchers to eschew best practices of traditional software development in favor of highly-optimized library routines. We contend that a sound software engineering solution to asymmetrical parallel computations provides decoupling of connectors from computational loci and reduces the complexity of development for the programmer while still providing an efficient solution both in terms of load-balancing and message-delivery . In this paper , we present such a solution . In the next section , we will discuss our motivations for creating the ADaPT protocol and architecture , including the load-balancing inefficiencies of \"optimized\" communications libraries when computing asymmetrical parallel computations . We will then present ADaPT , a communications protocol and associated software architecture for asymmetrical computations . Additionally , we will present analysis which shows ADaPT's ability to outperform", "label": ["collective calls", "adapt", "mpi", "software engineering", "asymamtrical parallel computations", "load balancing", "communication protocols", "high-performance computing", "high-performance computing", "asymmetrical parallel computations"], "stemmed_label": ["collect call", "adapt", "mpi", "softwar engin", "asymamtr parallel comput", "load balanc", "commun protocol", "high-perform comput", "high-perform comput", "asymmetr parallel comput"]}
{"doc": "Research in bioinformatics is driven by the experimental data . Current biological databases are populated by vast amounts of experimental data . Machine learning has been widely applied to bioinformatics and has gained a lot of success in this research area . At present , with various learning algorithms available in the literature , researchers are facing difficulties in choosing the best method that can apply to their data . We performed an empirical study on 7 individual learning systems and 9 different combined methods on 4 different biological data sets , and provide some suggested issues to be considered when answering the following questions: (i) How does one choose which algorithm is best suitable for their data set? (ii) Are combined methods better than a single approach? (iii) How does one compare the effectiveness of a particular algorithm to A href=\"31.html#1\" the others? Introduction In the post-genome era , research in bioinformatics has been overwhelmed by the experimental data . The complexity of biological data ranges from simple strings (nucleotides and amino acids sequences) to complex graphs (biochemical networks); from 1D (sequence data) to 3D (protein and RNA structures) . Considering the amount and complexity of the data , it is becoming impossible for an expert to compute and compare the entries within the current databases . Thus , machine learning and artificial intelligence techniques have been widely applied in this domain to discover and mine the knowledge in the databases . Quoting from Baldi and Brunak (Baldi and Brunak , 2001) \"As a result , the need for computer / statistical / machine learning techniques is today stronger rather than weaker.\" Shavlik et al . (Shavlik et al. , 1995) described the field of molecular biology as tailor-made for machine learning approaches . This is due to the nature of machine learning approaches that performs well in domains where there is a vast amount of data but little theory this is exactly the situation in bioinformatics . Since the introduction of machine learning to this field , various algorithms and methods have been produced and applied to study different data sets . Most of these studies compare a `new' algorithm with the conventional ones , asserting the effectiveness and efficiencies of their methods in particular data sets . The variety of learning algorithms currently available for the researchers are enormous and the main problems faced by researchers are: (i) How does one choose which algorithm is best suitable for their data set? (ii) Are combined methods better than a single approach? (iii) How does one compare the effectiveness of a particular algorithm to the others? Copyright 2003 , Australian Computer Society , Inc . This paper appeared at First Asia-Pacific Bioinformatics Conference, Adelaide , Australia . Conferences in Research and Practice in Information Technology , Vol . 19 . Yi-Ping Phoebe Chen , Ed. Reproduction for academic , not-for profit purposes permitted provided this text is included. The objective of this study is to provide some suggestions for the community by answering the above questions . This paper is organised", "label": ["classification", "supervised machine learning", "cross validation", "performance evaluation", "training data", "biological data", "supervised machine learning", "machine learning", "ensemble methods", "bioinformatics"], "stemmed_label": ["classif", "supervis machin learn", "cross valid", "perform evalu", "train data", "biolog data", "supervis machin learn", "machin learn", "ensembl method", "bioinformat"]}
{"doc": "C applications , in particular those using operating system level services , frequently comprise multiple crosscutting concerns : network protocols and security are typical examples of such concerns . While these concerns can partially be addressed during design and implementation of an application , they frequently become an issue at runtime , e.g. , to avoid server downtime . A deployed network protocol might not be efficient enough and may thus need to be replaced . Buffer overflows might be discovered that imply critical breaches in the security model of an application . A prefetching strategy may be required to enhance performance . While aspect-oriented programming seems attractive in this context , none of the current aspect systems is expressive and efficient enough to address such concerns . This paper presents a new aspect system to provide a solution to this problem . While efficiency considerations have played an important part in the design of the aspect language , the language allows aspects to be expressed more concisely than previous approaches . In particular , it allows aspect programmers to quantify over sequences of execution points as well as over accesses through variable aliases . We show how the former can be used to modularize the replacement of network protocols and the latter to prevent buffer overflows . We also present an implementation of the language as an extension of Arachne , a dynamic weaver for C applications . Finally , we present performance evaluations supporting that Arachne is fast enough to extend high performance applications , such as the Squid web cache . INTRODUCTION Real-world applications typically comprise multiple crosscutting concerns . This applies , in particular , to C applications using operating system level services . We have exam-ined three concerns which are typical for this domain in the context of a large application , the open source web cache Squid 36 . More concretely , we have considered translation of network protocols (which may be necessary for efficiency reasons) , insertion of checks for buffer overflows (which are at the heart of many of today's security issues) , and introduction of prefetching strategies within the cache (which can be used to enhance efficiency of the web cache) . We have found that all these concerns are scattered over large portions of the code of Squid. Hence , the three concerns are crosscutting in the sense of Aspect-Oriented Programming (AOP) 24 and aspects should therefore be a means of choice for their modularization . The concerns have three important characteristics. First , they must frequently be applied at runtime , e.g. , in order to rapidly fix a buffer overflow and thus prevent security breaches without incurring server downtime . A dynamic aspect weaver is therefore needed . Second , they expose intricate relationships between execution points , e.g. , network protocols are most concisely expressed in terms of sequences of execution points , not individual ones . The aspect system must therefore support expressive means for the definition of aspects , in particular pointcuts . Third , efficiency is", "label": ["aspect language", "buffer overflows", "prefetching", "sequence pointcut", "system applications", "binary code", "dynamic weaving", "arachne", "web cache", "operating system", "network protocol", "c applications"], "stemmed_label": ["aspect languag", "buffer overflow", "prefetch", "sequenc pointcut", "system applic", "binari code", "dynam weav", "arachn", "web cach", "oper system", "network protocol", "c applic"]}
{"doc": "It is important for successful electronic business to have a hi-quality business website . So we need an accurate and effective index system to evaluate and analyses the quality of the business website . In this paper , the evaluation index system following the `grey box' principle is proposed which considers both efficiency of business website and performance of electronic business system . Using R-Hierarchical clustering method to extract the typical indexes from sub-indexes is theoretically proved to have a rationality and effectiveness . Finally , the evaluation method is briefly discussed . INTRODUCTION Business website is an online media between buyer and seller. A hi-quality website is crucial to a company for a successful e-business . What is a hi-quality business website? In terms of maintaining the website , what do we focus on so that the quality meets the users' needs? Apparently , using click-through rate to assess the popularity cannot objectively and accurately evaluate the quality of the business websites . Instead , we need to rely on scientific evaluation index system and methods. At present , there are many methods available for business website comparison or ranking , such as Usage Ranking, Purchase Comparison , Expert Opinion and Synthesis Evaluation etc . You can find both official authority and non-governmental organization that issue their power ranking. The former one is to monitor and regulate the market , such as CNNIC , which organized the competition for the Top Ten Websites in domestic . The latter one , such as Consumerreports ( www.consumerreports . org ) , BizRate(www.bizrate.com), Forrester Research etc. , is mainly to guide the web users' activity . These kinds of comparison or ranking have special value in getting reputation and increasing recognition of the business websites among the users , however , e-business enterprise can not improve the quality of their websites directly based on the results of these kinds of assessments. The main purpose of this paper is to develop an index system for quantitative evaluation of the BtoC websites , which dose not emphasize the income of the website but focus on evaluating of its synthesis quality . We hope that the applying of this index system will provide the technique developers and maintainers some references for designing , appraising and diagnosing their e-business system to improve its quality level , and to support managers to make decisions for operation of the websites. OVERVIEW OF PREVIOUS STUDIES Comparing to the fast growing of e-business websites in the world , currently we can rarely find the particular research on the evaluation index system of business website . QEM (The website quality evaluation method) proposed by Olsina and Godoy etc . in 1999 can be considered as one of the representative approaches . It based on the main factors to evaluate the quality of the websites , including functionality (global search , navigability , and content relativity) , usability (website map , addresses directory) , efficiency and reliability . In 2000 , American researcher , Panla Solaman , presented e-SERVQUAL model based on the conventional", "label": ["business website", "b2c websites", "system performance", "representitive indexes", "fuzzy analysis", "r-hierarchical clustering", "evaluation system", "quality synthesis evaluation", "quality evaluation index", "r-hierarchical clustering method", "index optimization", "e-commerce", "clustering", "correlation index"], "stemmed_label": ["busi websit", "b2c websit", "system perform", "representit index", "fuzzi analysi", "r-hierarch cluster", "evalu system", "qualiti synthesi evalu", "qualiti evalu index", "r-hierarch cluster method", "index optim", "e-commerc", "cluster", "correl index"]}
{"doc": "In this paper , we present an expressive 3D animation environment that enables users to rapidly and visually prototype animated worlds with a fully 3D user-interface . A 3D device allows the specification of complex 3D motion , while virtual tools are visible mediators that live in the same 3D space as application objects and supply the interaction metaphors to control them . In our environment , there is no intrinsic difference between user interface and application objects . Multi-way constraints provide the necessary tight coupling among components that makes it possible to seamlessly compose animated and interactive behaviors . By recording the effects of manipulations , all the expressive power of the 3D user interface is exploited to define animations . Effective editing of recorded manipulations is made possible by compacting all continuous parameter evolutions with an incremental data-reduction algorithm , designed to preserve both geometry and timing . The automatic generation of editable representations of interactive performances overcomes one of the major limitations of current performance animation systems . Novel interactive solutions to animation problems are made possible by the tight integration of all system components . In particular , animations can be synchronized by using constrained manipulation during playback . The accompanying video-tape illustrates our approach with interactive sequences showing the visual construction of 3D animated worlds . All the demonstrations in the video were recorded live and were not edited . INTRODUCTION Modern 3D graphics systems allow a rapidly growing user community to create and animate increasingly sophisticated worlds . Despite their inherent three-dimensionality , these systems are still largely controlled by 2D WIMP user-interfaces . The lack of correlation between manipulation and effect and the high cognitive distance from users to edited models are the major drawbacks of this solution 13 . The inadequacy of user-interfaces based on 2D input devices and mindsets becomes particularly evident in the realm of interactive 3D animation . In this case , the low-bandwidth communication between user-interface and application and the restrictions in interactive 3D motion specification capabilities make it extremely difficult to define animations with straight-ahead actions . This inability to interactively specify the animation timing is a major obstacle in all cases where the spontaneity of the animated object's behavior is important 21; 35; 4 . In this paper , we present an expressive 3D animation environment that enables users to rapidly and visually prototype animated worlds with a fully 3D user-interface . A 3D device allows the specification of complex 3D motion , while virtual tools supply the interaction metaphors to control application objects . In our environment , there is no intrinsic difference between user interface and application objects . Multi-way constraints provide the necessary tight coupling among components that makes it possible to compose animated and interactive behaviors . By recording the effects of manipulations , all the expressive power of the 3D user interface is exploited to define animations . Effective editing of recorded manipulations is made possible by compacting all continuous parameter evolutions with our data-reduction algorithm , designed to preserve both geometry and", "label": ["animation synchronization", "computer graphics", "object-oriented graphics", "3d animation environment", "data reduction", "visualization", "multi-way constrained architecture", "human interaction", "data reduction", "3d animation", "local propagation constraints", "recording 3d manipulation", "virtual tools", "3d widgets", "3d user interface", "3d interaction", "dynamic model"], "stemmed_label": ["anim synchron", "comput graphic", "object-ori graphic", "3d anim environ", "data reduct", "visual", "multi-way constrain architectur", "human interact", "data reduct", "3d anim", "local propag constraint", "record 3d manipul", "virtual tool", "3d widget", "3d user interfac", "3d interact", "dynam model"]}
{"doc": "When testing database applications , in addition to creating in-memory fixtures it is also necessary to create an initial database state that is appropriate for each test case . Current approaches either require exact database states to be specified in advance , or else generate a single initial state (under guidance from the user) that is intended to be suitable for execution of all test cases . The first method allows large test suites to be executed in batch , but requires considerable programmer effort to create the test cases (and to maintain them) . The second method requires less programmer effort , but increases the likelihood that test cases will fail in non-fault situations , due to unexpected changes to the content of the database . In this paper , we propose a new approach in which the database states required for testing are specified intensionally , as constrained queries , that can be used to prepare the database for testing automatically . This technique overcomes the limitations of the other approaches , and does not appear to impose significant performance overheads . INTRODUCTION Modern information systems are typically organised as collections of independent application programs that communicate with one another by means of a central database. The database records the state of the organisation that the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. ICSE'06, May 2028 , 2006 , Shanghai , China. Copyright 2006 ACM 1-59593-085-X/06/0005 ... $ 5.00. information system supports , while the application programs implement the business processes that manipulate the state. To take a simple but ubiquitous example , a database system might record details of customers , products and sales, while the application programs associated with it handle operations such as new product purchases and update of the product catalogue , as well as supporting decision making by generating reports regarding the most profitable product lines , names and addresses of loss-making customers , etc. In order to test such application programs , it is necessary to create test fixtures that simulate the presence of the rest of the information system . Fixtures for traditional test cases typically consist of in-memory objects and data structures that provide the inputs to the program being tested . This kind of fixture is also needed when testing database applications (especially when performing unit testing); however, since it is unrealistic (and often incorrect) to execute test cases against an empty database , we need to create additional fixture elements within the database itself. Current practice in the software industry is to maintain one or more test databases that can be used for testing individual programs . These databases can be artificially", "label": ["database testing", "efficient testing", "software testing", "seamless integration", "query based language", "improvement for the intensional test cases", "dot-unit", "databases", "lesser programmer effort for test cases", "intensional test cases", "testing framework", "testing for database systems", "performance testing"], "stemmed_label": ["databas test", "effici test", "softwar test", "seamless integr", "queri base languag", "improv for the intension test case", "dot-unit", "databas", "lesser programm effort for test case", "intension test case", "test framework", "test for databas system", "perform test"]}
{"doc": "A neural network based clustering method for the analysis of soft handovers in 3G network is introduced . The method is highly visual and it could be utilized in explorative analysis of mobile networks . In this paper , the method is used to find groups of similar mobile cell pairs in the sense of handover measurements . The groups or clusters found by the method are characterized by the rate of successful handovers as well as the causes of failing handover attempts . The most interesting clusters are those which represent certain type of problems in handover attempts . By comparing variable histograms of a selected cluster to histograms of the whole data set an application domain expert may find some explanations on problems . Two clusters are investigated further and causes of failing handover attempts are discussed . INTRODUCTION Mobility management is a great challenge in current and future radio access networks . In third generation (3G) networks user experienced quality of service (QoS) under a move of mobile station (MS) from one mobile cell to another cell has been improved by implementing soft handover Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. MSWiM'06, October 26 , 2006 , Torremolinos , Malaga , Spain. Copyright 2006 ACM 1-59593-477-4/06/0010 ...$5.00. (SHO) . Soft handover makes it possible to have connections on several base stations (BS) simultaneously. In this paper , a set of measurements which can be used for soft handover decision making are analyzed and compared with other measurements in which statistics of successful-ness of handover attempts have been collected . We do not know exactly the parameters of used SHO algorithm . SHOs are investigated only on basis of data set and some general knowledge of 3G systems . Mobile cell pairs with handovers (HO) are divided in groups using clustering algorithm . Cell pairs in which SHOs are similar with each other fall in same group . Different types of SHO failures are analyzed using clustering information and distributions of measurements in each cluster. In Section 2 the soft handover concept , the measurements and used neural network algorithm are shortly introduced. Analysis methods which have been used are described in Section 3 . Preliminary results are shown and discussed in Section 4 . Finally , some conclusions are drawn in the last section. BACKGROUND In this section , the basics of soft handover in 3G network is explained and the available data set is introduced . Neural network algorithm used in data clustering is also presented. 2.1 Soft handover Soft handover is a state of MS being connected to several BSs simultaneously . In GSM networks , a fixed threshold", "label": ["two-phase clustering algorithm", "data mining", "mobility management", "key performance indicator of handover", "soft handover", "data mining", "soft handover", "visualization capability", "neural network algorithm", "neural networks", "hierarchical clustering", "self-organizing map", "cluster analysis", "histograms", "3g network", "decrease in computational complexity"], "stemmed_label": ["two-phas cluster algorithm", "data mine", "mobil manag", "key perform indic of handov", "soft handov", "data mine", "soft handov", "visual capabl", "neural network algorithm", "neural network", "hierarch cluster", "self-organ map", "cluster analysi", "histogram", "3g network", "decreas in comput complex"]}
{"doc": "Aspect Oriented Programming , a relatively new programming paradigm , earned the scientific community's attention . The paradigm is already evaluated for traditional OOP and component-based software development with remarkable results . However , most of the published work , while of excellent quality , is mostly theoretical or involves evaluation of AOP for research oriented and experimental software . Unlike the previous work , this study considers the AOP paradigm for solving real-life problems , which can be faced in any commercial software . We evaluate AOP in the development of a high-performance component-based web-crawling system , and compare the process with the development of the same system without AOP . The results of the case study mostly favor the aspect oriented paradigm . INTRODUCTION Aspect Oriented Programming , a relatively new programming paradigm introduced by Kiczales ( 2 ) , recently earned the scientific community's attention. Having around six years of life , the paradigm was already presented in important conferences , and recently triggered the creation of several conferences and workshops to deal with it. The paradigm is already evaluated for traditional OOP and component-based software development and is found very promising . Several evaluations consider it to be the continuation of the OOP paradigm . However , most of the published work while of excellent quality is mostly theoretical or involves evaluation of AOP for research oriented and experimental software . Unlike previous works , this study considers the AOP paradigm for solving real-life problems, which need to be faced in any commercial software . We evaluated Aspect Oriented Programming in the development of Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. SAC'04 March 14-17 , 2004 , Nicosia , Cyprus Copyright 2004 ACM 1-58113-812-1/03/04 ... $ 5.00. a high-performance component-based web-crawling system, and compared the process with the development of the same system without AOP . The results of the case study , mostly favoring the aspect oriented paradigm , are reported in this work. This introduction is followed by an introduction to the AOP approach . We then describe the application that was used for our evaluation and proceed with a description of our evaluation scenario . We then present and comment our evaluation results . We continue with references to similar evaluation attempts , and , finally , we summarize the conclusions from our evaluation , and report on future work. ASPECT ORIENTED PROGRAMMING Aspect Oriented Programming , as proposed by Kiczales ( 2 ) , is based on the aspectual decomposition . Aspectual decomposition is somewhat complementary to functional decomposition , and tries to overcome the limitation of functional decomposition to capture and represent crosscutting functionality .", "label": ["case study", "evaluation", "web crawler implementation", "aop", "component-based application", "aspect oriented programming", "development process experiment metrics", "software development process experiment", "programming paradigm comparison", "oop", "object oriented programming", "programming paradigms", "aspect oriented programming application"], "stemmed_label": ["case studi", "evalu", "web crawler implement", "aop", "component-bas applic", "aspect orient program", "develop process experi metric", "softwar develop process experi", "program paradigm comparison", "oop", "object orient program", "program paradigm", "aspect orient program applic"]}
{"doc": "In a large sensor network , in-network data aggregation , i.e. , combining partial results at intermediate nodes during message routing , significantly reduces the amount of communication and hence the energy consumed . Recently several researchers have proposed robust aggregation frameworks , which combine multi-path routing schemes with duplicate-insensitive algorithms , to accurately compute aggregates (e.g. , Sum , Count , Average) in spite of message losses resulting from node and transmission failures . However , these aggregation frameworks have been designed without security in mind . Given the lack of hardware support for tamper-resistance and the unattended nature of sensor nodes , sensor networks are highly vulnerable to node compromises . We show that even if a few compromised nodes contribute false sub-aggregate values , this results in large errors in the aggregate computed at the root of the hierarchy . We present modifications to the aggregation algorithms that guard against such attacks , i.e. , we present algorithms for resilient hierarchical data aggregation despite the presence of compromised nodes in the aggregation hierarchy . We evaluate the performance and costs of our approach via both analysis and simulation . Our results show that our approach is scalable and efficient . INTRODUCTION In large sensor networks , computing aggregates in-network , i.e., combining partial results at intermediate nodes during message routing , significantly reduces the amount of communication and hence the energy consumed 11 , 23 . An approach used by several data acquisition systems for sensor networks is to construct a spanning tree rooted at the querying node , and then perform in-network aggregation along the tree . Partial results propagate level-by-level up the tree , with each node awaiting messages from all its children before sending a new partial result to its parent. Tree-based aggregation approaches , however , are not resilient to communication losses resulting from node and transmission failures , which are relatively common in sensor networks 11 , 22, 23 . Because each communication failure loses an entire subtree of readings , a large fraction of sensor readings are potentially un-accounted for at the querying node , leading to a significant error in the query answer . To address this problem , researchers have proposed the use of multi-path routing techniques for forwarding sub-aggregates 11 . For aggregates such as Min and Max which are duplicate-insensitive , this approach provides a fault-tolerant solution . For duplicate-sensitive aggregates such as Count and Sum, however , multi-path routing leads to double-counting of sensor readings , resulting in an incorrect aggregate being computed. Recently researchers 3 , 12 , 14 have presented clever algorithms to solve the double-counting problem associated with multi-path approaches . A robust and scalable aggregation framework called Synopsis Diffusion has been proposed for computing duplicate- sensitive aggregates such as Count and Sum . There are two primary elements of this approach - the use of a ring-based topology instead of a tree-based topology for organizing the nodes in the aggregation hierarchy , and the use of duplicate-insensitive algorithms for computing aggregates based on Flajolet", "label": ["sensor networks", "node compromise prevention", "falsified local value attack", "in-network data aggregation", "attack resilient heirarchical data aggregation", "sum aggregate", "falsified sub-aggregate attack", "attack-resilient", "count aggregate", "sensor network security", "robust aggregation", "synopsis diffusion", "data aggregation", "synopsis diffusion aggregation framework", "network aggregation algorithms", "hierarchical aggregation"], "stemmed_label": ["sensor network", "node compromis prevent", "falsifi local valu attack", "in-network data aggreg", "attack resili heirarch data aggreg", "sum aggreg", "falsifi sub-aggreg attack", "attack-resili", "count aggreg", "sensor network secur", "robust aggreg", "synopsi diffus", "data aggreg", "synopsi diffus aggreg framework", "network aggreg algorithm", "hierarch aggreg"]}
{"doc": "To have a rich presentation of a topic , it is not only expected that many relevant multimodal information , including images , text , audio and video , could be extracted; it is also important to organize and summarize the related information , and provide users a concise and informative storyboard about the target topic . It facilitates users to quickly grasp and better understand the content of a topic . In this paper , we present a novel approach to automatically generating a rich presentation of a given semantic topic . In our proposed approach , the related multimodal information of a given topic is first extracted from available multimedia databases or websites . Since each topic usually contains multiple events , a text-based event clustering algorithm is then performed with a generative model . Other media information , such as the representative images , possibly available video clips and flashes (interactive animates) , are associated with each related event . A storyboard of the target topic is thus generated by integrating each event and its corresponding multimodal information . Finally , to make the storyboard more expressive and attractive , an incidental music is chosen as background and is aligned with the storyboard . A user study indicates that the presented system works quite well on our testing examples . INTRODUCTION In the multimedia field , a major objective of content analysis is to discover the high-level semantics and structures from the low-level features , and thus to facilitate indexing , browsing , searching, and managing the multimedia database . In recent years , a lot of technologies have been developed for various media types, including images , video , audio and etc . For example , various approaches and systems have been proposed in image content analysis , such as semantic classification 1 , content-based image retrieval 2 and photo album management 3 . There are also a lot of research focuses on video analysis , such as video segmentation 4 , highlight detection 5 , video summarization 6 7 , and video structure analysis 8 , applied in various data including news video , movie and sports video . Since audio information is very helpful for video analysis , many research works on audio are also developed to enhance multimedia analysis , such as audio classification 9 , and audio effect detection in different audio streams 10 . Most recently , there are more and more approaches and systems integrating multimodal information in order to improve analysis performance 11 12 . The main efforts of the above mentioned research have focused on understanding the semantics (including a topic , an event or the similarity) from the multimodal information . That is , after the multimedia data is given , we want to detect the semantics implied in these data . In this paper , we propose a new task , Rich Presentation , which is an inverse problem of the traditional multimedia content analysis . That is , if we have a semantic topic, how can we", "label": ["documentary and movie", "rich presentation", "events clustering", "communication and multimedia", "representative media", "images, videos and audio technologies", "rich video clips and flashes", "multi-modal information", "generate storyboard", "storyboard", "subjective multiple events", "multimedia fusion", "high-level semantics", "event clustering", "multimodality", "multimedia authoring"], "stemmed_label": ["documentari and movi", "rich present", "event cluster", "commun and multimedia", "repres media", "images, video and audio technolog", "rich video clip and flash", "multi-mod inform", "gener storyboard", "storyboard", "subject multipl event", "multimedia fusion", "high-level semant", "event cluster", "multimod", "multimedia author"]}
{"doc": "Database security has paramount importance in industrial , civilian and government domains . Despite its importance , our search reveals that only a small number of database security courses are being offered . In this paper , we share our experience in developing and offering an undergraduate elective course on database security with limited resources . We believe that database security should be considered in its entirety rather than being component specific . Therefore , we emphasize that students develop and implement a database security plan for a typical real world application . In addition to the key theoretical concepts , students obtain hands-on experience with two popular database systems . We encourage students to learn independently making use of the documentation and technical resources freely available on the Internet . This way , our hope is that they will be able to adapt to emerging systems and application scenarios . INTRODUCTION Database systems are designed to provide efficient access to large volumes of data . However , many application domains require that the data access be restricted for security reasons. For example , an unauthorized access to Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. SIGCSE'06 , March 1�5 , 2006 , Houston , Texas , USA. Copyright 2006 ACM 1-59593-259-3/06/0003... $ 5.00. a bank database can potentially cost millions of dollars. The federal Health Insurance Portability and Accountability Act (HIPAA) regulates the disclosure of information from a patient database , allowing access to health care providers, health plans , and health care clearinghouses , simultaneously protecting the privacy of patients . For obvious reasons , a Department of Defense (DoD) database needs to be protected from unauthorized access . Since many organizations increasingly entrust their information resources with database systems , in today's highly networked environment , the sensitive information can be at high risk unless there are security mechanisms in place to protect the data at the source itself . However , a large number of databases are incorrectly installed , configured , and maintained . This , in part , may be attributed to the lack of database security education in our computer science programs . We feel that a new undergraduate course on database security will help our students face the ever increasing challenges in this field. Our search shows that , despite the importance , only a handful database security courses are being offered . Most of the courses we found are graduate courses and are highly theoretical . We also found a few extension program courses, which are product specific. Although a large number of database courses exist at both undergraduate and graduate levels , we feel that", "label": ["database security course", "statistical database security", "statistical security", "database security", "high risk of sensitive information", "database security plan", "security breaches", "labs", "database system", "undergraduate database security course", "real life database security", "database security education", "xml security", "undergraduate students", "cryptography", "secure information", "hands-on experience", "database security plan", "data access", "security plan", "database privacy", "administrators", "hands-on", "database security course", "undergraduate course", "access privilege system", "database security", "privacy issues", "laboratory/active learning", "right blend of theory and practice", "assignments", "real life databases hands-on", "mysql security", "topics", "importance of database security", "few database security courses", "oracle security"], "stemmed_label": ["databas secur cours", "statist databas secur", "statist secur", "databas secur", "high risk of sensit inform", "databas secur plan", "secur breach", "lab", "databas system", "undergradu databas secur cours", "real life databas secur", "databas secur educ", "xml secur", "undergradu student", "cryptographi", "secur inform", "hands-on experi", "databas secur plan", "data access", "secur plan", "databas privaci", "administr", "hands-on", "databas secur cours", "undergradu cours", "access privileg system", "databas secur", "privaci issu", "laboratory/act learn", "right blend of theori and practic", "assign", "real life databas hands-on", "mysql secur", "topic", "import of databas secur", "few databas secur cours", "oracl secur"]}
{"doc": "In this paper , we propose a machine learning approach to title extraction from general documents . By general documents , we mean documents that can belong to any one of a number of specific genres , including presentations , book chapters , technical papers , brochures , reports , and letters . Previously , methods have been proposed mainly for title extraction from research papers . It has not been clear whether it could be possible to conduct automatic title extraction from general documents . As a case study , we consider extraction from Office including Word and PowerPoint . In our approach , we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data , train machine learning models , and perform title extraction using the trained models . Our method is unique in that we mainly utilize formatting information such as font size as features in the models . It turns out that the use of formatting information can lead to quite accurate extraction from general documents . Precision and recall for title extraction from Word is 0.810 and 0.837 respectively , and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data . Other important new findings in this work include that we can train models in one domain and apply them to another domain , and more surprisingly we can even train models in one language and apply them to another language . Moreover , we can significantly improve search ranking results in document retrieval by using the extracted titles . INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search , browsing , and filtering . Ideally, metadata is defined by the authors of documents and is then used by various systems . However , people seldom define document metadata by themselves , even when they have convenient metadata definition tools 26 . Thus , how to automatically extract metadata from the bodies of documents turns out to be an important research issue. Methods for performing the task have been proposed . However, the focus was mainly on extraction from research papers . For instance , Han et al . 10 proposed a machine learning based method to conduct extraction from research papers . They formalized the problem as that of classification and employed Support Vector Machines as the classifier . They mainly used linguistic features in the model. 1 In this paper , we consider metadata extraction from general documents . By general documents , we mean documents that may belong to any one of a number of specific genres . General documents are more widely available in digital libraries , intranets and the internet , and thus investigation on extraction from them is 1 The work was conducted when the first author was visiting Microsoft Research Asia. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are", "label": ["digital copies", "metadata extraction", "metadata processing", "search ranking results", "file formats extraction", "information search and retrieval", "powerpoint documents", "information extraction", "precision extraction", "file extraction", "search", "generic languages", "machine learning", "microsoft office automation"], "stemmed_label": ["digit copi", "metadata extract", "metadata process", "search rank result", "file format extract", "inform search and retriev", "powerpoint document", "inform extract", "precis extract", "file extract", "search", "gener languag", "machin learn", "microsoft offic autom"]}
{"doc": "Intrusion or misbehaviour detection systems are an important and widely accepted security tool in computer and wireless sensor networks . Their aim is to detect misbehaving or faulty nodes in order to take appropriate countermeasures , thus limiting the damage caused by adversaries as well as by hard or software faults . So far , however , once detected , misbehaving nodes have just been isolated from the rest of the sensor network and hence are no longer usable by running applications . In the presence of an adversary or software faults , this proceeding will inevitably lead to an early and complete loss of the whole network . For this reason , we propose to no longer expel misbehaving nodes , but to recover them into normal operation . In this paper , we address this problem and present a formal specification of what is considered a secure and correct node recovery algorithm together with a distributed algorithm that meets these properties . We discuss its requirements on the soft- and hardware of a node and show how they can be fulfilled with current and upcoming technologies . The algorithm is evaluated analytically as well as by means of extensive simulations , and the findings are compared to the outcome of a real implementation for the BTnode sensor platform . The results show that recovering sensor nodes is an expensive , though feasible and worthwhile task . Moreover , the proposed program code update algorithm is not only secure but also fair and robust . INTRODUCTION Wireless sensor networks (WSNs) consist of many wireless communicating sensor nodes . Essentially , these are mi-crocontrollers including a communication unit and a power supply , as well as several attached sensors to examine the environment . Sensor nodes typically have very limited computing and storage capacities and can only communicate with their direct neighbourhood . In addition , WSNs have to work unattended most of the time as their operation area cannot or must not be visited . Reasons can be that the area is inhospitable , unwieldy , or ecologically too sensitive for human visitation; or that manual maintenance would just be too expensive. More and more , WSN applications are supposed to operate in hostile environments , where their communication might be overheard and nodes can be removed or manipu-lated . Regarding attacks on sensor networks , one differentiates between so called outsider and insider attacks A href=\"41.html#9\" 21 . In the former , a potential attacker tries to disclose or influence a confidential outcome without participating in its computation ; for instance , by intercepting , modifying , or adding messages . In the latter , by contrast , an attacker appears as an adequate member of the WSN by either plausibly impersonating regular nodes or by capturing and compromising them. Cryptographic methods , such as encrypting or signing messages , are an effective protection against attacks from outside the network , but are of only limited help against insider attacks . Once an adversary possesses one or several valid node", "label": ["sensor networks", "countermeasures", "intrusion detection", "wireless sensor networks", "node recovery", "intrusion detection", "node recovery", "ids", "sensor nodes", "security", "distributed algorithm"], "stemmed_label": ["sensor network", "countermeasur", "intrus detect", "wireless sensor network", "node recoveri", "intrus detect", "node recoveri", "id", "sensor node", "secur", "distribut algorithm"]}
{"doc": "This paper explores the use of Bayesian online classifiers to classify text documents . Empirical results indicate that these classifiers are comparable with the best text classification systems . Furthermore , the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task . INTRODUCTION Faced with massive information everyday , we need automated means for classifying text documents . Since handcrafting text classifiers is a tedious process , machine learning methods can assist in solving this problem 15 , 7 , 27 . Yang & Liu 27 provides a comprehensive comparison of supervised machine learning methods for text classification. In this paper we will show that certain Bayesian classifiers are comparable with Support Vector Machines 23 , one of the best methods reported in 27 . In particular , we will evaluate the Bayesian online perceptron 17 , 20 and the Bayesian online Gaussian process 3 . For text classification and filtering , where the initial training set is large , online approaches are useful because they allow continuous learning without storing all the previously Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. SIGIR'02 , August 11-15 , 2002 , Tampere , Finland. Copyright 2002 ACM 1-58113-561-0/02/0008 ... $ 5.00. seen data . This continuous learning allows the utilization of information obtained from subsequent data after the initial training . Bayes' rule allows online learning to be performed in a principled way 16 , 20 , 17 . We will evaluate the Bayesian online perceptron , together with information gain considerations , on the batch-adaptive filtering task 18 . CLASSIFICATION AND FILTERING For the text classification taskdefined by Lewis 9 , we have a set of predefined categories and a set of documents. For each category , the document set is partitioned into two mutually exclusive sets of relevant and irrelevant documents. The goal of a text classification system is to determine whether a given document belongs to any of the predefined categories . Since the document can belong to zero , one , or more categories , the system can be a collection of binary classifiers , in which one classifier classifies for one category. In Text REtrieval Conference (TREC) , the above taskis known as batch filtering . We will consider a variant of batch filtering called the batch-adaptive filtering 18 . In this task, during testing , if a document is retrieved by the classifier, the relevance judgement is fed backto the classifier . This feedbackcan be used to improve the classifier. 2.1 Corpora and Data For text classification , we use the ModApte version of the Reuters-21578 corpus 1 , where unlabelled documents are removed . This", "label": ["text classification", "perceptron", "text classification", "filtering", "information gain", "bayesian online classifiers", "online", "machine learning", "continous learning", "machine learning", "text filtering", "gaussian process", "bayesian"], "stemmed_label": ["text classif", "perceptron", "text classif", "filter", "inform gain", "bayesian onlin classifi", "onlin", "machin learn", "contin learn", "machin learn", "text filter", "gaussian process", "bayesian"]}
{"doc": "Since the publication of Brin and Page's paper on PageRank , many in the Web community have depended on PageRank for the static (query-independent) ordering of Web pages . We show that we can significantly outperform PageRank using features that are independent of the link structure of the Web . We gain a further boost in accuracy by using data on the frequency at which users visit Web pages . We use RankNet , a ranking machine learning algorithm , to combine these and other static features based on anchor text and domain characteristics . The resulting model achieves a static ranking pairwise accuracy of 67.3% (vs . 56.7% for PageRank or 50% for random) . INTRODUCTION Over the past decade , the Web has grown exponentially in size. Unfortunately , this growth has not been isolated to good-quality pages . The number of incorrect , spamming , and malicious (e.g., phishing) sites has also grown rapidly . The sheer number of both good and bad pages on the Web has led to an increasing reliance on search engines for the discovery of useful information . Users rely on search engines not only to return pages related to their search query , but also to separate the good from the bad , and order results so that the best pages are suggested first. To date , most work on Web page ranking has focused on improving the ordering of the results returned to the user (query-dependent ranking , or dynamic ranking) . However , having a good query-independent ranking (static ranking) is also crucially important for a search engine . A good static ranking algorithm provides numerous benefits: Relevance : The static rank of a page provides a general indicator to the overall quality of the page . This is a useful input to the dynamic ranking algorithm. Efficiency : Typically , the search engine's index is ordered by static rank . By traversing the index from high-quality to low-quality pages , the dynamic ranker may abort the search when it determines that no later page will have as high of a dynamic rank as those already found . The more accurate the static rank , the better this early-stopping ability , and hence the quicker the search engine may respond to queries. Crawl Priority : The Web grows and changes as quickly as search engines can crawl it . Search engines need a way to prioritize their crawl--to determine which pages to re-crawl , how frequently , and how often to seek out new pages . Among other factors , the static rank of a page is used to determine this prioritization . A better static rank thus provides the engine with a higher quality , more up-to -date index. Google is often regarded as the first commercially successful search engine . Their ranking was originally based on the PageRank algorithm 5 27 . Due to this (and possibly due to Google's promotion of PageRank to the public) , PageRank is widely regarded as the best method for the static ranking", "label": ["anchor text", "relevance", "web pages", "pairwise accuracy", "frank", "popularity data", "dynamic ranking", "search engines", "pagerank", "static ranking", "static ranking", "static features", "ranknet"], "stemmed_label": ["anchor text", "relev", "web page", "pairwis accuraci", "frank", "popular data", "dynam rank", "search engin", "pagerank", "static rank", "static rank", "static featur", "ranknet"]}
{"doc": "It is well known that the secure computation of non-trivial functionalities in the setting of no honest majority requires computational assumptions . We study the way such computational assumptions are used . Specifically , we ask whether the secure protocol can use the underlying primitive (e.g. , one-way trapdoor permutation) in a black-box way , or must it be nonblack-box (by referring to the code that computes this primitive)? Despite the fact that many general constructions of cryptographic schemes (e.g. , CPA-secure encryption ) refer to the underlying primitive in a black-box way only , there are some constructions that are inherently nonblack-box . Indeed , all known constructions of protocols for general secure computation that are secure in the presence of a malicious adversary and without an honest majority use the underlying primitive in a nonblack-box way (requiring to prove in zero-knowledge statements that relate to the primitive) . In this paper , we study whether such nonblack-box use is essential . We present protocols that use only black-box access to a family of (enhanced) trapdoor permutations or to a homomorphic public-key encryption scheme . The result is a protocol whose communication complexity is independent of the computational complexity of the underlying primitive (e.g. , a trapdoor permutation) and whose computational complexity grows only linearly with that of the underlying primitive . This is the first protocol to exhibit these properties . INTRODUCTION It is a known fact that most cryptographic tasks require the use of computational hardness assumptions . These assumptions typically come in two types: specific assumptions like the hardness of factoring , RSA , discrete log and others, and general assumptions like the existence of one-way functions , trapdoor permutations and others . In this paper , we refer to general assumptions and how they are used . Specifically , we consider an intriguing question regarding how secure protocols utilize a primitive that is assumed to carry some hardness property . Here again , there is a clear distinction between two types of uses: 1 . Black-box usage: a protocol (or construction) uses a primitive in a black-box way if it refers only to the input/output behavior of the primitive. 1 For example, if the primitive is a trapdoor permutation , then the protocol may sample a permutation and its domain, and may compute the permutation and its inverse (if the trapdoor is given) . Beyond this , no reference is made to the primitive . In particular , the code used to compute the permutation (or carry out any other task) is not referred to by the protocol . The vast majority of constructions in cryptography are black-box. 2 . Nonblack-box usage: a protocol (or construction) uses a primitive in a nonblack-box way if it refers to the code for computing its functionality . A typical example of a nonblack-box construction is where a Karp reduction is applied to the circuit computing the function , say , in order to prove an N P zero-knowledge proof , as in 14 . A rich and fruitful body of", "label": ["oblivious transfer", "encryption scheme", "oblivious transfer protocol", "secure computation", "nonblack-box", "malicious adversary", "black-box", "theory of cryptography", "cryptographic", "black-box reductions", "trapdoor permutation"], "stemmed_label": ["oblivi transfer", "encrypt scheme", "oblivi transfer protocol", "secur comput", "nonblack-box", "malici adversari", "black-box", "theori of cryptographi", "cryptograph", "black-box reduct", "trapdoor permut"]}
{"doc": "Bluetooth is a cable replacement technology for Wireless Personal Area Networks . It is designed to support a wide variety of applications such as voice , streamed audio and video , web browsing , printing , and file sharing , each imposing a number of quality of service constraints including packet loss , latency , delay variation , and throughput . In addition to QOS support , another challenge for Bluetooth stems from having to share the 2.4 GHz ISM band with other wireless devices such as IEEE 802.11 . The main goal of this paper is to investigate the use of a dynamic scheduling algorithm that guarantees QoS while reducing the impact of interference . We propose a mapping between some common QoS parameters such as latency and bit rate and the parameters used in the algorithm . We study the algorithm's performance and obtain simulation results for selected scenarios and configurations of interest . Introduction Today most radio technologies considered by Wireless Personal Area Network (WPAN) industry consortia and standard groups including the Bluetooth Special Interest Group 1 , HomeRF , and the IEEE 802.15 , employ the 2.4 GHz ISM frequency band . This same frequency band is already in use by microwave ovens and the popular Wireless Local Area Network (WLAN) devices implementing the IEEE 802.11 standard specifications 8 . However , instead of competing with WLANs for spectrum and applications , WPANs are intented to augment many of the usage scenarios and operate in conjunction with WLANs, i.e. , come together in the same laptop , or operate in proximity in an office or conference room environment . For example, Bluetooth can be used to connect a headset , or PDA to a desk-top computer , that , in turn , may be using WLAN to connect to an Access Point placed several meters away. Thus , an issue of growing concern is the coexistence of WLAN and WPAN in the same environment . Several techniques and algorithms aimed at reducing the impact of interference have been considered. These techniques range from collaborative schemes intended for Bluetooth and IEEE 802.11 protocols to be implemented in the same device to fully independent solutions that rely on interference detection and estimation . In particular: Collaborative mechanisms . Mechanisms for collaborative schemes have been proposed to the IEEE 802.15 Coexistence Task Group and are based on a Time Division Multiple Access (TDMA) solution that alternates the transmission of Bluetooth and WLAN packets (assuming both protocols are implemented in the same device and use a common transmitter) 9 . A priority of access is given to Bluetooth for transmitting voice packets , while WLAN is given priority for transmitting data. Non-collaborative mechanisms . The non-collaborative mechanisms range from adaptive frequency hopping 11 to packet scheduling and traffic control 4 . They all use similar techniques for detecting the presence of other devices in the band such as measuring the bit or frame error rate , the signal strength or the signal to interference ratio (often implemented as the Received Signal Indicator", "label": ["wlan", "bias", "qos", "inteference", "dynamic scheduling", "bluetooth", "scheduling priorities", "interference", "coexistence", "mac scheduling", "wpans", "wpan"], "stemmed_label": ["wlan", "bia", "qo", "intefer", "dynam schedul", "bluetooth", "schedul prioriti", "interfer", "coexist", "mac schedul", "wpan", "wpan"]}
{"doc": "This paper examines the average page quality over time of pages downloaded during a web crawl of 328 million unique pages . We use the connectivity-based metric PageRank to measure the quality of a page . We show that traversing the web graph in breadth-first search order is a good crawling strategy , as it tends to discover high-quality pages early on in the crawl . INTRODUCTION According to a study released in October 2000 , the directly accessible \"surface web\" consists of about 2.5 billion pages , while the \"deep web\" (dynamically generated web pages) consists of about 550 billion pages , 95% of which are publicly accessible 9 . By comparison , the Google index released in June 2000 contained 560 million full-text-indexed pages 5 . In other words , Google -- which , according to a recent measurement 6 , has the greatest coverage of all search engines -covers only about 0.1% of the publicly accessible web , and the other major search engines do even worse. Increasing the coverage of existing search engines by three orders of magnitude would pose a number of technical challenges , both with respect to their ability to discover , download , and index web pages , as well as their ability to serve queries against an index of that size . (For query engines based on inverted lists , the cost of serving a query is linear to the size of the index.) Therefore , search engines should attempt to download the best pages and include (only) them in their index. Cho , Garcia-Molina , and Page 4 suggested using connectivity -based document quality metrics to direct a crawler towards high-quality pages . They performed a series of crawls over 179,000 pages in the stanford.edu domain and used Copyright is held by the author/owner. WWW10 , May 1-5 , 2001 , Hong Kong. ACM 1-58113-348-0/01/0005. different ordering metrics -- breadth-first , backlink count, PageRank 2 , and random -- to direct the different crawls. Under the breath-first ordering , pages are crawled in the order they are discovered . Under the backlink ordering , the pages with the highest number of known links to them are crawled first . Under the PageRank ordering , pages with the highest PageRank (a page quality metric described below) are crawled first . Under the random ordering , the crawler selects the next page to download at random from the set of uncrawled pages . (For repeatability , these crawls were \"virtual\"; that is , they were performed over a cached copy of these 179,000 pages.) Cho et al . evaluated the effectiveness of each ordering metric by examining how fast it led the crawler to all the \"hot\" pages . In this context , a \"hot\" page is a page with either a high number of links pointing to it , or a page with a high PageRank . They found that using the PageRank metric to direct a crawler works extremely well . However , they also discovered that performing the crawl in breadth-first", "label": ["", "high quality pages", "breadth first search", "crawl order", "ordering metrics", "crawling", "crawling", "pagerank", "page quality metric", "breadth-first search", "connectivity-based metrics"], "stemmed_label": ["", "high qualiti page", "breadth first search", "crawl order", "order metric", "crawl", "crawl", "pagerank", "page qualiti metric", "breadth-first search", "connectivity-bas metric"]}
{"doc": "Many instant messenger (IM) clients let a person specify the identifying name that appears in another person's contact list . We have noticed that many people add extra information to this name as a way to broadcast information to their contacts . Twelve IM contact lists comprising 444 individuals were monitored over three weeks to observe how these individuals used and altered their display names . Almost half of them changed their display names at varying frequencies , where the new information fell into seventeen different categories of communication supplied to others . Three themes encompass these categories: Identification (\"who am I\"?) , Information About Self (\"this is what is going on with me\") and Broadcast Message (\"I am directing information to the community\") . The design implication is that systems supporting person to person casual interaction , such as IM , should explicitly include facilities that allow people to broadcast these types of information to their community of contacts . INTRODUCTION Millions of people use instant messenger (IM) clients daily to communicate with friends , relatives , co-workers and even online dating contacts . With this explosion of use , researchers have taken to studying instant messaging and its impact . Much of the research regarding IM has been focused on its primary uses: maintaining awareness of a contact's presence and availability, how people (usually dyads) converse via text chat , and how they exploit other features such as file sharing and receipt of notifications . For example , studies of IM use in the workplace expose how it supports collaboration , communication and project activities 3 , 10 , 13 , as well as its negative effects 15 such as disruption 4 . In more social contexts , researchers found a positive relationship between the amount of IM use and verbal , affective and social intimacy 9 . IM also proves important in the life of teens , where it helps support the maintenance of their social relationships 8 . Other computer-mediated communication tools , such as MUDs (Multi-User Domains or Multi-User Dungeons) , IRC (Internet Relay Chat) , and broadcast messaging tools also allow spontaneous real-time (or synchronous) communication with other users . However , there are significant differences between them . IM is predominately used between people who are known to each other outside of cyberspace , e.g. , friends and associates. IM conversations are also private , and tend to be between pairs of people . They are also person centered and not group centered: while a contact list may collect one's `buddies' , these lists are not shared across contacts . In contrast , MUDs and IRC are public channels , where any conversation is heard by all people currently in the MUD or IRC . Most tend to be used by `strangers' , i.e. , those who are unknown to each other in real space , and usually involve more than two individuals . Indeed, the norm is for participants to protect their anonymity by displaying a pseudonym rather than their real names . Any", "label": ["communication", "communication catogories", "name variation handles", "identification is fundamental", "related im research", "distribution frequency of various catogories", "display names", "instant messenger", "awareness", "msn messager", "broadcast information", "catorgorisation of display names", "instant messaging", "display name"], "stemmed_label": ["commun", "commun catogori", "name variat handl", "identif is fundament", "relat im research", "distribut frequenc of variou catogori", "display name", "instant messeng", "awar", "msn messag", "broadcast inform", "catorgoris of display name", "instant messag", "display name"]}
{"doc": "This paper describes the building of a research library for studying the Web , especially research on how the structure and content of the Web change over time . The library is particularly aimed at supporting social scientists for whom the Web is both a fascinating social phenomenon and a mirror on society . The library is built on the collections of the Internet Archive , which has been preserving a crawl of the Web every two months since 1996 . The technical challenges in organizing this data for research fall into two categories: high-performance computing to transfer and manage the very large amounts of data , and human-computer interfaces that empower research by non-computer specialists . 1 . BACKGROUND 1.1 Research in the History of the Web The Web is one of the most interesting artifacts of our time . For social scientists , it is a subject of study both for itself and for the manner in which it illuminates contemporary social phenomena . Yet a researcher who wishes to study the Web is faced with major difficulties. An obvious problem is that the Web is huge . Any study of the Web as a whole must be prepared to analyze billions of pages and hundreds of terabytes of data . Furthermore , the Web changes continually . It is never possible to repeat a study on the actual Web with quite the same data . Any snapshot of the whole Web requires a crawl that will take several weeks to gather data . Because the size and boundaries of the Web are ill defined , basic parameters are hard to come by and it is almost impossible to generate random samples for statistical purposes. But the biggest problem that social scientists face in carrying out Web research is historical: the desire to track activities across time. The Web of today can be studied by direct Web crawling , or via tools such as the Google Web API1 , while Amazon has recently made its older Alexa corpus commercially available for the development of searching and related services2 . However , the only collection that can be used for more general research into the history of the Web is the Web collection of the Internet Archive . 1.2 The Internet Archive Everybody with an interest in the history of the Web must be grateful to Brewster Kahle for his foresight in preserving the content of the Web for future generations , through the not-for-profit Internet Archive and through Alexa Internet , Inc. , which he also founded. 1 The Google Web Search API allows a client to submit a limited number of search requests , using the SOAP and WSDL standards . See: http://www.google.com/apis/. 2 See http://websearch.alexa.com/welcome.html for the Alexa corpus made available by Amazon . This site also has a description of the relationship between Alexa Internet and the Internet Archive. 3 The Internet Archive's Web site is http://www.archive.org/. Copyright is held by the author/owner(s). JCDL'06 , June 11-15 , 2006 , Chapel Hill , North Carolina , USA.", "label": ["user interface", "dataflow", "internet archive. 1. background 1.1 research in the history of the web the web is one of the most interesting artifacts of our time. for social scientists", "history of the web", "basic parameters are hard to come by and it is almost impossible to generate random samples for statistical purposes. but the biggest problem that social scientists face in carrying out web research is historical", "or via tools such as the google web api", "database management", "it is a subject of study both for itself and for the manner in which it illuminates contemporary social phenomena. yet a researcher who wishes to study the web is faced with major difficulties. an obvious problem is that the web is huge. any study of the web as a whole must be prepared to analyze billions of pages and hundreds of terabytes of data. furthermore", "storage", "flexible preload system", "internet archive", "digital libraries", "scalability", "the web changes continually. it is never possible to repeat a study on the actual web with quite the same data. any snapshot of the whole web requires a crawl that will take several weeks to gather data. because the size and boundaries of the web are ill defined", "database access", "user support", "computational social science", "full text indexes", "the desire to track activities across time. the web of today can be studied by direct web crawling"], "stemmed_label": ["user interfac", "dataflow", "internet archive. 1. background 1.1 research in the histori of the web the web is one of the most interest artifact of our time. for social scientist", "histori of the web", "basic paramet are hard to come by and it is almost imposs to gener random sampl for statist purposes. but the biggest problem that social scientist face in carri out web research is histor", "or via tool such as the googl web api", "databas manag", "it is a subject of studi both for itself and for the manner in which it illumin contemporari social phenomena. yet a research who wish to studi the web is face with major difficulties. an obviou problem is that the web is huge. ani studi of the web as a whole must be prepar to analyz billion of page and hundr of terabyt of data. furthermor", "storag", "flexibl preload system", "internet archiv", "digit librari", "scalabl", "the web chang continually. it is never possibl to repeat a studi on the actual web with quit the same data. ani snapshot of the whole web requir a crawl that will take sever week to gather data. becaus the size and boundari of the web are ill defin", "databas access", "user support", "comput social scienc", "full text index", "the desir to track activ across time. the web of today can be studi by direct web crawl"]}
{"doc": "This working group laid the groundwork for the collection and analysis of oral histories of women computing educators . This endeavor will eventually create a body of narratives to serve as role models to attract students , in particular women , to computing; it will also serve to preserve the history of the female pioneers in computing education . Pre-conference work included administration of a survey to assess topical interest . The working group produced aids for conducting interviews , including an opening script , an outline of topics to be covered , guidelines for conducting interviews , and a set of probing questions to ensure consistency in the interviews . The group explored issues such as copyright and archival that confront the large-scale implementation of the project and suggested extensions to this research . This report includes an annotated bibliography of resources . The next steps will include training colleagues in how to conduct interviews and establishing guidelines for archival and use of the interviews . INTRODUCTION During the SIGCSE Technical Symposium held in Reno , NV in February 2003 , a significant number of events focused on under-representation of women in the computing curriculum and as computing educators . Eric Roberts' keynote talk , \"Expanding the Audience for Computer Science\" 21 , was a moving discussion of inclusiveness and a lament about the consequences of non-inclusion . At the Friday luncheon , Jane Margolis and Allan Fisher discussed results from their groundbreaking work, Unlocking the Clubhouse 15 . Several private discussions begun at the conference and continuing for some time afterward led to a November 2004 , proposal for this Working Group. In this report , we document the results from a Working Group of computer science educators at the 2005 ITiCSE conference held in Lisbon , Portugal . We were drawn together by our shared concern about women's under-representation among computing educators . We wished to honor women who had persevered in the early days of this field and to make their stories available as a resource for those following after. ITiCSE working groups are convened for the purpose of intensive collaborative work on a topic of common interest among the participants , prior to and during the conference , generally 174 completing the Working Group's task by conference end . In contrast , this group was convened to lay the groundwork for a project that we hope will continue for some time to come . The Working Group leaders spent the preceding 18 months formulating the charter for the Working Group: to collect oral histories from pioneering women in computing education . The goal of the Working Group meetings at ITiCSE in Lisbon was to formulate a plan that could bring the charter to fruition. We envision that the result of this project will be a large oral history collection of broad scope with potential value to researchers and others engaged in a variety of different projects. Because this project could result in a large quantity of data , it cannot be stored by one person in her", "label": ["oral history", "computing education history"], "stemmed_label": ["oral histori", "comput educ histori"]}
{"doc": "Emerging technologies are set to provide further provisions for computing in times when the limits of current technology of microelectronics become an ever closer presence . A technology roadmap document lists biologically-inspired computing and quantum computing as two emerging technology vectors for novel computing architectures A href=\"5.html#12\" 43 . But the potential benefits that will come from entering the nanoelectronics era and from exploring novel nanotechnologies are foreseen to come at the cost of increased sensitivity to influences from the surrounding environment . This paper elaborates on a dependability perspective over these two emerging technology vectors from a designer's standpoint . Maintaining or increasing the dependability of unconventional computational processes is discussed in two different contexts: one of a bio-inspired computing architecture (the Embryonics project) and another of a quantum computational architecture (the QUERIST project) . INTRODUCTION High-end computing has reached nearly every corner of our present day life , in a variety of forms taylored to accommodate either general purpose or specialized applications . Computers may be considerred as fine exponents of the present days' technological wave if not their finest , they certainly do count as solid , indispensable support for the finest. From the very beginning of the computing advent , the main target was squeezing out any additional performance . The inception period was not always trouble-free , accurate computation results being required at an ever faster pace on a road that has become manifold: some applications do require computational speed as a top priority; others are set for the highest possible dependability, while still delivering sufficient performance levels. Several definitions for dependability have been proposed: \"the ability of a system to avoid service failures that are more frequent or more severe than is acceptable\" A href=\"5.html#11\" 2 , or \"the property of a computer system such that reliance can justifiably be placed on the service it delivers\" A href=\"5.html#11\" 9 A href=\"5.html#12\" 45 . Dependability is therefore a synthetic term specifying a qualitative system descriptor that can generally be quantified through a list of attributes including reliability , fault tolerance , availability , and others. In real world , a dependable system would have to operate normally over extended periods of time before experiencing any fail (reliability , availability) and to recover quickly from errors (fault tolerance , self-test and self-repair) . The term \"acceptable\" has an essential meaning within the dependability's definition, setting the upper limits of the damages that can be supported by the system while still remaining functional or computationally accurate . A dependability analysis should take into consideration if not quantitative figures for the acceptable damage limit , at least a qualitative parameter representation for its attributes. Dependable systems are therefore crucial for applications that prohibit or limit human interventions , such as long-term exposure to aggressive (or even hostile) environments . The best examples are long term operating machines as required by managing deep-underwater/nuclear activities and outer space exploration. There are three main concerns that should be posed through a system's design in order to achieve high dependability A href=\"5.html#12\" 42 :", "label": ["emerging technologies", "self replication", "embryonics", "computing technology", "error detection", "fault tolerance", "digital devices", "computing architecture", "environment", "soft errors", "dependable system", "computing system", "system design", "correction techniques", "bio-inspired digital design", "bio-inspired computing", "reliability", "dependability", "nano computing", "failure rate", "emerging technologies", "nanoelectronics", "bio-inspired computing", "self repair", "evolvable hardware", "computer system", "quantum computing", "fault-tolerance assessment", "querist", "bio-computing", "reliability", "quantum computing"], "stemmed_label": ["emerg technolog", "self replic", "embryon", "comput technolog", "error detect", "fault toler", "digit devic", "comput architectur", "environ", "soft error", "depend system", "comput system", "system design", "correct techniqu", "bio-inspir digit design", "bio-inspir comput", "reliabl", "depend", "nano comput", "failur rate", "emerg technolog", "nanoelectron", "bio-inspir comput", "self repair", "evolv hardwar", "comput system", "quantum comput", "fault-toler assess", "querist", "bio-comput", "reliabl", "quantum comput"]}
{"doc": "This paper introduces a rationale for and approach to the study of sustainability in computerized community information systems . It begins by presenting a theoretical framework for posing questions about sustainability predicated upon assumptions from social construction of technology and adaptive structuration theories . Based in part on the literature and in part on our own experiences in developing a community information system , we introduce and consider three issues related to sustainability: stakeholder involvement , commitment from key players , and the development of critical mass . INTRODUCTION New technologies make it feasible and in many cases practical for individuals , groups , and organizations to collaborate in the development of joint information systems . In fact , over the last three decades of evolution , few applications of information technology have stimulated so much interest on the part of so many . Collaborative information systems are attractive to users because they make it possible to find information from diverse sources in an easy and efficient way . Such systems make good sense for information providers because it becomes possible to attract a larger audience than a solitary effort might otherwise be able to command and to pool resources to achieve certain economies in scale and technology expense . The advantages of collaborative computerized information systems have been widely recognized , but this has been particularly the case for those with the goal of making community information more available, accessible , and oriented toward community development. Computerized community information systems are diverse in form and , over time , have come to be known by many different names, including community bulletin boards , civic networks , community networks , community information networks , televillages , smart communities , and Free-Nets . They have been initiated by many different sponsors , including government organizations at the federal , state , and local levels , academic organizations , libraries, and ad hoc groups of citizens that may or may not later transform their enterprises into not-for-profit organizations 7 . With respect to longevity , these projects have come and gone , only to be replaced by newer and more sophisticated manifestations of the same basic information sharing capabilities. Consistent with the evolution of technology over the last thirty years , Kubicek and Wagner 14 analyze the historical trajectory of community networks to understand how these applications have evolved over time based upon their animating ideas , the zeitgeist of the time , the state of technology access , and the kinds of services such applications make available . Their analysis makes it possible to see that there has never been a standard for design or operation when it comes to community information systems . Instead , each such project has been very much a social experiment , born of a cluster of varied ideas related to the general theme of using technology to promote the development of vibrant geographically-based communities. Since there has been no standard to follow , each instance of computerized community information system can be seen as an experiment in", "label": ["critical mass", "construction of technology", "key players", "community network", "computerized community information system", "participatory design", "sustainability", "skateholder involvement", "community networks"], "stemmed_label": ["critic mass", "construct of technolog", "key player", "commun network", "computer commun inform system", "participatori design", "sustain", "skatehold involv", "commun network"]}
{"doc": "Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications , such as intrusion detection systems and spam e-mail filtering . However , machine learning algorithms themselves can be a target of attack by a malicious adversary . This paper provides a framework for answering the question , \"Can machine learning be secure?\" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems , a variety of defenses against those attacks , a discussion of ideas that are important to security for machine learning , an analytical model giving a lower bound on attacker's work function , and a list of open problems . INTRODUCTION Machine learning techniques are being applied to a growing number of systems and networking problems , particularly those problems where the intention is to detect anomalous system behavior . For instance , network Intrusion Detection Systems (IDS) monitor network traffic to detect abnormal activities , such as attacks against hosts or servers . Machine learning techniques offer the benefit that they can detect novel differences in traffic (which presumably represent attack traffic) by being trained on normal (known good) and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. ASIACCS'06 , March 2124 , 2006 , Taipei , Taiwan Copyright 2006 ACM 1-59593-272-0/06/0003 ... $ 5.00 attack (known bad) traffic . The traditional approach to designing an IDS relied on an expert codifying rules defining normal behavior and intrusions 26 . Because this approach often fails to detect novel intrusions , a variety of researchers have proposed incorporating machine learning techniques into intrusion detection systems 1 , 16 , 18 , 24 , 38 , 41 . On the other hand , use of machine learning opens the possibility of an adversary who maliciously \"mis-trains\" a learning system in an IDS . A natural question arises: what techniques (in their attacks) can an adversary use to confuse a learning system? This paper explores the larger question , as posed in the title of this paper , can machine learning be secure? Specific questions that we examine include: Can the adversary manipulate a learning system to permit a specific attack? For example , can an attacker leverage knowledge about the machine learning system used by a spam e-mail filtering system to bypass the filtering? Can an adversary degrade the performance of a learning system to the extent that system administrators are forced to disable the IDS? For example , could the attacker confuse the system and cause valid e-mail to be rejected? What defenses exist against adversaries manipulating (attacking) learning systems? More generally , what is the", "label": ["indiscriminate attack", "targeted attack", "statistical learning", "exploratory attack", "machine learning", "security metrics", "integrity", "availability", "spam filters", "causative attack", "intrusion detection", "machine learning", "intrusion detection system", "computer security", "game theory", "adversarial learning", "learning algorithms", "computer networks", "security"], "stemmed_label": ["indiscrimin attack", "target attack", "statist learn", "exploratori attack", "machin learn", "secur metric", "integr", "avail", "spam filter", "caus attack", "intrus detect", "machin learn", "intrus detect system", "comput secur", "game theori", "adversari learn", "learn algorithm", "comput network", "secur"]}
{"doc": "The Catenaccio system integrates information retrieval with sketch manipulations . The system is designed especially for pen-based computing and allows users to retrieve information by simple pen manipulations such as drawing a picture . When a user draws a circle and writes a keyword , information nodes related to the keyword are collected automatically inside the circle . In addition , the user can create a Venn diagram by repeatedly drawing circles and keywords to form more complex queries . Thus , the user can retrieve information both interactively and visually without complex manipulations . Moreover , the sketch interaction is so simple that it is possible to combine it with other types of data such as images and real-world information for information retrieval . In this paper , we describe our Catenaccio system and how it can be effectively applied . INTRODUCTION Pen-based computers , such as personal digital assistants (PDA) and tablet PCs , have been developed . These computers are characterized by simple sketch interfaces similar to drawing a picture on paper in the real world . This drawing manipulation is not especially useful for communicating details , but is effective for general use . It is especially useful for creative activities , so there have been a number of research reports on improving sketch manipulation 1 , 2 , 3 . In addition , some game devices (e.g. , Nintendo DS 4 ) support such kinds of interactions and provide many types of game content . In these systems , a user can use the entire system window as a workspace and create 3D CG from 2D drawings . However , as the original applications may not support information retrieval, the user has to use conventional retrieval applications along with pen-based input styles. Considerable research has been done to support the use of information visualization for retrieving information 5 . Technical visualization methods such as zooming and scaling can be used to effectively display huge amounts of data 6 , 7 , 8 . However, existing visualization systems focus on mouse manipulation (e.g., click and drag) , so they are not effectively designed for pen-based interactions such as a drawing. The most popular method of retrieving information is no doubt keyword searching . Search engines via the Web (e.g. , Google and Yahoo) have been generally used for keyword searching 12 , 13 , and people feel that they cannot live without such search engines. Generally , keyword searching requires users to input one or more keywords . In these systems , users can retrieve information related to the keywords with Boolean operations (e.g. , AND , OR and NOT) . However , the systems are based on conventional input methods . Users of pen-based computers have to write a query into a fixed dialog box with a stylus or pen. Therefore , we have been developing an information retrieval system based on simple sketch manipulations . Our goal is to devise an effective and simple information retrieval system that works on pen-based computers , so we integrated", "label": ["sketch manipulation", "information node", "venn diagram", "visual information retrieval", "pen-based computing", "image data", "information retrieval", "keyword searching", "2d system", "sketch manipulations", "interactive system", "catnaccio system", "interactive information retrieval system"], "stemmed_label": ["sketch manipul", "inform node", "venn diagram", "visual inform retriev", "pen-bas comput", "imag data", "inform retriev", "keyword search", "2d system", "sketch manipul", "interact system", "catnaccio system", "interact inform retriev system"]}
{"doc": "Compression reduces both the size of indexes and the time needed to evaluate queries . In this paper , we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms , considering two approaches to improving retrieval efficiency:better implementation and better choice of integer compression schemes . First , we propose several simple optimisations to well-known integer compression schemes , and show experimentally that these lead to significant reductions in time . Second , we explore the impact of choice of compression scheme on retrieval efficiency . In experiments on large collections of data , we show two surprising results:use of simple byte-aligned codes halves the query evaluation time compared to the most compact Golomb-Rice bitwise compression schemes; and , even when an index fits entirely in memory , byte-aligned codes result in faster query evaluation than does an uncompressed index , emphasising that the cost of transferring data from memory to the CPU cache is less for an appropriately compressed index than for an uncompressed index . Moreover , byte-aligned schemes have only a modest space overhead:the most compact schemes result in indexes that are around 10% of the size of the collection , while a byte-aligned scheme is around 13% . We conclude that fast byte-aligned codes should be used to store integers in inverted lists . INTRODUCTION Search engines have demanding performance requirements. Users expect fast answers to queries , many queries must be processed per second , and the quantity of data that must be searched in response to each query is staggering . The demands continue to grow:the Google search engine , for example , indexed around one billion documents a year ago and now manages more than double that figure 1 . Moreover, the increasing availability and affordability of large storage devices suggests that the amount of data stored online will continue to grow. Inverted indexes are used to evaluate queries in all practical search engines 14 . Compression of these indexes has three major benefits for performance . First , a compressed index requires less storage space . Second , compressed data makes better use of the available communication bandwidth; more information can be transfered per second than when the data is uncompressed . For fast decompression schemes, the total time cost of transfering compressed data and sub-sequently decompressing is potentially much less than the cost of transferring uncompressed data . Third , compression increases the likelihood that the part of the index required to evaluate a query is already cached in memory , thus entirely avoiding a disk access . Thus index compression can reduce costs in retrieval systems. We have found that an uncompressed inverted index that stores the location of the indexed words in web documents typically consumes more than 30% of the space required to store the uncompressed collection of documents . (Web documents often include a great deal of information that is not indexed , such as HTML tags; in the TREC web data, which we use in our experiments , on", "label": ["variable byte", "decoding", "efficiency", "integer coding", "bytewise compression", "search engine", "retrieval efficiency", "integer compression", "inverted indexes", "index compression", "optimisation", "compression", "inverted index", "document retrieval"], "stemmed_label": ["variabl byte", "decod", "effici", "integ code", "bytewis compress", "search engin", "retriev effici", "integ compress", "invert index", "index compress", "optimis", "compress", "invert index", "document retriev"]}
{"doc": "A consistent query answer in a possibly inconsistent database is an answer which is true in every (minimal) repair of the database . We present here a practical framework for computing consistent query answers for large , possibly inconsistent relational databases . We consider relational algebra queries without projection , and denial constraints . Because our framework handles union queries , we can effectively (and efficiently) extract indefinite disjunctive information from an inconsistent database . We describe a number of novel optimization techniques applicable in this context and summarize experimental results that validate our approach . INTRODUCTION Traditionally , the main role of integrity constraints in databases was to enforce consistency. The occurrence of integrity violations was prevented by DBMS software . However , while integrity constraints continue to express important semantic properties of data , enforcing the constraints has become problematic in current database applications. For example , in data integration systems integrity violations may be due to the presence of multiple autonomous data sources . The sources may separately satisfy the constraints , but when they are integrated the constraints may not hold . Moreover , because the sources are autonomous, the violations cannot be simply fixed by removing the data involved in the violations. Example 1 . Let Student be a relation schema with the attributes Name and Address and the key functional dependency N ame Address . Consider the following instance of Student: The first two tuples may come from different data sources, so it may be impossible or impractical to resolve the inconsistency between them . However , there is clearly a difference between the first two tuples and the third one . We don't know whether Jeremy Burford lives in Los Angeles or New York , but we do know that Linda Kenner lives in Chicago. An approach to query answering that ignores inconsistencies will be unable to make this distinction the distinction between reliable and unreliable data . On the other hand , any approach that simply masks out inconsistent data (the first two tuples in this example) will lose indefinite information present in inconsistent databases . In this example , we know that there is a student named Jeremy Burford (existential information) and that Jeremy Burford lives in Los Angeles or New York (disjunctive information). The above example illustrates the need to modify the standard notion of query answer in the context of inconsistent databases . We need to be able to talk about query answers that are unaffected by integrity violations . In 2 , the notion of consistent query answer was proposed to achieve that ob jective . 2 introduced the notion of repair: a database that satisfies the integrity constraints and is minimally different from the original database . A consistent answer to a query, in this framework , is an answer present in the result of the query in every repair. Example 2 . In Example 1 , there are two repairs corresponding to two different ways of restoring the consistency: either the first or the second tuple is deleted", "label": ["knowledge gathering", "conflict hypergraph", "inconsistency", "denial constraints", "inconsistent database", "optimization", "relational algebra", "polynomial time", "consistent query answer", "disjunctive query", "integrity constraints", "query processing", "repair"], "stemmed_label": ["knowledg gather", "conflict hypergraph", "inconsist", "denial constraint", "inconsist databas", "optim", "relat algebra", "polynomi time", "consist queri answer", "disjunct queri", "integr constraint", "queri process", "repair"]}
{"doc": "Research in consistent query answering studies the definition and computation of \"meaningful\" answers to queries posed to inconsistent databases , i.e. , databases whose data do not satisfy the integrity constraints (ICs) declared on their schema . Computing consistent answers to conjunctive queries is generally coNP-hard in data complexity , even in the presence of very restricted forms of ICs (single , unary keys) . Recent studies on consistent query answering for database schemas containing only key dependencies have an-alyzed the possibility of identifying classes of queries whose consistent answers can be obtained by a first-order rewriting of the query , which in turn can be easily formulated in SQL and directly evaluated through any relational DBMS . In this paper we study consistent query answering in the presence of key dependencies and exclusion dependencies . We first prove that even in the presence of only exclusion dependencies the problem is coNP-hard in data complexity , and define a general method for consistent answering of conjunctive queries under key and exclusion dependencies , based on the rewriting of the query in Datalog with negation . Then , we identify a subclass of conjunctive queries that can be first-order rewritten in the presence of key and exclusion dependencies , and define an algorithm for computing the first-order rewriting of a query belonging to such a class of queries . Finally , we compare the relative efficiency of the two methods for processing queries in the subclass above mentioned . Experimental results , conducted on a real and large database of the computer science engineering degrees of the University of Rome \"La Sapienza\" , clearly show the computational advantage of the first-order based technique . INTRODUCTION Suppose to have a database whose data violate the integrity constraints (ICs) declared on its schema . What are the answers that have to be returned to queries posed to such a database? The standard approach to this problem is through data cleaning , i.e. , by explicitly modifying the data in order to eliminate violation of ICs: only when data are \"repaired\" , i.e. , are consistent with the ICs , queries can be answered . However , in many situations it would be much more desirable to derive significant information from the database even in the presence of data inconsistent with the ICs . Indeed , in many application scenarios , the explicit repair of data is not convenient , or even not possible. This happens , for instance , in data integration applications, which provide a unified , virtual view of a set of autonomous information sources 5 . This alternative approach is the one followed by research in consistent query answering , which studies the definition (and computation) of \"meaningful\" answers to queries posed to databases whose data do not satisfy the ICs declared on the database schema 1 , 14 , 4 . All these approaches are based on the following principle: schema is stronger than data . In other words , the database schema (i.e. , the set of integrity constraints) is", "label": ["relational database", "query rewriting", "integrity constraints", "query rewriting", "consistent query answering", "computational complexity", "conjunctive queries", "inconsistent database", "inconsistency", "database schemas"], "stemmed_label": ["relat databas", "queri rewrit", "integr constraint", "queri rewrit", "consist queri answer", "comput complex", "conjunct queri", "inconsist databas", "inconsist", "databas schema"]}
{"doc": "Apart from completeness usability , performance and maintainability are the key quality aspects for Web information systems . Considering usability as key implies taking usage processes into account right from the beginning of systems development . Context-awareness appears as a promising idea for increasing usability of Web Information Systems . In the present paper we propose an approach to context-awareness of Web Information Systems that systematically distinguishes among the various important kinds of context . We show how parts of this context can be operationalized for increasing customers' usage comfort . Our approach permits designing Web information systems such that they meet high quality expectations concerning usability , performance and maintainability . We demonstrate the validity of our approach by discussing the part of a banking Web Information System dedicated to online home-loan application . Introduction 1.1 Generations of Web Services Understanding Web Information Systems (WIS) as monolithic and presentation-oriented query-answer systems would be too simplistic . Implementing the individual services of a WIS only on the basis of XML or (D)HTML suites suffices for the interface accessible by a particular customer . The quality of service provided by a WIS both expected and implemented, however , evolved over the last decade and has evolved beyond mere completeness . Extending the classification in (Berger 2003 , p.146) we distinguish between different generations of WIS. First generation (1G): \"build it , and they will come\" First develop a WIS , then customers will come, because they believe that it is useful . Many of the 1G-WIS were informational , i.e. , they weren't interactive . Second generation (2G): \"advertise online sales , and they will come\" Develop a WIS and market it . Customers will Copyright c 2004 , Australian Computer Society , Inc . This paper appeared at First Asia-Pacific Conference on Conceptual Modelling (APCCM 2004) , Dunedin , New Zealand . Conferences in Research and Practice in Information Technology , Vol. 31 . Sven Hartmann , John Roddick , Ed . Reproduction for academic , not-for profit purposes permitted provided this text is included. come , because the advertisement convinced them about the WIS's usability. The WIS may be transactional , i.e. , contain interactive interfaces to company products and services . A standard interface is provided but hard to learn . No particular customer usage aid is offered. Third generation (3G): \"realize a pleasant use of high quality services , and they will come\" Customers will find using the WIS helpful . They will do the marketing . 3G-WIS's typical characteristics are: high value and up-to-date content, high performance, brand value of the provider , and pleasant and easy use for casual as well as for frequent customers. Many WIS including several banking WIS are still 2G . However , impressive and well-developed WIS, e.g. , the Amazon web-site , demonstrate the feasibil-ity of 3G-WIS . The success of such WIS is based on deep understanding of the application area , the customers needs , abilities and habits . Adaptation to customers -- if provided -- is based on allocating", "label": ["scenarios", "web information systems", "web site", "web services", "usability", "story boarding", "context-awareness", "context-aware information systems", "web information system", "media objects", "scenes", "media type", "sitelang"], "stemmed_label": ["scenario", "web inform system", "web site", "web servic", "usabl", "stori board", "context-awar", "context-awar inform system", "web inform system", "media object", "scene", "media type", "sitelang"]}
{"doc": "This paper discusses the problem of partial object recognition in image databases . We propose the method to reconstruct and estimate partially occluded shapes and regions of objects in images from overlapping and cutting . We present the robust method for recognizing partially occluded objects based on symmetry properties , which is based on the contours of objects . Our method provides simple techniques to reconstruct occluded regions via a region copy using the symmetry axis within an object . Based on the estimated parameters for partially occluded objects , we perform object recognition on the classification tree . Since our method relies on reconstruction of the object based on the symmetry rather than statistical estimates , it has proven to be remarkably robust in recognizing partially occluded objects in the presence of scale changes , rotation , and viewpoint changes . INTRODUCTION Most existing methods for object recognition are based on full objects. However , many images in electronic catalogs contain multiple objects with occluded shapes and regions . Due to the occlusion of objects, image retrieval can provide incomplete , uncertain , and inaccurate results . To resolve this problem , we propose new method to reconstruct objects using symmetry properties since most objects in a given image database are represented by symmetrical figures. Even though there have been several efforts in object recognition with occlusion , currents methods have been highly sensitive to object pose, rotation , scaling , and visible portion of occluded objects 12 9 17 3 15 . In addition , many appearance-based and model-based object recognition methods assumed that they have known occluded regions of objects or images through extensive training processes with statistical approach . However , our approach is not limited to recognizing occluded objects by pose and scale changes , and does not need extensive training processes. Unlike existing methods , our method finds shapes and regions to reconstruct occluded shapes and regions within objects . Our approach can handle object rotation and scaling for dealing with occlusion , and does not require extensive training processes . The main advantage of our approach is that it becomes simple to reconstruct objects from occlusions . We present the robust method , which is based on the contours of objects , for recognizing partially occluded objects based on symmetry properties . The contour-based approach finds a symmetry axis using the maximum diameter from the occluded object. In experiments , we demonstrate how our method reconstruct and recognize occluded shapes and regions using symmetry . Experiments use rotated and scaled objects for dealing with occlusion . We also evaluate the recognition rate of the reconstructed objects using symmetry and the visible portion of the occluded objects for recognition. The rest of this paper is organized as follows . In Section 2 , we briefly review work related to this study . In Section 3 , we describe a method to recognize partial objects from given classes . In Section 4 , we describe experimental results for partial object recognition . Finally, we summarize this paper", "label": ["object recognition", "reconstruction", "object", "contour", "recognition", "symmetry", "image", "contour", "occlusion", "estimation", "symmetry"], "stemmed_label": ["object recognit", "reconstruct", "object", "contour", "recognit", "symmetri", "imag", "contour", "occlus", "estim", "symmetri"]}
{"doc": "In this paper we explore the connection between clustering categorical data and entropy: clusters of similar poi lower entropy than those of dissimilar ones . We use this connection to design an incremental heuristic algorithm , COOLCAT , which is capable of efficiently clustering large data sets of records with categorical attributes , and data streams . In contrast with other categorical clustering algorithms published in the past , COOLCAT's clustering results are very stable for different sample sizes and parameter settings . Also , the criteria for clustering is a very intuitive one , since it is deeply rooted on the well-known notion of entropy . Most importantly , COOLCAT is well equipped to deal with clustering of data streams (continuously arriving streams of data point) since it is an incremental algorithm capable of clustering new points without having to look at every point that has been clustered so far . We demonstrate the efficiency and scalability of COOLCAT by a series of experiments on real and synthetic data sets . INTRODUCTION Clustering is a widely used technique in which data points are partitioned into groups , in such a way that points in the same group , or cluster , are more similar among themselves than to those in other clusters . Clustering of categorical attributes (i.e. , attributes whose domain is not numeric) is a difficult , yet important task: many fields , from statistics to psychology deal with categorical data. In spite of its importance , the task of categorical clustering has received scant attention in the KDD community as of late , with only a handful of publications addressing the problem ( 18 , 14, 12 ). Much of the published algorithms to cluster categorical data rely on the usage of a distance metric that captures the separation between two vectors of categorical attributes, such as the Jaccard coefficient . In this paper , we present COOLCAT (the name comes from the fact that we reduce the entropy of the clusters , thereby \"cooling\" them) , a novel method which uses the notion of entropy to group records. We argue that a classical notion such as entropy is a more natural and intuitive way of relating records , and more importantly does not rely in arbitrary distance metrics . COOLCAT is an incremental algorithm that aims to minimize the expected entropy of the clusters . Given a set of clusters , COOLCAT will place the next point in the cluster where it minimizes the overall expected entropy . COOLCAT acts incrementally , and it is capable to cluster every new point without having to re-process the entire set . Therefore , COOLCAT is suited to cluster data streams (contin-uosly incoming data points) 2 . This makes COOLCAT applicable in a large variety of emerging applications such as intrusion detection , and e-commerce data. This paper is set up as follows . Section 2 offers the background and relationship between entropy and clustering , and formulates the problem . Section 3 reviews the related work. Section 4 describes", "label": ["data streams", "incremental algorithm", "coolcat", "categorical clustering", "data stream", "entropy", "clustering"], "stemmed_label": ["data stream", "increment algorithm", "coolcat", "categor cluster", "data stream", "entropi", "cluster"]}
{"doc": "This paper provides an account of new measures of coupling and cohesion developed to assess the reusability of Java components retrieved from the internet by a search engine . These measures differ from the majority of established metrics in two respects: they reflect the degree to which entities are coupled or resemble each other , and they take account of indirect couplings or similarities . An empirical comparison of the new measures with eight established metrics shows the new measures are consistently superior at ranking components according to their reusability . INTRODUCTION The work reported in this paper arose as part of a project that retrieves Java components from the internet 1 . However, components retrieved from the internet are notoriously variable in quality . It seems highly desirable that the search engine should also provide an indication of both how reliable the component is and how readily it may be adapted in a larger software system. A well designed component , in which the functionality has been appropriately distributed to its various subcomponents , is more likely to be fault free and easier to adapt . Appropriate distribution of function underlies two key concepts: coupling and cohesion. Coupling is the extent to which the various subcomponents interact . If they are highly interdependent then changes to one are likely to have significant effects on others . Hence loose coupling is desirable . Cohesion is the extent to which the functions performed by a subsystem are related . If a subcomponent is responsible for a number of unrelated functions then the functionality has been poorly distributed to subcomponents. Hence high cohesion is a characteristic of a well designed subcomponent. We decided that the component search engine should provide the quality rankings of retrieved components based on measures of their coupling and cohesion . There is a substantial literature on coupling and cohesion metrics which is surveyed in the next section . We then describe in detail the metrics we have developed which attempt to address some of the limitations of existing metrics . In particular , we consider both the strength and transitivity of dependencies . The following section describes an empirical comparison of our proposed metrics and several popular alternatives as predictors of reusability . Section 5 presents an analysis of the results which demonstrate that our proposed metrics consistently outperform the others . The paper concludes with a discussion of the implications of the research. COUPLING AND COHESION METRICS Cohesion is a measure of the extent to which the various functions performed by an entity are related to one another . Most metrics assess this by considering whether the methods of a class access similar sets of instance variables . Coupling is the degree of interaction between classes . Many researches have been done on software metrics 8 , the most important ones are selected used in our comparative study . Table 1 and Table 2 summarize the characteristics of these cohesion and coupling metrics. Table 1 . Coupling metrics Name Definition CBO 4 5 11 Classes are coupled if", "label": ["binary quantity", "experimentary comparsion", "component search engine", "search engine technology", "spearman rank correlation", "intransitive relation", "reusability", "coupling", "cohesion metric", "linear regression", "cohesion", "java components"], "stemmed_label": ["binari quantiti", "experimentari compars", "compon search engin", "search engin technolog", "spearman rank correl", "intransit relat", "reusabl", "coupl", "cohes metric", "linear regress", "cohes", "java compon"]}
{"doc": "We present Repo-3D , a general-purpose , object-oriented library for developing distributed , interactive 3D graphics applications across a range of heterogeneous workstations . Repo-3D is designed to make it easy for programmers to rapidly build prototypes using a familiar multi-threaded , object-oriented programming paradigm . All data sharing of both graphical and non-graphical data is done via general-purpose remote and replicated objects , presenting the illusion of a single distributed shared memory . Graphical objects are directly distributed , circumventing the \"duplicate database\" problem and allowing programmers to focus on the application details . Repo-3D is embedded in Repo , an interpreted , lexically-scoped , distributed programming language , allowing entire applications to be rapidly prototyped . We discuss Repo-3D's design , and introduce the notion of local variations to the graphical objects , which allow local changes to be applied to shared graphical structures . Local variations are needed to support transient local changes , such as highlighting , and responsive local editing operations . Finally , we discuss how our approach could be applied using other programming languages , such as Java . INTRODUCTION Traditionally , distributed graphics has referred to the architecture of a single graphical application whose components are distributed over multiple machines 14 , 15 , 19 , 27 A href=\"6.html#1\" (Figure 1 a ) . By taking advantage of the combined power of multiple machines , and the particular features of individual machines , otherwise impractical applications became feasible . However , as machines have grown more powerful and application domains such as Computer 1 . bm,feiner @cs.columbia.edu , http://www.cs.columbia.edu/graphics Supported Cooperative Work (CSCW) and Distributed Virtual Environments (DVEs) have been making the transition from research labs to commercial products , the term distributed graphics is increasingly used to refer to systems for distributing the shared graphical state of multi-display/multi-person , distributed , interactive applications A href=\"6.html#1\" (Figure 1b) . This is the definition that we use here. While many excellent , high-level programming libraries are available for building stand-alone 3D applications (e.g . Inventor 35 , Performer 29 , Java 3D 33 ) , there are no similarly powerful and general libraries for building distributed 3D graphics applications . All CSCW and DVE systems with which we are familiar (e.g. , 1 , 7 , 11 , 12 , 16 , 28 , 30 , 31 , 32 , 34 , 37 , 41 ) use the following approach: A mechanism is provided for distributing application state (either a custom solution or one based on a general-purpose distributed programming environment , such as ISIS 4 or Obliq 8 ) , and the state of the graphical display is maintained separately in the local graphics library . Keeping these \"dual databases\" synchronized is a complex , tedious , and error-prone endeavor . In contrast , some non-distributed libraries , such as Inventor 35 , allow programmers to avoid this problem by using the graphical scene description to encode application state . Extending this \"single database\" model to a distributed 3D graphics library is the goal", "label": ["data sharing", "programming language", "distributed graphics", "data structures", "interactive graphical application", "3d graphics library", "change notification", "library", "replicated object", "object representation", "distributed virtual environments", "shared memory", "syncronisation", "data distribution", "object-oriented graphics", "java", "programming", "duplicate database", "local variations", "multi-threaded programming", "heterogeneous workstation", "multi-user interaction", "3d graphics application", "repo-3d", "3d graphics", "callbacks", "prototyping", "distributed systems", "client-server approach", "object-oriented library", "programming language", "shared-data object model", "prototype", "graphical objects", "client-server", "object-oriented", "local variation", "3d graphics", "object oriented", "distributed applications", "distributed shared memory", "distributed processes", "properties"], "stemmed_label": ["data share", "program languag", "distribut graphic", "data structur", "interact graphic applic", "3d graphic librari", "chang notif", "librari", "replic object", "object represent", "distribut virtual environ", "share memori", "syncronis", "data distribut", "object-ori graphic", "java", "program", "duplic databas", "local variat", "multi-thread program", "heterogen workstat", "multi-us interact", "3d graphic applic", "repo-3d", "3d graphic", "callback", "prototyp", "distribut system", "client-serv approach", "object-ori librari", "program languag", "shared-data object model", "prototyp", "graphic object", "client-serv", "object-ori", "local variat", "3d graphic", "object orient", "distribut applic", "distribut share memori", "distribut process", "properti"]}
{"doc": "It is important yet hard to identify navigational queries in Web search due to a lack of sufficient information in Web queries , which are typically very short . In this paper we study several machine learning methods , including naive Bayes model , maximum entropy model , support vector machine (SVM) , and stochastic gradient boosting tree (SGBT) , for navigational query identification in Web search . To boost the performance of these machine techniques , we exploit several feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance . Different from most prior work that uses a small number of features , in this paper , we study the problem of identifying navigational queries with thousands of available features , extracted from major commercial search engine results , Web search user click data , query log , and the whole Web's relational content . A multi-level feature extraction system is constructed . Our results on real search data show that 1) Among all the features we tested , user click distribution features are the most important set of features for identifying navigational queries . 2) In order to achieve good performance , machine learning approaches have to be coupled with good feature selection methods . We find that gradient boosting tree , coupled with linear SVM feature selection is most effective . 3) With carefully coupled feature selection and classification approaches , navigational queries can be accurately identified with 88.1% F1 score , which is 33% error rate reduction compared to the best uncoupled system , and 40% error rate reduction compared to a well tuned system without feature selection . INTRODUCTION Nowadays , Web search has become the main method for information seeking . Users may have a variety of intents while performing a search . For example , some users may already have in mind the site they want to visit when they type a query; they may not know the URL of the site or may not want to type in the full URL and may rely on the search engine to bring up the right site . Yet others may have no idea of what sites to visit before seeing the results . The information they are seeking normally exists on more than one page. Knowing the different intents associated with a query may dramatically improve search quality . For example , if a query is known to be navigational , we can improve search results by developing a special ranking function for navigational queries . The presentation of the search results or the user-perceived relevance can also be improved by only showing the top results and reserving the rest of space for other purposes since users only care about the top result of a navigational query . According to our statistics , about 18% of queries in Web search are navigational (see Section 6) . Thus, correctly identifying navigational queries has a great potential to improve search performance. Navigational query identification is not trivial due to a lack of", "label": ["stochastic gradient boosting tree", "linear svm feature ranking", "gradient boosting tree", "information gain", "naive bayes classifier", "support vector machine", "experiments results", "machine learning", "maximum entropy classifier", "navigational query classification", "navigational and informational query", "multiple level feature system"], "stemmed_label": ["stochast gradient boost tree", "linear svm featur rank", "gradient boost tree", "inform gain", "naiv bay classifi", "support vector machin", "experi result", "machin learn", "maximum entropi classifi", "navig queri classif", "navig and inform queri", "multipl level featur system"]}
{"doc": "Functional verification is widely acknowledged as the bottleneck in the hardware design cycle . This paper addresses one of the main challenges of simulation based verification (or dynamic verification ) , by providing a new approach for Coverage Directed Test Generation (CDG) . This approach is based on Bayesian networks and computer learning techniques . It provides an efficient way for closing a feedback loop from the coverage domain back to a generator that produces new stimuli to the tested design . In this paper , we show how to apply Bayesian networks to the CDG problem . Applying Bayesian networks to the CDG framework has been tested in several experiments , exhibiting encouraging results and indicating that the suggested approach can be used to achieve CDG goals . INTRODUCTION Functional verification is widely acknowledged as the bottleneck in the hardware design cycle 1 . To date , up to 70% of the design development time and resources are spent on functional verification . The increasing complexity of hardware designs raises the need for the development of new techniques and methodologies that can provide the verification team with the means to achieve its goals quickly and with limited resources. The current practice for functional verification of complex designs starts with a definition of a test plan , comprised of a large set of events that the verification team would like to observe during the verification process . The test plan is usually implemented using random test generators that produce a large number of test-cases , and coverage tools that detect the occurrence of events in the test plan , and provide information related to the progress of the test plan . Analysis of the coverage reports allows the verification team to modify the directives for the test generators and to better hit areas or specific tasks in the design that are not covered well 5 . The analysis of coverage reports , and their translation to a set of test generator directives to guide and enhance the implementation of the test plan , result in major manual bottlenecks in the otherwise highly automated verification process . Considerable effort is invested in finding ways to close the loop of coverage analysis and test generation . Coverage directed test generation (CDG) is a technique to automate the feedback from coverage analysis to test generation . The main goals of CDG are to improve the coverage progress rate , to help reaching uncovered tasks , and to provide many different ways to reach a given coverage task . Achieving these goals should increase the efficiency and quality of the verification process and reduce the time and effort needed to implement a test plan. In this paper , we propose a new approach for coverage directed test generation . Our approach is to cast CDG in a statistical inference framework , and apply computer learning techniques to achieve the CDG goals . Specifically , our approach is based on modeling the relationship between the coverage information and the directives to the test generator using Bayesian networks", "label": ["coverage directed test generation", "conditional probability", "functional verification", "bayesian networks", "bidirectional inferences", "maximal a posteriori", "dynamic bayesian network", "design under test", "coverage model", "coverage analysis", "most probable explanation", "markov chain"], "stemmed_label": ["coverag direct test gener", "condit probabl", "function verif", "bayesian network", "bidirect infer", "maxim a posteriori", "dynam bayesian network", "design under test", "coverag model", "coverag analysi", "most probabl explan", "markov chain"]}
{"doc": "An index connects readers with information . Creating an index for a single book is a time-honored craft . Creating an index for a massive library of HTML topics is a modern craft that has largely been discarded in favor of robust search engines . The authors show how they optimized a single-sourced index for collections of HTML topics , printed books , and PDF books . With examples from a recent index of 24,000 entries for 7,000 distinct HTML topics also published as 40 different PDF books , the authors discuss the connections between modern technology and traditional information retrieval methods that made the index possible , usable , and efficient to create and maintain . THE PROBLEM A project with a large library of existing documentation of about 40 books (several with multiple volumes) required a high-quality master index . The material was written using a proprietary SGML authoring tool and converted to both HTML for browser-based information and PDF . The library was being converted from a set of books into a task-oriented , topic-based set of documentation , so very little indexing time was available for the approximately 20 authors in two sites fully engaged in rewriting the documentation to conform to new guidelines for online topics, to incorporate new content , and to change content for the next release of their product . The four project editors responsible for the complete library were likewise busy assisting the structural conversion of the library . Customers were asking for more direct ways to find information . At a user conference , 52 percent of users said the former book indexes were their primary entry point into product information. Given these imposing (but sadly typical) resource constraints, the information architect for the project and an editor with extensive indexing experience worked together to develop an approach that would use technology to maximize the available efforts of both authors and editors . This paper describes the approach from the perspectives of the authors , editors , and , most importantly , the users of this set of documentation . The approach is described in enough detail to enable other projects to adapt the approach to the constraints of their situation . The paper also indicates the future directions that the project will explore. The challenges to producing a high-quality master index were not just posed by available resources or by available technology, but also by the writing culture of the project . The project had historically been heavily oriented towards writing and producing books--the HTML documentation set had been simply a collection of books converted to HTML . As a result , navigation through the HTML documentation was difficult; there were almost no links between books , and each book was organized under its own table of contents . Full-text search of the HTML books had been the only method of finding information across the complete library , if a user didn't know which book to consult directly. The product market share was expanding , and new users had to learn", "label": ["book indexes", "information retrieval methods", "massive master", "drop-down selection of terms", "indexing", "sql data type", "search", "primary index entry", "internally developed format", "automation", "indexing problems", "human factors", "html master index", "online information", "navigation"], "stemmed_label": ["book index", "inform retriev method", "massiv master", "drop-down select of term", "index", "sql data type", "search", "primari index entri", "intern develop format", "autom", "index problem", "human factor", "html master index", "onlin inform", "navig"]}
{"doc": "Many criteria can be used to evaluate the performance of supervised learning . Different criteria are appropriate in different settings , and it is not always clear which criteria to use . A further complication is that learning methods that perform well on one criterion may not perform well on other criteria . For example , SVMs and boosting are designed to optimize accuracy , whereas neural nets typically optimize squared error or cross entropy . We conducted an empirical study using a variety of learning methods (SVMs , neural nets , k-nearest neighbor , bagged and boosted trees , and boosted stumps) to compare nine boolean classification performance metrics: Accuracy , Lift , F-Score , Area under the ROC Curve , Average Precision , Precision/Recall Break-Even Point , Squared Error , Cross Entropy , and Probability Calibration . Multidimensional scaling (MDS) shows that these metrics span a low dimensional manifold . The three metrics that are appropriate when predictions are interpreted as probabilities: squared error , cross entropy , and calibration , lay in one part of metric space far away from metrics that depend on the relative order of the predicted values: ROC area , average precision , break-even point , and lift . In between them fall two metrics that depend on comparing predictions to a threshold: accuracy and F-score . As expected , maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy , but perform poorly on probability metrics such as squared error . What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision . We introduce a new metric , SAR , that combines squared error , accuracy , and ROC area into one metric . MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics , suggesting that it is a good general purpose metric to use when more specific criteria are not known . INTRODUCTION In supervised learning , finding a model that could predict the true underlying probability for each test case would be optimal . We refer to such an ideal model as the One True Model . Any reasonable performance metric should be optimized (in expectation , at least) by the one true model , and no other model should yield performance better than it. Unfortunately , we usually do not know how to train models to predict the true underlying probabilities . The one true model is not easy to learn . Either the correct parametric model type for the domain is not known , or the training sample is too small for the model parameters to be esti-mated accurately , or there is noise in the data . Typically, all of these problems occur together to varying degrees. Even if magically the one true model were given to us , we would have difficulty selecting it from other less true models. We do not have performance metrics that will reliably assign best performance to the", "label": ["lift", "precision", "performance metric", "roc", "supervised learning", "supervised learning", "squared error", "svms", "pairwise", "recall", "algorithmns", "cross entropy", "ordering metric", "euclidean distance", "performance evaluation", "standard deviation", "backpropagation", "metrics"], "stemmed_label": ["lift", "precis", "perform metric", "roc", "supervis learn", "supervis learn", "squar error", "svm", "pairwis", "recal", "algorithmn", "cross entropi", "order metric", "euclidean distanc", "perform evalu", "standard deviat", "backpropag", "metric"]}
{"doc": "Database Security course is an important part of the InfoSec curriculum . In many institutions this is not taught as an independent course . Parts of the contents presented in this paper are usually incorporated in other courses such as Network Security . The importance of database security concepts stems from the fact that a compromise of data at rest could expose an organization to a greater security threat than otherwise . Database vulnerabilities exposed recently in several high profile incidents would be a good reason to dedicate a full course to this important topic . In this paper we present key topics such as technologies for database protection , access control , multilevel security , database vulnerabilities and defenses , privacy and legal issues , impact of policies and some well known secure database models . INTRODUCTION Information Security curriculum is receiving greater attention from many institutions , thanks to the standardization efforts by the Committee on National Security Systems (CNSS) . The CNSS members come from the National Security Agency, Department of Defense , and the Department of Homeland Security , among others . The CNSS standardization efforts are based on the Presidential Decision Directive 24 issued in 1998 for training professionals to protect the nation's critical infrastructure . To achieve this goal , CNSS has developed five major standards known as the National Security Telecommunications Information Systems Security Instruction (NSTISSI) . The NSTISSI standards are numbered 4011 , 4012, 4013 , 4014 and 4015 8 . Additional standards under this sequence are in the offing as well . The relevance of these standards is that they include a vast number of topics that cover the entire gamut of information assurance and database security topics are included in many of these standards . First , we will briefly outline the main content of each of these standards and then move onto the main content of this paper. The 4011 standard covers the information security foundation topics such as wired and wireless communications basics, operations security , transmission security , information security from a policy perspective , cryptography , key management , legal aspects of security , contingency planning and disaster recovery, risk management , trust , auditing , and monitoring . At present, coverage of topics mentioned in this standard is considered essential by CNSS in every InfoSec curriculum . The 4012 standard is primarily aimed at training Designated Approving Authority personnel . A quick look at the following topics would show the relationship of these standards vis--vis database security . The primary topics of this standard include: liabilities, legal issues , security policy , sensitive data access policy , threats, vulnerabilities , incident response , life cycle management, configuration management , and contingency management . The purpose of 4013 standard is to provide a minimum set of topics necessary for certifying Systems Administrators in Information Systems Security . Some of the topics in this category include: development and maintenance of security policies and procedures , education , training and awareness of such policies, development of countermeasures for known attacks", "label": ["inference channel", "access control", "buffer overflows", "cia", "privacy", "polyinstantiation", "database", "inference", "database", "encryption", "multilevel security", "authentication", "policy", "security"], "stemmed_label": ["infer channel", "access control", "buffer overflow", "cia", "privaci", "polyinstanti", "databas", "infer", "databas", "encrypt", "multilevel secur", "authent", "polici", "secur"]}
{"doc": "This paper addresses the problem of scatternet formation for single-hop Bluetooth based ad hoc networks , with minimal communication overhead . We adopt the well-known structure de Bruijn graph to form the backbone of Bluetooth scatternet , hereafter called dBBlue , such that every master node has at most seven slaves , every slave node is in at most two piconets , and no node assumes both master and slave roles . Our structure dBBlue also enjoys a nice routing property: the diameter of the graph is O(log n) and we can find a path with at most O(log n) hops for every pair of nodes without any routing table . Moreover , the congestion of every node is at most O(log n/n) , assuming that a unit of total traffic demand is equally distributed among all pair of nodes . We discuss in detail a vigorous method to locally update the structure dBBlue using at most O(log n) communications when a node joins or leaves the network . In most cases , the cost of updating the scatternet is actually O(1) since a node can join or leave without affecting the remaining scatternet . The number of nodes affected when a node joins or leaves the network is always bounded from above by a constant . To facilitate self-routing and easy updating , we design a scalable MAC assigning mechanism for piconet , which guarantees the packet delivery during scatternet updating . The dBBlue scatternet can be constructed incrementally when the nodes join the network one by one . Previously no method can guarantee all these properties although some methods can achieve some of the properties . INTRODUCTION Bluetooth 8 is a promising new wireless technology , which enables portable devices to form short-range wireless ad hoc networks based on a frequency hopping physical layer . Bluetooth ad-hoc networking presents some technical challenges , such as scheduling , network forming and routing . User mobility poses additional challenges for connection rerouting and QoS services . It has been widely predicted that Bluetooth will be the major technology for short range wireless networks and wireless personal area networks. This paper deals with the problem of building ad hoc networks using Bluetooth technology. According to the Bluetooth standard , when two Bluetooth devices come into each other's communication range , one of them assumes the role of master of the communication and the other becomes the slave . This simple one hop network is called a piconet, and may include more slaves . The network topology resulted by the connection of piconets is called a scatternet . There is no limit on the maximum number of slaves connected to one master , although the number of active slaves at one time cannot exceed . If a master node has more than slaves , some slaves must be parked . To communicate with a parked slave , a master has to unpark it , thus possibly parking another active slave instead . The standard also allows multiple roles for the same device . A", "label": ["scalable mac assignment", "scatternet formation", "low diameter", "ad hoc networks", "self-routing", "const updating cost", "de bruijn graph", "bluetooth", "network topology", "self-routing scatternet", "bluetooth networks", "bruijn graph", "equal traffic", "easy updating", "low diameter", "single-hop"], "stemmed_label": ["scalabl mac assign", "scatternet format", "low diamet", "ad hoc network", "self-rout", "const updat cost", "de bruijn graph", "bluetooth", "network topolog", "self-rout scatternet", "bluetooth network", "bruijn graph", "equal traffic", "easi updat", "low diamet", "single-hop"]}
{"doc": "This text has analyzed the development of E-commerce in some developed countries such as Canada , U.S.A. , Japan , etc and put forward several suggestions on how to set up the system of E-commerce in our country taking the national conditions of our country into account . INTRODUCTION Since the 1990s , the rapid development of e-commerce has brought extensive , enormous and far-reaching influence on the economy of the countries all over the world . E-commerce has already become the contemporary chief trend of economic and social development . As representatives of advanced productivity of new economic period , the level of its development has already become important signs of measuring the modernization level and comprehensive strength of countries and cities; it has become important means to make changeover in the economic system and reform the style of economic , promote the upgrading of the industrial structure , promote the modernized level of the city and strengthen international competitiveness . So , the governments all over the world have paid close attention to the development of E-commerce Statistics. Though the development of informationization in our country is very quick , it still has great disparity with the developed countries for relatively late start . Our country is still in the interim facing the double task of informationization and industrialization at present . So , in order to carry on an instant, accurate Statistics to the development level of E-commerce and set up perfect E-commerce Statistics system , we must understand , absorb and bring in the theories and methods of E-commerce Statistics from the main foreign countries to make E-commerce Statistics become effective guarantee in leading and promoting e-commerce in a healthy way , combining social source and promoting national power. DEVELOOPMENT STATES OF E-COMMERCE STATISTICS IN THE WORLD We have chosen some representative countries in the world and analyzed the development of E-commerce Statistics in these countries. 2.1 Definitions of e-commerce in main Developed countries. The definition of e-commerce is the standard of carrying on E-commerce Statistics , but there are various kinds of definition of E-commerce in the world because of visual angles are different . So , it is necessary for each country to make a distinct, standard , practical , wide meaningful and measurable definition which should be suitable for each field and can be amenable to time. 2.1.1 Definition of e-commerce in OECD (Organization for Economic Cooperation and Development) There are broadly-defined e-commerce and the narrowly-defined e-commerce . The broadly-defined e-commerce means the activity of electronic transaction on item and service , no matter the transaction occurred between enterprise , family , government and other public or individual organizations . It uses network as a intermediary . Goods and service should be ordered on the network but the payment and goods service don't need to carry on the net; the narrowly-defined e-commerce is only referred to trade activity carrying on through Internet. It is worth pointing out that the source of OECD definition of e-commerce is the Canada official Statistical department. 2.1.2 Definition of e-commerce", "label": ["development stage", "definition of e-commerce", "survey", "authority", "statistics", "statistical methods", "measurement", "implications", "e-commerce statistics", "statistical survey", "china", "e-commerce"], "stemmed_label": ["develop stage", "definit of e-commerc", "survey", "author", "statist", "statist method", "measur", "implic", "e-commerc statist", "statist survey", "china", "e-commerc"]}
{"doc": "Many authors have recognised the importance of structure in shaping information system (IS) design and use . Structuration theory has been used in IS research and design to assist with the identification and understanding of the structures in which the IS is situated . From a critical theoretical perspective , focusing on the Habermas' theory of communicative action , a community based child health information system was designed and implemented in a municipality in rural South Africa . The structures which shaped and influenced the design of this IS (the restructured health services and social tradition) are explored and discussed . From this case study the implications of using IS design as a developmental tool are raised: namely the development of a shared understanding , the participation of key players and the agreement on joint action . INTRODUCTION Many authors Walsham and Sahay 1996; Walsham and Han 1991; Jones 1997; Rose 1999; Orlikowski 1992; Orlikowski and Baroudi 1991; Orlikowski and Robey 1991 have recognised the importance of structure in shaping information system (IS) design and use . Structuration theory has been used in IS research and design to assist with the identification of the structures in which they are situated . Using this meta-analysis tool , information systems have been used to redefine and/or reinforce some of these structures . The IS design process is particularly important , not just in shaping the structures , but also in terms of understanding what structures exist and how they were formed. Critical approaches to IS examine those structures with the perspective of questioning and changing some of them . Critical social researchers seek to emancipate people by finding alternatives to existing social conditions as well as challenging taken-for-granted conditions . In particular , Habermas 1987 examines communication and how through striving for an ideal speech situation these structures can be challenged . In the process of IS design communication is especially important , as is who participates , and how. In this paper the author explores the existing structures which have contributed to the accessibility , or as the case may be inaccessibility , of the health services in the Okhahlamba municipality , KwaZulu-Natal , South Africa. Through the design of the community-based child health information system these structures were explored and addressed throughout the design process . Communication and participation were integral to the process , as well as the recognition of the importance of the context in which the system is designed. The rest of this paper is structured in the following manner . The following section looks at what is meant by structure , the process of structuration and its application to IS design . The third section looks at critical social theory in IS design , in particular Habermas' notion of communicative action . The fourth section outlines the existing structures in a community in KwaZulu-Natal that were important in shaping the IS design process. The fifth section explores how the process of IS design acknowledged and challenged these structures and the last section discusses the implications for IS design", "label": ["communicative action", "critical social theory", "moral codes or norms", "community information systems", "information system design", "structuration theory", "interpretative schemes", "critical social theory in is design", "conducive environment", "community monitoring system", "marginalisation", "health information systems", "duality of structure", "structuration theory", "the ideal speech situation"], "stemmed_label": ["commun action", "critic social theori", "moral code or norm", "commun inform system", "inform system design", "structur theori", "interpret scheme", "critic social theori in is design", "conduc environ", "commun monitor system", "marginalis", "health inform system", "dualiti of structur", "structur theori", "the ideal speech situat"]}
{"doc": "When failures occur in Internet overlay connections today , it is difficult for users to determine the root cause of failure . An overlay connection may require TCP connections between a series of overlay nodes to succeed , but accurately determining which of these connections has failed is difficult for users without access to the internal workings of the overlay . Diagnosis using active probing is costly and may be inaccurate if probe packets are filtered or blocked . To address this problem , we develop a passive diagnosis approach that infers the most likely cause of failure using a Bayesian network modeling the conditional probability of TCP failures given the IP addresses of the hosts along the overlay path . We collect TCP failure data for 28.3 million TCP connections using data from the new Planetseer overlay monitoring system and train a Bayesian network for the diagnosis of overlay connection failures . We evaluate the accuracy of diagnosis using this Bayesian network on a set of overlay connections generated from observations of CoDeeN traffic patterns and find that our approach can accurately diagnose failures . INTRODUCTION When failures occur in Internet overlay connections today , it is difficult for users to determine the root cause of failure . The proliferation of TCP overlays such as content distribution networks and HTTP proxies means that frequently network communication requires a series of TCP connections between overlay nodes to succeed . For example , an HTTP request using the CoDeeN 9 content distribution network first requires a TCP connection to a CoDeeN node and then a connection from a CoDeeN node to a server or another CoDeeN node . A failure in any one of the TCP connections along the overlay path causes the user's HTTP request to fail . If the user knows which TCP connection failed , then they can take appropriate action to repair or circumvent the failure . For instance, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. SIGCOMM'06 Workshops September 11-15 , 2006 , Pisa , Italy. Copyright 2006 ACM 1-59593-417-0/06/0009 ... $ 5.00. if they know that the connection from the proxy to the server failed, then they complain to the web server administrator . On the other hand , if the user/proxy connection fails , perhaps they can try connecting to the proxy using a different ISP . If multiple overlay paths exist between the source and destination , nodes and applications may also use this type of diagnostic information to automatically recover or route around failures 1 . Unfortunately , accurately determining which TCP connection in an overlay connection has failed is difficult for end users , who typically", "label": ["fault diagnosis", "passive diagnosis", "nat", "bayesian networks", "planetseer overlay monitoring system", "active probing for diagnosis", "inter-as tcp failure probabilities", "tcp overlay connections", "bayesian networks modelling", "codeen traffic patterns", "tcp overlay path diagnosis", "planetseer", "clustering", "network address translation"], "stemmed_label": ["fault diagnosi", "passiv diagnosi", "nat", "bayesian network", "planets overlay monitor system", "activ probe for diagnosi", "inter-a tcp failur probabl", "tcp overlay connect", "bayesian network model", "codeen traffic pattern", "tcp overlay path diagnosi", "planets", "cluster", "network address translat"]}
{"doc": "Digital Asset Management (DAM) , the management of digital content so that it can be cataloged , searched and re-purposed , is extremely challenging for organizations that rely on image handling and expect to gain business value from these assets . Metadata plays a crucial role in their management , and XML , with its inherent support for structural representation , is an ideal technology for this . This paper analyzes the capabilities of a native XML database solution via the development of a \"proof of concept\" and describes implementation requirements , strategy , and advantages and disadvantages of this solution . INTRODUCTION Digital asset creation and management evolved in the late 1990s. Companies have created massive digital assets in the form of images , video and audio files , streaming media , Power Point templates , web pages , and PDF files containing engineering specs, legal documents , internal memos and more . The World Wide Web has drastically increased the need for digital information and its exchange . Ille 8 identifies a digital asset as a strategic asset like any other form of working capital , and states that its efficient management is being increasingly recognized as a competitive lever for companies all over the world. Development of the model for storing any form of digital object into a structured format requires a deft combination of asset analysis , strategic thinking , business planning and appropriate technology . Companies can achieve early strategic advantage by implementing management systems for digital assets that can be repurposed or customized , providing process efficiencies in collaborative work . Digital Asset Management (DAM) can deliver competitive advantage for advertising agencies , technical or engineering documentation departments , designers , producers and others by reducing time spent locating creative assets. Enterprises often require reusing or sharing their digital assets . It is indispensable to store content in an organized way to reuse or process it for future needs . Global enterprises are facing the daunting challenge of figuring out how best to address the growing complexity of creating digital assets and managing the flow of assets through a single infrastructure 11 . Exacerbating the challenge is the fact that companies are creating a massive volume of digital assets but are rarely examining their organized storage and retrieval methods. SIGNIFICANCE OF THE PROBLEM DAM systems are still relatively new , but organizations have started realizing the importance and need for digital asset management . The Gartner Group affirms that only a limited number of technically advanced commercial content providers use DAM systems today to digitally construct , store and distribute rich media content in single medium forms 7 . The systems also have limited corporate use in advertising firms and marketing departments . Gartner predicts that by 2005 more than 25% of all the enterprises with commercial internet operations will use DAM systems . By 2010 , more than 45% of all the enterprises with commercial internet operations will use DAM systems. Recently reported cases provide evidence that companies have started investing in technology for DAM", "label": ["keyword search", "metadata", "multimedia", "digital asset management", "semi structured data", "database", "digital images", "digital asset management", "xml database", "storage and retrieval", "native xml", "dam", "heterogenous data", "proof of concept"], "stemmed_label": ["keyword search", "metadata", "multimedia", "digit asset manag", "semi structur data", "databas", "digit imag", "digit asset manag", "xml databas", "storag and retriev", "nativ xml", "dam", "heterogen data", "proof of concept"]}
{"doc": "mechanisms and algorithms necessary to set up and maintain them . The operation of a scatternet requires some Bluetooth units to be inter-piconet units (gateways) , which need to time-division multiplex their presence among their piconets . This requires a scatternet-scheduling algorithm that can schedule the presence of these units in an efficient manner . In this paper , we propose a distributed scatternet-scheduling scheme that is implemented using the HOLD mode of Bluetooth and adapts to non-uniform and changing traffic . Another attribute of the scheme is that it results in fair allocation of bandwidth to each Bluetooth unit . This scheme provides an integrated solution for both intra-and inter-piconet scheduling , i.e. , for polling of slaves and scheduling of gateways . Introduction The Bluetooth 10 technology was developed as a replacement of cables between electronic devices and this is perhaps its most obvious use . But , it is the ability of Bluetooth devices to form small networks called piconets that opens up a whole new arena for applications where information may be exchanged seamlessly among the devices in the piconet . Typ-ically , such a network , referred to as a PAN (Personal Area Network) , consists of a mobile phone , laptop , palmtop , headset , and other electronic devices that a person carries around in his every day life . The PAN may , from time to time , also include devices that are not carried along with the user , e.g., an access point for Internet access or sensors located in a room . Moreover , devices from other PANs can also be interconnected to enable sharing of information. The networking capabilities of Bluetooth can be further enhanced by interconnecting piconets to form scatternets. This requires that some units be present in more than one piconet . These units , called gateways , need to time-division their presence among the piconets . An important issue with the gateways is that their presence in different piconets needs to be scheduled in an efficient manner . Moreover , since the gateway cannot receive information from more than one piconet at a time , there is a need to co-ordinate the presence of masters and gateways. Some previous work has looked at scheduling in a piconet 2,5 and also in a scatternet . In 4 , the authors define a Rendezvous-Point based architecture for scheduling in a scatternet , which results in the gateway spending a fixed fraction of its time in each piconet . Such a fixed time-division of the gateway may clearly be inefficient since traffic is dynamic. In 9 , the authors propose the Pseudo-Random Coordinated Scatternet Scheduling (PCSS) scheme in which Bluetooth nodes assign meeting points with their peers . The sequence of meeting points follows a pseudo-random process that leads to unique meeting points for different peers of a node . The intensity of these meeting points may be increased or decreased according to the traffic intensity . This work presents performance results for various cases . In 11 , a scatternet-scheduling", "label": ["scheduling scheme", "round robin", "distributed algorithm", "scheduling", "traffic adaptive", "scatternet presence fraction", "fairness", "traffic rate", "scatternets", "scatternet", "bluetooth", "rendezvous points", "scheduling algorithm", "information exchange", "heuristic", "gateway", "scheduling of gateways", "slaves", "non-uniform traffic", "changing traffic", "efficiency", "fair share", "virtual slave", "rendezvous point", "blueooth", "piconet presence fraction", "hold mode", "slave unit", "polling algorithm", "gateway slave traffic", "scatternets", "master unit", "allocation of bandwidth", "rendezvous point", "fairness", "piconet", "piconet", "slave", "traffic dependent scheduling", "time-division multiplex", "scatternet", "fair share", "bluetooth technology", "non-gateway slave traffic", "bandwidth utilization", "allocation of bandwidth", "gateway", "master", "round robin polling"], "stemmed_label": ["schedul scheme", "round robin", "distribut algorithm", "schedul", "traffic adapt", "scatternet presenc fraction", "fair", "traffic rate", "scatternet", "scatternet", "bluetooth", "rendezv point", "schedul algorithm", "inform exchang", "heurist", "gateway", "schedul of gateway", "slave", "non-uniform traffic", "chang traffic", "effici", "fair share", "virtual slave", "rendezv point", "blueooth", "piconet presenc fraction", "hold mode", "slave unit", "poll algorithm", "gateway slave traffic", "scatternet", "master unit", "alloc of bandwidth", "rendezv point", "fair", "piconet", "piconet", "slave", "traffic depend schedul", "time-divis multiplex", "scatternet", "fair share", "bluetooth technolog", "non-gateway slave traffic", "bandwidth util", "alloc of bandwidth", "gateway", "master", "round robin poll"]}
{"doc": "ABSTRACT Web Directories are repositories of Web pages organized in a hierarchy of topics and sub-topics . In this paper , we present DirectoryRank , a ranking framework that orders the pages within a given topic according to how informative they are about the topic . Our method works in three steps: first , it processes Web pages within a topic in order to extract structures that are called lexical chains , which are then used for measuring how informative a page is for a particular topic . Then , it measures the relative semantic similarity of the pages within a topic . Finally , the two metrics are combined for ranking all the pages within a topic before presenting them to the users . INTRODUCTION A Web Directory is a repository of Web pages that are organized in a topic hierarchy . Typically , Directory users locate the information sought simply by browsing through the topic hierarchy , identifying the relevant topics and finally examining the pages listed under the relevant topics . Given the current size and the high growth rate of the Web 10 , a comprehensive Web Directory may contain thousands of pages within a particular category . In such a case , it might be impossible for a user to look through all the relevant pages within a particular topic in order to identify the ones that best represent the current topic . Practically , it would be more time-efficient for a user to view the Web pages in order of importance for a particular topic , rather than go through a large list of pages. One way to alleviate this problem is to use a ranking function which will order the pages according to how \"informative\" they are of the topic that they belong to . Currently , the Open Directory Project 3 lists the pages within a category alphabetically , while the Google Directory 1 orders the pages within a category according to their PageRank 11 value on the Web . While these rankings can work well in some cases , they do not directly capture the closeness of the pages to the topic that they belong to. In this paper , we present DirectoryRank , a new ranking framework that we have developed in order to alleviate the problem of ranking the pages within a topic based on how \"informative\" these pages are to the topic . DirectoryRank is based on the intuition that the quality (or informativeness) of a Web page with respect to a particular topic is determined by the amount of information that the page communicates about the given topic , relative to the other pages that are categorized in the same topic . Our method takes as input a collection of Web pages that we would like to rank along with a Web Directory's topic hierarchy that we would like to use. At a high level , our method proceeds as follows: first , we identify the most important words inside every page and we link them together , creating", "label": ["topic hierachy", "semantic similarity", "ranking metric", "scoring", "web directory", "ranking", "lexical chains", "directoryrank", "topic importance", "pagerank", "information retrieval", "web directory", "semantic similarities"], "stemmed_label": ["topic hierachi", "semant similar", "rank metric", "score", "web directori", "rank", "lexic chain", "directoryrank", "topic import", "pagerank", "inform retriev", "web directori", "semant similar"]}
{"doc": "In this paper we present a personalized web service discovery and ranking technique for discovering and ranking relevant data-intensive web services . Our first prototype called BASIL supports a personalized view of data-intensive web services through source-biased focus . BASIL provides service discovery and ranking through source-biased probing and source-biased relevance metrics . Concretely , the BASIL approach has three unique features: (1) It is able to determine in very few interactions whether a target service is relevant to the given source service by probing the target with very precise probes; (2) It can evaluate and rank the relevant services discovered based on a set of source-biased relevance metrics; and (3) It can identify interesting types of relationships for each source service with respect to other discovered services , which can be used as value-added metadata for each service . We also introduce a performance optimization technique called source-biased probing with focal terms to further improve the effectiveness of the basic source-biased service discovery algorithm . The paper concludes with a set of initial experiments showing the effectiveness of the BASIL system . INTRODUCTION Most web services today are web-enabled applications that can be accessed and invoked using a messaging system , typically relying on standards such as XML , WSDL , and SOAP 29 . Many companies have latched onto the web services mantra , including major software developers , business exchanges , eCom-merce sites , and search engines 15 , 9 , 2 , 1 , 7 . A large and growing portion of the web services today can be categorized as data-intensive web services. This research is partially supported by NSF CNS CCR , NSF ITR , DoE SciDAC , DARPA , CERCS Research Grant , IBM Faculty Award , IBM SUR grant , HP Equipment Grant , and LLNL LDRD. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. ICSOC'04 , November 1519 , 2004 , New York , New York , USA. Copyright 2004 ACM 1-58113-871-7/04/0011 ... $ 5.00. Data-intensive web services provide access to huge and growing data stores and support tools for searching , manipulating, and analyzing those data stores . For example , both Amazon 1 and Google 7 now provide XML- and SOAP-based web service interfaces to their underlying data repositories with support for advanced search operators over , collectively , billions of items . In the life sciences domain , many bioinformatics services are transitioning from human-in-the-loop web interfaces to the web services model 9 , providing direct access to unprecedented amounts of raw data and specialized research tools to provide high-level analysis and search over these data services. With the increasing visibility of web services", "label": ["focal terms", "biased discovery", "ranking", "data-intensive web services", "data-intensive services", "query-biased probing", "web service discovery", "source-biased probing"], "stemmed_label": ["focal term", "bias discoveri", "rank", "data-intens web servic", "data-intens servic", "query-bias probe", "web servic discoveri", "source-bias probe"]}
{"doc": "In visual information retrieval the careful choice of suitable proximity measures is a crucial success factor . The evaluation presented in this paper aims at showing that the distance measures suggested by the MPEG-7 group for the visual descriptors can be beaten by general-purpose measures . Eight visual MPEG-7 descriptors were selected and 38 distance measures implemented . Three media collections were created and assessed , performance indicators developed and more than 22500 tests performed . Additionally , a quantisation model was developed to be able to use predicate-based distance measures on continuous data as well . The evaluation shows that the distance measures recommended in the MPEG-7-standard are among the best but that other measures perform even better . INTRODUCTION The MPEG-7 standard defines among others a set of descriptors for visual media . Each descriptor consists of a feature extraction mechanism , a description (in binary and XML format) and guidelines that define how to apply the descriptor on different kinds of media (e.g . on temporal media) . The MPEG-7 descriptors have been carefully designed to meet partially complementary requirements of different application domains: archival , browsing, retrieval , etc . 9 . In the following , we will exclusively deal with the visual MPEG-7 descriptors in the context of media retrieval. The visual MPEG-7 descriptors fall in five groups: colour, texture , shape , motion and others (e.g . face description) and sum up to 16 basic descriptors . For retrieval applications , a rule for each descriptor is mandatory that defines how to measure the similarity of two descriptions . Common rules are distance functions , like the Euclidean distance and the Mahalanobis distance . Unfortunately , the MPEG-7 standard does not include distance measures in the normative part , because it was not designed to be (and should not exclusively understood to be) retrieval-specific . However , the MPEG-7 authors give recommendations , which distance measure to use on a particular descriptor . These recommendations are based on accurate knowledge of the descriptors' behaviour and the description structures. In the present study a large number of successful distance measures from different areas (statistics , psychology , medicine, social and economic sciences , etc.) were implemented and applied on MPEG-7 data vectors to verify whether or not the recommended MPEG-7 distance measures are really the best for any reasonable class of media objects . From the MPEG-7 tests and the recommendations it does not become clear , how many and which distance measures have been tested on the visual descriptors and the MPEG-7 test datasets . The hypothesis is that analytically derived distance measures may be good in general but only a quantitative analysis is capable to identify the best distance measure for a specific feature extraction method. The paper is organised as follows . Section 2 gives a minimum of background information on the MPEG-7 descriptors and distance measurement in visual information retrieval (VIR , see 3 , 16 ). Section 3 gives an overview over the implemented distance measures . Section 4 describes the test", "label": ["similarity perception", "mpeg-7 descriptors", "distance measurement", "content-based image retrieval", "mpeg-7", "distance measure", "quantisation", "content-based video retrieval", "similarity measurement", "visual information retrieval", "visual information retrieval", "human similarity perception"], "stemmed_label": ["similar percept", "mpeg-7 descriptor", "distanc measur", "content-bas imag retriev", "mpeg-7", "distanc measur", "quantis", "content-bas video retriev", "similar measur", "visual inform retriev", "visual inform retriev", "human similar percept"]}
{"doc": "In the not too distant future Intelligent Creatures (robots , smart devices , smart vehicles , smart buildings , etc) will share the everyday living environment of human beings . It is important then to analyze the attitudes humans are to adopt for interaction with morphologically different devices , based on their appearance and behavior . In particular , these devices will become multi-modal interfaces , with computers or networks of computers , for a large and complex universe of applications . Our results show that children are quickly attached to the word `dog' reflecting a conceptualization that robots that look like dogs (in particular SONY Aibo) are closer to living dogs than they are to other devices . By contrast , adults perceive Aibo as having stronger similarities to machines than to dogs (reflected by definitions of robot) . Illustration of the characteristics structured in the definition of robot are insufficient to convince children Aibo is closer to a machine than to a dog . Introduction The play R . U . R . (Rossum's Universal Robots) , written by the Czech author Karel Capek , was produced in London in 1923 . The term robot entered the English language (in Czech the word `robota' means `heavy labor') . The robot concept remained science fiction until 1961 when Unimation Inc . installed the world's first industrial robot in the US . Unimation Inc . made Australia's first robot , installed in 1974. The emergence of legged autonomous robots and their commercial release (as in Honda's Asimo and Sony's Aibo) contribute to support the hypothesis that mobile robots will soon become common in our everyday environments . The commercial release of the Personal Computer (PC) occurred just a generation ago, yet now it is a common household item . This forecast has prompted some studies into the acceptabil-Copyright c 2004 , Australian Computer Society , Inc . This paper appeared at 5th Australasian User Interface Conference (AUIC2004) , Dunedin . Conferences in Research and Practice in Information Technology , Vol . 28 . A . Cockburn , Ed . Reproduction for academic , not-for profit purposes permitted provided this text is included. This work was supported by a Griffith University Research Grant as part of the project \"Robots to guide the blind the application of physical collaborative autonomous intelligent agents\". ity and attitudes these artifacts generate among human beings (Fong , Nourbakhsh & Dautenhahn 2003). For example , Kahn et al have recently embarked on a series of studies with children (Kahn Jr. , Friedman , Freier & Severson 2003) and adults (Kahn Jr., Friedman & Hagman 2002) investigating matters such as humans attributing social and ethical stance to robots like Sony's Aibo . Earlier Bumby and Dautenhahn (Bumby & Dautenhahn 1999) explored reactions of children as they interacted with a robot. Reports have recently appeared in the media of cases where humans also build emotional attachments to robots that do not look like animals , similar to those they have for home pets . An example is the robotic vacuum cleaners", "label": ["intelligent creatures", "human attitudes", "language", "essence", "agency", "robots", "perceived attitude", "behavioral science", "zoo-morphological autonomous mobile robots", "robot attributes", "multi-modal interfaces", "interaction", "feelings", "hci"], "stemmed_label": ["intellig creatur", "human attitud", "languag", "essenc", "agenc", "robot", "perceiv attitud", "behavior scienc", "zoo-morpholog autonom mobil robot", "robot attribut", "multi-mod interfac", "interact", "feel", "hci"]}
{"doc": "An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites . These pages are often referred to as the Hidden Web or the Deep Web . Since there are no static links to the Hidden Web pages , search engines cannot discover and index such pages and thus do not return them in the results . However , according to recent studies , the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users . In this paper , we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web . Since the only \"entry point\" to a Hidden Web site is a query interface , the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site . Here , we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically . Our policies proceed iteratively , issuing a different query in every iteration . We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising . For instance , in one experiment , one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents ) after issuing fewer than 100 queries . INTRODUCTION Recent studies show that a significant fraction of Web content cannot be reached by following links 7 , 12 . In particular , a large part of the Web is \"hidden\" behind search forms and is reachable only when users type in a set of keywords , or queries , to the forms. These pages are often referred to as the Hidden Web 17 or the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. JCDL'05 , June 711 , 2005 , Denver , Colorado , USA. Copyright 2005 ACM 1-58113-876-8/05/0006 ... $ 5.00. Deep Web 7 , because search engines typically cannot index the pages and do not return them in their results (thus , the pages are essentially \"hidden\" from a typical Web user). According to many studies , the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface 7 . In 12 , Chang et al. estimate that well over 100,000 Hidden-Web sites currently exist on", "label": ["crawler", "deep web", "hidden web", "hidden web crawling", "query selection", "efficiency", "deep web crawler", "coverage", "keyword selection", "adaptive algorithm", "potential bias", "adaptive algorithmn", "accurate language model", "keyword query", "keyword queries"], "stemmed_label": ["crawler", "deep web", "hidden web", "hidden web crawl", "queri select", "effici", "deep web crawler", "coverag", "keyword select", "adapt algorithm", "potenti bia", "adapt algorithmn", "accur languag model", "keyword queri", "keyword queri"]}
{"doc": "Domain-specific languages hold the potential of automating the software development process . Nevertheless , the adop-tion of a domain-specific language is hindered by the difficulty of transitioning to different language syntax and employing a separate translator in the software build process . We present a methodology that simplifies the development and deployment of small language extensions , in the context of Java . The main language design principle is that of language extension through unobtrusive annotations . The main language implementation idea is to express the language as a generator of customized AspectJ aspects , using our Meta-AspectJ tool . The advantages of the approach are twofold . First , the tool integrates into an existing software application much as a regular API or library , instead of as a language extension . This means that the programmer can remove the language extension at any point and choose to implement the required functionality by hand without needing to rewrite the client code . Second , a mature language implementation is easy to achieve with little effort since AspectJ takes care of the low-level issues of interfacing with the base Java language INTRODUCTION AND MOTIVATION The idea of extensible languages has fascinated programmers for many decades , as evident by the extensibility fea This material is based upon work supported by the National Science Foundation under Grants No . CCR-0220248 and CCR-0238289. Copyright is held by the author/owner. ICSE'06 , May 2028 , 2006 , Shanghai , China. ACM 1-59593-085-X/06/0005. tures in languages as old as Lisp. From a Software Engineering standpoint , the main advantages of expressing a concept as a language feature , as opposed to a library API, are in terms of conciseness , safety , and performance. A language feature allows expressing the programmer's intent much more concisely--in contrast , libraries are limited to a function- or method-call syntax . A language feature also allow better static error checking--a library can only check the static types of arguments of a function call against the declared types of the formals . Finally , a language feature can take advantage of context information and employ an optimized implementation , while a library routine cannot be customized according to its uses. Despite these advantages , there are excellent reasons why full language extensibility is undesirable . Changing the syntax and semantics of a programming language is confusing and can lead to incomprehensible code . Furthermore , programming languages are complex entities , designed to provide a small number of features but allow them to be combined as generally as possible. A new feature can either increase the complexity of the language implementation significantly (because of interactions with all existing features), or will need to be limited in its interactions , which is a bad language design principle that leads to single-use features and design bloat. In past work 3 , we have advocated expressing small language extensions purely through unobtrusive annotations . Indeed , the introduction of user-defined annotations in mainstream programming languages , such as C# and Java , has allowed", "label": ["language extensions", "annotation", "domain-specific language", "language extension", "meta-aspectj", "java", "simplicity", "domain-specific languages"], "stemmed_label": ["languag extens", "annot", "domain-specif languag", "languag extens", "meta-aspectj", "java", "simplic", "domain-specif languag"]}
{"doc": "We review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a general-purpose commercial web search service . Previously , query logs have been studied from a single , cumulative view . In contrast , our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day . We examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre-categorized by human editors . This represents 13% of the query traffic . We show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories . This analysis provides valuable insight for improving retrieval effectiveness and efficiency . It is also relevant to the development of enhanced query disambiguation , routing , and caching algorithms . INTRODUCTION Understanding how queries change over time is critical to developing effective , efficient search services . We are unaware of any log analysis that studies differences in the query stream over the hours in a day; much less how those differences are manifested within topical categories . We focus on Circadian changes in popularity and uniqueness of topical categories. Emphasis on changing query stream characteristics over this longitudinal (time) aspect of query logs distinguishes this work from prior static log analysis , surveyed A href=\"76.html#7\" in 7 . We began with the hypothesis that there are very different characteristics during peak hours and off-peak hours during a day. After reviewing a week's worth of data hundreds of millions of queries - we have found , not surprisingly , that: The number of queries issued is substantially lower during non-peak hours than peak hours. However , we knew little about how often queries are repeated from one hour of the day to the next . After examining the behavior of millions of queries from one hour of the day to the next we have found the less obvious result: The average number of query repetitions in an hour does not change significantly on an hourly basis throughout the day. Most queries appear no more than several times per hour. These queries consistently account for a large portion of total query volume throughout the course of the day. The queries received during peak hours are more similar to each other than their non-peak hour counterparts. We also analyze the queries representing different topics using a topical categorization of our query stream . These cover approximately 13% of the total query volume . We hypothesized that traffic behavior for some categories would change over time and that others would remain stable . For 16 different categories, we examined their traffic characteristics: Some topical categories vary substantially more in popularity than others as we move through an average day. Some topics are more popular during particular times of the day , while others have a more constant level of interest over time. The query sets for different categories have differing similarity over time . The", "label": ["query traffic", "query stream", "frequency distribution", "topical categories", "log analysis", "query log", "query log analysis", "web search"], "stemmed_label": ["queri traffic", "queri stream", "frequenc distribut", "topic categori", "log analysi", "queri log", "queri log analysi", "web search"]}
{"doc": "Text categorization is an important research area and has been receiving much attention due to the growth of the on-line information and of Internet . Automated text categorization is generally cast as a multi-class classification problem . Much of previous work focused on binary document classification problems . Support vector machines (SVMs) excel in binary classification , but the elegant theory behind large-margin hyperplane cannot be easily extended to multi-class text classification . In addition , the training time and scaling are also important concerns . On the other hand , other techniques naturally extensible to handle multi-class classification are generally not as accurate as SVM . This paper presents a simple and efficient solution to multi-class text categorization . Classification problems are first formulated as optimization via discriminant analysis . Text categorization is then cast as the problem of finding coordinate transformations that reflects the inherent similarity from the data . While most of the previous approaches decompose a multiclass classification problem into multiple independent binary classification tasks , the proposed approach enables direct multi-class classification . By using Generalized Singular Value Decomposition (GSVD) , a coordinate transformation that reflects the inherent class structure indicated by the generalized singular values is identified . Extensive experiments demonstrate the efficiency and effectiveness of the proposed approach . INTRODUCTION With the ever-increasing growth of the on-line information and the permeation of Internet into daily life , methods that assist users in organizing large volumes of documents are in huge demand. In particular , automatic text categorization has been extensively studied recently . This categorization problem is usually viewed as supervised learning , where the gaol is to assign predefined category labels to unlabeled documents based on the likelihood in-ferred from the training set of labeled documents . Numerous approaches have been applied , including Bayesian probabilistic approaches 20 , 31 , nearest neighbor 22 , 19 , neural networks 33 , decision trees 2 , inductive rule learning 4 , 9 , support vector machines 18 , 14 , Maximum Entropy 26 , boosting 28 , and linear discriminate projection 3 (see 34 for comparative studies of text categorization methods). Although document collections are likely to contain many different categories , most of the previous work was focused on binary document classification . One of the most effective binary classification techniques is the support vector machines (SVMs) 32 . It has been demonstrated that the method performs superbly in binary discriminative text classification 18 , 34 . SVMs are accurate and robust , and can quickly adapt to test instances . However , the elegant theory behind the use of large-margin hyperplanes cannot be easily extended to multi-class text categorization problems . A number of techniques for reducing multi-class problems to binary problems have been proposed , including one-versus-the-rest method, pairwise comparison 16 and error-correcting output coding 8 , 1 . In these approaches , the original problems are decomposed into a collection of binary problems , where the assertions of the binary classifiers are integrated to produce the final output . In practice,", "label": ["multi-class classification", "text categorization", "gsvd", "discriminant analysis", "multi-class text categorization", "svms", "gda", "discriminant analysis"], "stemmed_label": ["multi-class classif", "text categor", "gsvd", "discrimin analysi", "multi-class text categor", "svm", "gda", "discrimin analysi"]}
{"doc": "Search engines need to evaluate queries extremely fast , a challenging task given the vast quantities of data being indexed . A significant proportion of the queries posed to search engines involve phrases . In this paper we consider how phrase queries can be efficiently supported with low disk overheads . Previous research has shown that phrase queries can be rapidly evaluated using nextword indexes , but these indexes are twice as large as conventional inverted files . We propose a combination of nextword indexes with inverted files as a solution to this problem . Our experiments show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase queries in half the time required to evaluate such queries with an inverted file alone , and the space overhead is only 10% of the size of the inverted file . Further time savings are available with only slight increases in disk requirements . Categories and Subject Descriptors INTRODUCTION Search engines are used to find data in response to ad hoc queries . On the Web , most queries consist of simple lists of words . However , a significant fraction of the queries include phrases , where the user has indicated that some of the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. SIGIR'02 , August 11-15 , 2002 , Tampere , Finland. Copyright 2002 ACM 1-58113-561-0/02/0008 ... $ 5.00. query terms must be adjacent , typically by enclosing them in quotation marks . Phrases have the advantage of being unambiguous concept markers and are therefore viewed as a valuable retrieval technique 5 , 6 , 7 , 10 In this paper , we explore new techniques for efficient evaluation of phrase queries. A standard way to evaluate phrase queries is to use an inverted index , in which for each index term there is a list of postings , and each posting includes a document identifier, an in-document frequency , and a list of offsets . These offsets are the ordinal word positions at which the term occurs in the document . Given such a word-level inverted index and a phrase query , it is straightforward to combine the postings lists for the query terms to identify matching documents. This does not mean , however , that the process is fast . Even with an efficient representation of postings 16 , the list for a common term can require several megabytes for each gigabyte of indexed text . Worse , heuristics such as frequency-ordering 13 or impact-ordering 1 are not of value , as the frequency of a word in a document does not determine its frequency of", "label": ["common words", "evaluation efficiency", "stopping", "indexing", "nextword index", "index representation", "phrase query evaluation", "query evaluation", "phrase query", "inverted index"], "stemmed_label": ["common word", "evalu effici", "stop", "index", "nextword index", "index represent", "phrase queri evalu", "queri evalu", "phrase queri", "invert index"]}
{"doc": "We propose an indexing technique for the fast retrieval of objects in 2D images basedon similarity between their boundary shapes . Our technique is robust in the presence of noise andsupports several important notions of similarity including optimal matches irrespective of variations in orientation and/or position . Our method can also handle size-invariant matches using a normalization technique , although optimality is not guaranteedhere . We implementedour method and performed experiments on real (hand-written digits) data . Our experimental results showedthe superiority of our method comparedto search basedon sequential scanning , which is the only obvious competitor . The performance gain of our method increases with any increase in the number or the size of shapes . Introduction There is an increasing interest in storing andretrieving non-textual objects in databases . For example , this kind of data can be storedin the form of extenders in DB2 , DataBlades in In-formix , and cartridges in Oracle . Non-textual objects are frequently in the form of images or shapes . In cases where the key information for description or classification of an object can be found in its boundary , it is natural to store only the boundary anddo retrieval basedon that . Among the areas of applications for boundary shape matching are industrial inspection, object recognition in satellite images , character recognition, classification of chromosomes , andtarget recognition. For example , consider the following query: Query 1 Findall shapes similar to a given shape. A basic question here is how we judge whether two shapes (for example the two shown in Fig . 1) are similar . There is a large body of work in the area of pattern recognition and computer vision on extracting boundary features of a shape and doing shape matching based on those features . The boundary of an object can be described in terms of simple descriptors such as length , diameter , and curvature ( MM86 ) , chain Fig . 1. Two shape boundaries both representing character `9' codes ( BG80,Bri81 ) , Fourier descriptors ( PF77,ZR72 ) or moments ( BSA91 ) . Among these features , we use Fourier descriptors as our shape features . Theoretical andexperimen-tal evidence in favor of Fourier descriptors can be found in the literature PF77,KSP95 . Similar shapes often have differences in size and orientation . For example , consider the two shapes shown in Fig . 1 . The Euclidean distance between their Fourier descriptors is 22.88. If we rotate the shape on the right by 30 in the clockwise (cw) direction , the Euclidean distance between their Fourier descriptors drops to zero . A simple approach to remove differences due to shifting , scaling , and rotation is to normalize Fourier descriptors before storing them in a database . However , there are still two problems with normalization . First, normalization is not guaranteedto minimize the distance between two arbitrary shapes . Second , normalization is not always desirable; for example , the shapes `9' and`6' shouldnot be treatedas similar if we are doing character recognition", "label": ["fingerprint", "shape retrieval similarity retrieval fourier descriptors", "non textual objects", "efficiency", "database", "handwriting recognition", "fourier descriptors", "image databases", "search", "queries", "shape classification", "indexing techniques", "similarity queries"], "stemmed_label": ["fingerprint", "shape retriev similar retriev fourier descriptor", "non textual object", "effici", "databas", "handwrit recognit", "fourier descriptor", "imag databas", "search", "queri", "shape classif", "index techniqu", "similar queri"]}
{"doc": "In this paper we present a simple but general 3D slicer for voxelizing polygonal model . Instead of voxelizing a model by projecting and rasterizing triangles with clipping planes , the distance field is used for more accurate and stable voxelization . Distance transform is used with triangles on voxels of each slice . A voxel is marked with opacity only when the shortest distance between it and triangles is judged as intersection . With advanced programmable graphics hardware assistance , surface and solid voxelization are feasible and more efficient than on a CPU . Introduction Object representation is a broad topic in research . In computer graphics , polygons play a dominant role in 3D graphics because they approximate arbitrary surfaces by meshes . In games and animations , surface representation is the main technique used in rendering and visualization . However , volumetric representation , an alternative method to traditional geometric representation , is well known since the 1980s . It provides a simple and uniform description to measure and model a volumetric objects and establish the research field of volumetric graphics . Voxelization is a process of constructing a volumetric representation of an object . Voxelizing a polygonal object is not only a shift of the representation , it gives an opportunity to manipulate mesh object with volumetric operations such as morphological operation and solid modeling . Many applications , for example , CSG modeling , virtual medicine , haptic rendering , visualization of geometric model , collision detection , 3D spatial analysis , and model fixing , work on volumetric representation or use it as the inter-medium. In this paper , we calculate the accurate distance field on GPU to compute coverage of each voxel in voxelization for polygonal models . Our method works for arbitrary triangulated models without any preprocessing for models , except organizing meshes slab by slab in order to prune the unnecessary computation while voxelizing complex models . By using the power of GPU , Hausdorff distance is guaranteed between each sampled voxel and the polygonal model . Surface voxelization with distance field on a GPU works well and runs faster than on a PC with a CPU . Our method is a reliable and accurate solution for the polygonal model under a given distribution of sampling voxels and a specific opacity criterion . Besides , error tolerance in voxelization is easy to manipulate by adjusting the threshold of opacity criterion for voxelization , which also dominates the smoothness of features and the thickness of surface voxelization. e-mail:wktai@mail.ndhu.edu.tw The rest of paper is organized as follows . Some related works are surveyed in the section 2 . In section 3 , we present the computation of Hausdorff distance and our framework . The experimental results are illustrated in section 4 . Finally , we conclude the proposed approach and point out some future works. Related Works Volume construction approaches are often referred to as scan-conversion or voxelization methods . Researchers mainly focused on modeling aspects such as robustness and accuracy . Wang and Kaufman", "label": ["graphics hardware", "hausdorff distance", "voxelization", "distance field", "voxelization", "local distance field", "object representation", "rasterization", "polygonal object", "gpu-based 3d slicer approach", "gpu", "3d slicer", "slice-based approach", "rendering", "rendering", "adaptive dense voxelization", "volumetric representation", "pixel shader", "opacity", "surface voxelization", "polygonal model", "surface representation", "rendering cost", "gpu computation", "hausforff distance", "object representation", "polygonal objects", "volumetric representation", "triangles", "rendering pipeline", "distance transform", "volume construction", "modeling", "computational geometry", "geometric representation", "hausdorff distance", "distance field", "computer graphics", "polygonal model", "3d modelling", "infinitude"], "stemmed_label": ["graphic hardwar", "hausdorff distanc", "voxel", "distanc field", "voxel", "local distanc field", "object represent", "raster", "polygon object", "gpu-bas 3d slicer approach", "gpu", "3d slicer", "slice-bas approach", "render", "render", "adapt dens voxel", "volumetr represent", "pixel shader", "opac", "surfac voxel", "polygon model", "surfac represent", "render cost", "gpu comput", "hausforff distanc", "object represent", "polygon object", "volumetr represent", "triangl", "render pipelin", "distanc transform", "volum construct", "model", "comput geometri", "geometr represent", "hausdorff distanc", "distanc field", "comput graphic", "polygon model", "3d model", "infinitud"]}
{"doc": "This paper presents the notion of Semantic Associations as complex relationships between resource entities . These relationships capture both a connectivity of entities as well as similarity of entities based on a specific notion of similarity called -isomorphism . It formalizes these notions for the RDF data model , by introducing a notion of a Property Sequence as a type . In the context of a graph model such as that for RDF , Semantic Associations amount to specific certain graph signatures . Specifically , they refer to sequences (i.e . directed paths) here called Property Sequences , between entities , networks of Property Sequences (i.e . undirected paths) , or subgraphs of ρ-isomorphic Property Sequences . The ability to query about the existence of such relationships is fundamental to tasks in analytical domains such as national security and business intelligence , where tasks often focus on finding complex yet meaningful and obscured relationships between entities . However , support for such queries is lacking in contemporary query systems , including those for RDF . This paper discusses how querying for Semantic Associations might be enabled on the Semantic Web , through the use of an operator ρ . It also discusses two approaches for processing ρ-queries on available persistent RDF stores and memory resident RDF data graphs , thereby building on current RDF query languages . INTRODUCTION The Semantic Web 13 proposes to explicate the meaning of Web resources by annotating them with metadata that have been described in an ontology . This will enable machines to \"understand\" the meaning of resources on the Web , thereby unleashing the potential for software agents to perform tasks on behalf of humans . Consequently , significant effort in the Semantic Web research community is devoted to the development of machine processible ontology representation formalisms. Some success has been realized in this area in the form of W3C standards such as the eXtensible Markup Language (XML) 16 which is a standard for data representation and exchange on the Web , and the Resource Description Framework (RDF) 42 , along with its companion specification , RDF Schema (RDFS) 17 , which together provide a uniform format for the description and exchange of the semantics of web content . Other noteworthy efforts include OWL 25 , Topic Maps 53 , DAML+OIL 31 . There are also related efforts in both the academic and commercial communities , which are making available tools for semi-automatic 30 and automatic 49 29 semantic (ontology-driven and/or domain-specific) metadata extraction and annotation. With the progress towards realizing the Semantic Web , the development of semantic query capabilities has become a pertinent research problem . Semantic querying techniques will exploit the semantics of web content to provide superior results than present-day techniques which rely mostly on lexical (e.g. search engines) and structural properties (e.g . XQuery 24 ) of a document . There are now a number of proposals for querying RDF data including RQL 40 , SquishQL 45 , TRIPLE 49 , RDQL 48 . These languages offer most of the essential", "label": ["ai", "analysis", "isomorphism", "complex data relationships", "rdf", "rooted directed path", "semantic associations", "automation", "graph traversals", "semantic association", "semantic web querying", "relationship", "semantic web", "query processing", "property sequence"], "stemmed_label": ["ai", "analysi", "isomorph", "complex data relationship", "rdf", "root direct path", "semant associ", "autom", "graph travers", "semant associ", "semant web queri", "relationship", "semant web", "queri process", "properti sequenc"]}
{"doc": "With the tremendous growth of system memories , memory-resident databases are increasingly becoming important in various domains . Newer memories provide a structured way of storing data in multiple chips , with each chip having a bank of memory modules . Current memory-resident databases are yet to take full advantage of the banked storage system , which offers a lot of room for performance and energy optimizations . In this paper , we identify the implications of a banked memory environment in supporting memory-resident databases , and propose hardware (memory-directed) and software (query-directed) schemes to reduce the energy consumption of queries executed on these databases . Our results show that high-level query-directed schemes (hosted in the query optimizer) better utilize the low-power modes in reducing the energy consumption than the respective hardware schemes (hosted in the memory controller) , due to their complete knowledge of query access patterns . We extend this further and propose a query restructuring scheme and a multi-query optimization . Queries are restructured and regrouped based on their table access patterns to maximize the likelihood that data accesses are clustered . This helps increase the inter-access idle times of memory modules , which in turn enables a more effective control of their energy behavior . This heuristic is eventually integrated with our hardware optimizations to achieve maximum savings . Our experimental results show that the memory energy reduces by 90% if query restructuring method is applied along with basic energy optimizations over the unoptimized version . The system-wide performance impact of each scheme is also studied simultaneously . INTRODUCTION Memory-resident databases (also called in-memory databases A href=\"81.html#10\" 6 ) are emerging to be more significant due to the current era of memory-intensive computing . These databases are used in a wide range of systems ranging from real-time trading applications to IP routing . With the growing complexities of embedded systems (like real-time constraints) , use of a commercially developed structured Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. CIKM'04 , November 813 , 2004 , Washington , DC , USA. Copyright 2004 ACM 1-58113-874-1/04/0011 ... $ 5.00. memory database is becoming very critical A href=\"81.html#10\" 5 . Consequently, device developers are turning to commercial databases , but existing embedded DBMS software has not provided the ideal fit. Embedded databases emerged well over a decade ago to support business systems , with features including complex caching logic and abnormal termination recovery . But on a device , within a set-top box or next-generation fax machine , for example , these abilities are often unnecessary and cause the application to exceed available memory and CPU resources . In addition , current in-memory database", "label": ["hardware energy scheme", "query-directed energy management", "power consumption", "memory-resident databases", "database", "energy", "low-power modes", "query-directed scheme", "banked memory", "multi-query optimization", "dram", "energy optimization", "query restructuring"], "stemmed_label": ["hardwar energi scheme", "query-direct energi manag", "power consumpt", "memory-resid databas", "databas", "energi", "low-pow mode", "query-direct scheme", "bank memori", "multi-queri optim", "dram", "energi optim", "queri restructur"]}
{"doc": "Existing security models require that information of a given security level be prevented from \"leaking\" into lower-security information . High-security applications must be demonstrably free of such leaks , but such demonstration may require substantial manual analysis . Other authors have argued that the natural way to enforce these models automatically is with information-flow analysis , but have not shown this to be practicable for general purpose programming languages in current use . Modern safety-critical systems can contain software components with differing safety integrity levels , potentially operating in the same address space . This case poses problems similar to systems with differing security levels; failure to show separation of data may require the entire system to be validated at the higher integrity level . In this paper we show how the information flow model enforced by the SPARK Examiner provides support for enforcing these security and safety models . We describe an extension to the SPARK variable annotations which allows the specification of a security or safety level for each state variable , and an extension to the SPARK analysis which automatically enforces a given information flow policy on a SPARK program . INTRODUCTION Software is often used to develop security-critical applications . Some of these applications are required to manage information of different classification levels , ensuring that each user may only access the data for which he or she has adequate authorisation . This requirement has two components; the developer must produce a design and implementation which supports this model and its constraints , and the implementation must support verification that the constraints are never violated . It is not enough for the implementation to be correct; for it to be secure , it must be seen to be correct. In this paper we examine the problem of developing and verifying a security-critical application containing this security structure (often termed multi-level security) . We will analyse recent work on information flow and security and show how the information flow analysis of the SPARK Examiner tool is appropriate for solving this problem . We will show how it is also important for the efficient analysis of safety-critical systems . We will then describe modifications to the current definition of the SPARK Ada language 2 and the Examiner which permit complete validation of Ada programs against defined security models such as the Bell-LaPadula model 3 . We intend that the additional flow analysis features described here will appear in a future commercial Examiner release . Although we anticipate possible minor changes to the syntax and analysis performed , we expect the released version to implement the analysis described here in all substantial aspects. EXISTING WORK In this section we identify typical standards which will inform the development and verification of a security-critical or safety-critical application . We then analyse a survey paper on information flow and security , and compare its conclusions against the information flow model of the SPARK annotated subset of Ada95. 2.1 Standards The Common Criteria for IT Security 5 specify the development and verification activities that are", "label": ["security level", "spark ada", "integrity", "information flow", "dolev-yao", "subprogram", "spark", "information flow", "safety", "security", "bell-lapadula"], "stemmed_label": ["secur level", "spark ada", "integr", "inform flow", "dolev-yao", "subprogram", "spark", "inform flow", "safeti", "secur", "bell-lapadula"]}
{"doc": "Emergent self-organization in multi-agent systems appears to contradict the second law of thermodynamics . This paradox has been explained in terms of a coupling between the macro level that hosts self-organization (and an apparent reduction in entropy) , and the micro level (where random processes greatly increase entropy) . Metaphorically , the micro level serves as an entropy \"sink,\" permitting overall system entropy to increase while sequestering this increase from the interactions where self-organization is desired . We make this metaphor precise by constructing a simple example of pheromone-based coordination , defining a way to measure the Shannon entropy at the macro (agent) and micro (pheromone) levels , and exhibiting an entropy-based view of the coordination . INTRODUCTION Researchers who construct multi-agent systems must cope with the world's natural tendency to disorder . Many applications require a set of agents that are individually autonomous (in the sense that each agent determines its actions based on its own state and the state of the environment , without explicit external command) , but corporately structured . We want individual local decisions to yield coherent global behavior. Self-organization in natural systems (e.g. , human culture , insect colonies) is an existence proof that individual autonomy is not incompatible with global order . However , widespread human experience warns us that building systems that exhibit both individual autonomy and global order is not trivial. Not only agent researchers , but humans in general , seek to impose structure and organization on the world around us . It is a universal experience that the structure we desire can be achieved only through hard work , and that it tends to fall apart if not tended. This experience is sometimes summarized informally as \"Murphy's Law,\" the observation that anything that can go wrong , will go wrong and at the worst possible moment . At the root of the ubiquity of disorganizing tendencies is the Second Law of Thermodynamics , that \"energy spontaneously tends to flow only from being concentrated in one place to becoming diffused and spread out.\" 9 Adding energy to a system can overcome the Second Law's \"spontaneous tendency\" and lead to increasing structure. However , the way in which energy is added is critical . Gasoline in the engines of construction equipment can construct a building out of raw steel and concrete , while the same gasoline in a bomb can reduce a building to a mass of raw steel and concrete. Agents are not immune to Murphy . The natural tendency of a group of autonomous processes is to disorder , not to organization. Adding information to a collection of agents can lead to increased organization , but only if it is added in the right way . We will be successful in engineering agent-based systems just to the degree that we understand the interplay between disorder and order. The fundamental claim of this paper is that the relation between self-organization in multi-agent systems and thermodynamic concepts such as the second law is not just a loose metaphor , but can provide quantitative", "label": ["thermodynamic", "pheromones", "entropy", "entropy", "coordination", "autonomy", "pheromones", "multi-agent system", "self-organization", "self-organization"], "stemmed_label": ["thermodynam", "pheromon", "entropi", "entropi", "coordin", "autonomi", "pheromon", "multi-ag system", "self-organ", "self-organ"]}
{"doc": "We propose an entropy-based sensor selection heuristic for localization . Given 1) a prior probability distribution of the target location , and 2) the locations and the sensing models of a set of candidate sensors for selection , the heuristic selects an informative sensor such that the fusion of the selected sensor observation with the prior target location distribution would yield on average the greatest or nearly the greatest reduction in the entropy of the target location distribution . The heuristic greedily selects one sensor in each step without retrieving any actual sensor observations . The heuristic is also computationally much simpler than the mutual-information-based approaches . The effectiveness of the heuristic is evaluated using localization simulations in which Gaussian sensing models are assumed for simplicity . The heuristic is more effective when the optimal candidate sensor is more informative . INTRODUCTION The recent convergence of micro-electro-mechanical systems (MEMS) technology , wireless communication and networking technology , and low-cost low-power miniature digital hardware design technology has made the concept of wireless sensor networks viable and a new frontier of research 2 , 1 . The limited on-board energy storage and the limited wireless channel capacity are the major constraints of wireless sensor networks . In order to save precious resources, a sensing task should not involve more sensors than necessary . From the information-theoretic point of view , sensors are tasked to observe the target in order to increase the information (or to reduce the uncertainty) about the target state . The information gain attributable to one sensor may be very different from that attributable to another when sensors have different observation perspectives and sensing uncertainties . Selective use of informative sensors reduces the number of sensors needed to obtain information about the target state and therefore prolongs the system lifetime . In the scenario of localization or tracking using wireless sensor networks , the belief state of the target location can be gradually improved by repeatedly selecting the most informative unused sensor until the required accuracy (or uncertainty) level of the target state is achieved. There have been several investigations into information-theoretic approaches to sensor fusion and management . The idea of using information theory in sensor management was first proposed in 8 . Sensor selection based on expected information gain was introduced for decentralized sensing systems in 12 . The mutual information between the predicted sensor observation and the current target location distribution was proposed to evaluate the expected information gain about the target location attributable to a sensor in 11 , 6 . On the other hand , without using information theory , Yao et . al . 16 found that the overall localization accuracy depends on not only the accuracy of individual sensors but also the sensor locations relative to the target location during the development of localization algorithms . We propose a novel entropy-based heuristic for sensor selection based on our experiences with target localization . It is computationally more efficient than mutual-information-based methods proposed in 11 , 6 . 36 We use the following notations throughout", "label": ["shannon entropy", "entropy", "target localization", "localization", "target tracking", "wireless sensor networks", "mutual information", "information-directed resource management", "sensor selection", "heuristic", "information fusion"], "stemmed_label": ["shannon entropi", "entropi", "target local", "local", "target track", "wireless sensor network", "mutual inform", "information-direct resourc manag", "sensor select", "heurist", "inform fusion"]}
{"doc": "Localized search engines are small-scale systems that index a particular community on the web . They offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build , and can provide more precise and complete search capability over their relevant domains . One disadvantage such systems have over large-scale search engines is the lack of global PageRank values . Such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole . In this paper , we present well-motivated algorithms to estimate the global PageRank values of a local domain . The algorithms are all highly scalable in that , given a local domain of size n , they use O(n) resources that include computation time , bandwidth , and storage . We test our methods across a variety of localized domains , including site-specific domains and topic-specific domains . We demonstrate that by crawling as few as n or 2n additional pages , our methods can give excellent global PageRank estimates . INTRODUCTION Localized search engines are small-scale search engines that index only a single community of the web . Such communities can be site-specific domains , such as pages within Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and/or a fee. KDD'06 , August 2023 , 2006 , Philadelphia , Pennsylvania , USA. Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. the cs.utexas.edu domain , or topic-related communities-for example , political websites . Compared to the web graph crawled and indexed by large-scale search engines , the size of such local communities is typically orders of magnitude smaller . Consequently , the computational resources needed to build such a search engine are also similarly lighter . By restricting themselves to smaller , more manageable sections of the web , localized search engines can also provide more precise and complete search capabilities over their respective domains. One drawback of localized indexes is the lack of global information needed to compute link-based rankings . The PageRank algorithm 3 , has proven to be an effective such measure . In general , the PageRank of a given page is dependent on pages throughout the entire web graph . In the context of a localized search engine , if the PageRanks are computed using only the local subgraph , then we would expect the resulting PageRanks to reflect the perceived popularity within the local community and not of the web as a whole . For example , consider a localized search engine that indexes political pages with conservative views . A person wishing to research the opinions on global warming within the conservative political community", "label": ["node selection", "experimentation", "global pagerank", "algorithms", "crawling", "site specific domain", "localized search engines"], "stemmed_label": ["node select", "experiment", "global pagerank", "algorithm", "crawl", "site specif domain", "local search engin"]}
{"doc": "Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering , which makes recommendations to users based on their collective past behavior . While many similarity measures have been proposed and individually evaluated , they have not been evaluated relative to each other in a large real-world environment . We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network . We determine the usefulness of the different recommendations by actually measuring users' propensity to visit and join recommended communities . We also examine how the ordering of recommendations influenced user selection , as well as interesting social issues that arise in recommending communities within a real social network . INTRODUCTION The amount of information available online grows far faster than an individual's ability to assimilate it . For example, consider \"communities\" (user-created discussion groups) within Orkut , a social-networking website (http://www.orkut.com) Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. KDD'05 , August 2124 , 2005 , Chicago , Illinois , USA. Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00. affiliated with Google . The original mechanisms for users to find communities were labor-intensive , including searching for keywords in community titles and descriptions or browsing other users' memberships . Four months after its January 2004 debut , Orkut had over 50,000 communities, providing the necessity and opportunity for data-mining for automated recommendations . There are now (May 2005) over 1,500,000 communities. While there are many forms of recommender systems 3 , we chose a collaborative filtering approach 13 based on overlapping membership of pairs of communities . We did not make use of semantic information , such as the description of or messages in a community (although this may be an area of future work) . Our recommendations were on a per-community , rather than a per-user basis; that is , all members of a given community would see the same recommendations when visiting that community's page . We chose this approach out of the belief , which was confirmed , that community memberships were rich enough to make very useful recommendations without having to perform more compu-tationally intensive operations , such as clustering of users or communities or computing nearest neighbor relations among users . Indeed , Sarwar et al . have found such item-based algorithms to be both more efficient and successful than user-based algorithms 13 . By measuring user acceptance of recommendations , we were able to evaluate the absolute and relative utility of six different similarity measures on a large volume of data. MEASURES OF SIMILARITY The input", "label": ["collaborative filtering", "online communities", "community", "recommender system", "social network", "social networks", "similarity measure", "data mining"], "stemmed_label": ["collabor filter", "onlin commun", "commun", "recommend system", "social network", "social network", "similar measur", "data mine"]}
{"doc": "We present in this paper the design and an evaluation of a novel interface called the Relation Browser++ (RB++) for searching and browsing large information collections . RB++ provides visualized category overviews of an information space and allows dynamic filtering and exploration of the result set by tightly coupling the browsing and searching functions . A user study was conducted to compare the effectiveness , efficiency and user satisfaction of completing various types of searching and browsing using the RB++ interface and a traditional form-fillin interface for a video library . An exploration set of tasks was also included to examine the effectiveness of and user satisfaction with the RB++ when applied to a large federal statistics website . The comparison study strongly supported that RB++ was more effective , efficient , and satisfying for completing data exploration tasks . Based on the results , efforts to automatically populate the underlying database using machine learning techniques are underway . Preliminary implementations for two large-scale federal statistical websites have been installed on government servers for internal evaluation . INTRODUCTION The size and breadth of large government websites and digital libraries makes it difficult for people to quickly grasp what content is and is not available . Dynamic overviews and previews of collections can help people decide if it is worthwhile to look further 8 . As they do look further , it is helpful for their searching and browsing to quickly see partitions of the collection and how many items are available in different partitions . We believe that government website users will be well-served by highly interactive user interfaces that support alternative views of the collections , partitions , and results sets. This paper describes a user interface that aims to provide agile control for browse and search , reports results from a user study comparing this interface to a typical WWW search interface , and describes the ongoing evolution of the interface , including efforts to automate discovery of topical categories and assignment of webpages to those categories. Faceted category structure is one way to help people understand the composition of an information collection . A faceted approach provides different ways to slice and dice the information space , which allows people to look at the information space from different perspectives . Allowing people to explore the relationships among different facets may further deepen their understanding and support new insights . The relation browser (RB) is an interface which provides an overview of the collection by displaying different categories and enables people to explore the relationships among these categories 13 . The different facet values also serve as selectable objects that may be employed as query widgets for a search so that the entire space can quickly be partitioned with simple mouse moves and with consequent immediate display of the resulting partition in the results panel . Figure 1 shows the mock-up interface of an early version of the relation browser in the domain of U.S . federal statistics websites . The web pages in the site were sliced into four", "label": ["searching", "efficiency", "user satisfaction", "user study", "information search", "information storage and retrieval", "interaction patterns with interface", "interface design", "visualization", "relation browser++", "browsing", "browse and search interface", "rb++", "interactive system", "faceted category structure", "information browsing", "effectiveness", "search", "dynamic query", "facets", "browse", "human factor", "visual display", "modeling user interaction", "satisfaction", "user interface", "category overview", "user interface"], "stemmed_label": ["search", "effici", "user satisfact", "user studi", "inform search", "inform storag and retriev", "interact pattern with interfac", "interfac design", "visual", "relat browser++", "brows", "brows and search interfac", "rb++", "interact system", "facet categori structur", "inform brows", "effect", "search", "dynam queri", "facet", "brows", "human factor", "visual display", "model user interact", "satisfact", "user interfac", "categori overview", "user interfac"]}
{"doc": "With the overwhelming volume of online news available today , there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner . Previous research focused only on organizing news stories by their topics into a flat hierarchy . We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly . In this work , we attempt to capture the rich structure of events and their dependencies in a news topic through our event models . We call the process of recognizing events and their dependencies event threading . We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories . We formally define the novel problem , suggest evaluation metrics and present a few techniques for solving the problem . Besides the standard word based features , our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies . Our experiments on a manually labeled data sets show that our models effec-tively identify the events and capture dependencies among them . INTRODUCTION News forms a major portion of information disseminated in the world everyday . Common people and news analysts alike are very interested in keeping abreast of new things that happen in the news, but it is becoming very difficult to cope with the huge volumes Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. CIKM'04 , November 813 , 2004 , Washington,DC,USA. Copyright 2004 ACM 1-58113-874-1/04/0011 ... $ 5.00. of information that arrives each day . Hence there is an increasing need for automatic techniques to organize news stories in a way that helps users interpret and analyze them quickly . This problem is addressed by a research program called Topic Detection and Tracking (TDT) 3 that runs an open annual competition on standardized tasks of news organization. One of the shortcomings of current TDT evaluation is its view of news topics as flat collection of stories . For example , the detection task of TDT is to arrange a collection of news stories into clusters of topics . However , a topic in news is more than a mere collection of stories: it is characterized by a definite structure of inter-related events . This is indeed recognized by TDT which defines a topic as `a set of news stories that are strongly related by some seminal real-world event' where an event is defined as `something that happens at a specific time and location' 3", "label": ["complete-link model", "event", "intelligent information retrieval", "event threading", "threading", "meaningful and efficient analysis and presentation of news", "information browsing and organization", "nearest parent model", "information searching", "dependency modeling", "agglomerative clustering with time decay", "dependency", "news topic modeling", "topic detection and tracking", "clustering", "temporal localization of news stories"], "stemmed_label": ["complete-link model", "event", "intellig inform retriev", "event thread", "thread", "meaning and effici analysi and present of news", "inform brows and organ", "nearest parent model", "inform search", "depend model", "agglom cluster with time decay", "depend", "news topic model", "topic detect and track", "cluster", "tempor local of news stori"]}
{"doc": "In this paper we embed evolutionary computation into statistical learning theory . First , we outline the connection between large margin optimization and statistical learning and see why this paradigm is successful for many pattern recognition problems . We then embed evolutionary computation into the most prominent representative of this class of learning methods , namely into Support Vector Machines (SVM) . In contrast to former applications of evolutionary algorithms to SVMs we do not only optimize the method or kernel parameters . We rather use both evolution strategies and particle swarm optimization in order to directly solve the posed constrained optimization problem . Transforming the problem into the Wolfe dual reduces the total runtime and allows the usage of kernel functions . Exploiting the knowledge about this optimization problem leads to a hybrid mutation which further decreases convergence time while classification accuracy is preserved . We will show that evolutionary SVMs are at least as accurate as their quadratic programming counterparts on six real-world benchmark data sets . The evolutionary SVM variants frequently outperform their quadratic programming competitors . Additionally , the proposed algorithm is more generic than existing traditional solutions since it will also work for non-positive semidefinite kernel functions and for several , possibly competing , performance criteria . INTRODUCTION In this paper we will discuss how evolutionary algorithms can be used to solve large margin optimization problems. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. GECCO'06 , July 812 , 2006 , Seattle , Washington , USA. Copyright 2006 ACM 1-59593-186-4/06/0007 ... $ 5.00. We explore the intersection of three highly active research areas , namely machine learning , statistical learning theory, and evolutionary algorithms . While the connection between statistical learning and machine learning was analyzed before , embedding evolutionary algorithms into this connection will lead to a more generic algorithm which can deal with problems today's learning schemes cannot cope with. Supervised machine learning is often about classification problems . A set of data points is divided into several classes and the machine learning method should learn a decision function in order to decide into which class an unseen data point should be classified. The maximization of a margin between data points of different classes , i . e . the distance between a decision hyperplane and the nearest data points , interferes with the ideas of statistical learning theory . This allows the definition of an error bound for the generalization error . Furthermore , the usage of kernel functions allows the learning of non-linear decision functions . We focus on Support Vector Machines (SVM) as they are the most prominent representatives for large margin problems . Since", "label": ["support vector machines", "statistical learning theory", "kernel methods", "svm", "evolution strategies", "large margin", "particle swarms", "machine learning", "hybrid mutation", "evolutionary computation", "kernels"], "stemmed_label": ["support vector machin", "statist learn theori", "kernel method", "svm", "evolut strategi", "larg margin", "particl swarm", "machin learn", "hybrid mutat", "evolutionari comput", "kernel"]}
{"doc": "This paper presents a CORBA-compliant middleware architecture that is more flexible and extensible compared to standard CORBA . The portable design of this architecture is easily integrated in any standard CORBA middleware; for this purpose , mainly the handling of object references (IORs) has to be changed . To encapsulate those changes , we introduce the concept of a generic reference manager with portable profile managers . Profile managers are pluggable and in extreme can be downloaded on demand . To illustrate the use of this approach , we present a profile manager implementation for fragmented objects and another one for bridging CORBA to the Jini world . The first profile manager supports truly distributed objects , which allow seamless integration of partitioning , scalability , fault tolerance , end-to-end quality of service , and many more implementation aspects into a distributed object without losing distribution and location transparency . The second profile manager illustrates how our architecture enables fully transparent access from CORBA applications to services on non-CORBA platforms INTRODUCTION Middleware systems are heavily used for the implementation of complex distributed applications . Current developments like mobile environments and ubiquitous computing lead to new requirements that future middleware systems will have to meet . Examples for such requirements are the support for self-adaptation and self-optimisation as well as scalability , fault-tolerance , and end-to-end quality of service in the context of high dynamics . Heterogeneity in terms of various established middleware platforms calls for cross-platform interoperability . In addition , not all future requirements can be predicted today . A proper middleware design should be well-prepared for such future extensions. CORBA is a well-known standard providing an architecture for object -based middleware systems 5 . CORBA-based applications are built from distributed objects that can transparently interact with each other , even if they reside on different nodes in a distributed environment . CORBA objects can be implemented in different programming languages . Their interface has to be defined in a single, language-independent interface description language (IDL) . Problem -specific extensions allow to add additional features to the underlying base architecture. This paper discusses existing approaches towards a more flexible middleware infrastructure and proposes a novel modularisation pattern that leads to a flexible and extensible object middleware . Our design separates the handling of remote references from the object request broker (ORB) core and introduces the concept of ORB-independent portable profile managers , which are managed by a generic reference manager . The profile managers encapsulate all tasks related to reference handling , i.e. , reference creation , reference marshalling and unmarshalling , external representation of references as strings , and type casting of representatives of remote objects . The profile managers are independent from a specific ORB, and may even be loaded dynamically into the ORB . Only small modifications to existing CORBA implementations are necessary to support such a design. Our architecture enables the integration of a fragmented object model into CORBA middleware platforms , which allows transparent support of many implementation aspects of complex distributed systems , like partitioning", "label": ["integration", "flexible and extensible object middleware", "iiop", "software architecture for middleware", "corba", "object oriented", "extensions", "extensibility", "extensibility", "middleware architecture", "distiributed applications", "ubiquitous computing", "object references", "middleware", "extensible and reconfigurable middleware", "interoperability", "distributed objects", "implementation", "fault tolerant corba", "reference manager", "profile manager", "middleware interoperability", "middleware architecture", "profile manager", "remote object", "encapsulation", "flexibility", "ior", "middleware platform", "middleware systems"], "stemmed_label": ["integr", "flexibl and extens object middlewar", "iiop", "softwar architectur for middlewar", "corba", "object orient", "extens", "extens", "extens", "middlewar architectur", "distiribut applic", "ubiquit comput", "object refer", "middlewar", "extens and reconfigur middlewar", "interoper", "distribut object", "implement", "fault toler corba", "refer manag", "profil manag", "middlewar interoper", "middlewar architectur", "profil manag", "remot object", "encapsul", "flexibl", "ior", "middlewar platform", "middlewar system"]}
{"doc": "A goal of human-robot interaction is to allow one user to operate multiple robots simultaneously . In such a scenario the robots provide leverage to the user's attention . The number of such robots that can be operated is called the fan-out of a human-robot team . Robots that have high neglect tolerance and lower interaction time will achieve higher fan-out . We define an equation that relates fan-out to a robot's activity time and its interaction time . We describe how to measure activity time and fan-out . We then use the fan-out equation to compute interaction effort . We can use this interaction effort as a measure of the effectiveness of a human-robot interaction design . We describe experiments that validate the fan-out equation and its use as a metric for improving human-robot interaction . INTRODUCTION As computing becomes smaller , faster and cheaper the opportunity arises to embed computing in robots that perform a variety of \"dull , dirty and dangerous\" tasks that humans would rather not perform themselves . For the foreseeable future robots will not be fully autonomous , but will be directed by humans . This gives rise to the field of human-robot interaction (HRI) . Human-robot interaction differs from traditional desktop GUI-based direct manipulation interfaces in two key ways . First , robots must operate in a physical world that is not completely under software control . The physical world imposes its own forces , timing and unexpected events that must be handled by HRI . Secondly , robots are expected to operate independently for extended periods of time . The ability for humans to provide commands that extend over time and can accommodate unexpected circumstances complicates the HRI significantly . This problem of developing interfaces that control autonomous behavior while adapting to the unexpected is an interesting new area of research. We are very interested in developing and validating metrics that guide our understanding of how humans interact with semiautonomous robots . We believe that such laws and metrics can focus future HRI development . What we are focused on are not detailed cognitive or ergonomic models but rather measures for comparing competing human-robot interfaces that have some validity . In this paper we look at a particular aspect of HRI , which is the ability for an individual to control multiple robots simultaneously . We refer to this as the fan-out of a human-robot team . We hypothesize that the following fan-out equation holds, IT AT FO = where FO=fan-out or the number of robots a human can control simultaneously, AT=activity time or the time that a robot is actively effective after receiving commands from a user, IT=interaction time or the time that it takes for a human to interact with a given robot. In this paper we develop the rationale for the fan-out equation and report several experiments validating this equation . We show that the equation does describe many phenomena surrounding HRI but that the relationships are more complex than this simple statement of fan-out implies. We also describe the experimental", "label": ["multiple robots", "human-robot interaction", "interaction time", "fan-out", "interaction effort", "human-robot interaction", "neglect time", "user interface", "fan-out equation", "activity time"], "stemmed_label": ["multipl robot", "human-robot interact", "interact time", "fan-out", "interact effort", "human-robot interact", "neglect time", "user interfac", "fan-out equat", "activ time"]}
{"doc": "We give experimental evidence for the benefits of order-preserving compression in sorting algorithms . While , in general , any algorithm might benefit from compressed data because of reduced paging requirements , we identified two natural candidates that would further benefit from order-preserving compression , namely string-oriented sorting algorithms and word-RAM algorithms for keys of bounded length . The word-RAM model has some of the fastest known sorting algorithms in practice . These algorithms are designed for keys of bounded length , usually 32 or 64 bits , which limits their direct applicability for strings . One possibility is to use an order-preserving compression scheme , so that a bounded-key-length algorithm can be applied . For the case of standard algorithms , we took what is considered to be the among the fastest nonword RAM string sorting algorithms , Fast MKQSort , and measured its performance on compressed data . The Fast MKQSort algorithm of Bentley and Sedgewick is optimized to handle text strings . Our experiments show that order-compression techniques results in savings of approximately 15% over the same algorithm on noncompressed data . For the word-RAM , we modified Andersson's sorting algorithm to handle variable-length keys . The resulting algorithm is faster than the standard Unix sort by a factor of 1.5X . Last , we used an order-preserving scheme that is within a constant additive term of the optimal HuTucker , but requires linear time rather than O(m log m) , where m = | | is the size of the alphabet . INTRODUCTION In recent years , the size of corporate data collections has grown rapidly . For example , in the mid-1980s , a large text collection was in the order of 500 MB. Today , large text collections are over a thousand times larger . At the same time , archival legacy data that used to sit in tape vaults is now held on-line in large data warehouses and regularly accessed . Data-storage companies , such as EMC , have emerged to serve this need for data storage , with market capitalizations that presently rival that of all but the largest PC manufacturers. Devising algorithms for these massive data collections requires novel techniques . Because of this , over the last 10 years there has been renewed interest in research on indexing techniques , string-matching algorithms , and very large database-management systems among others. Consider , for example , a corporate setting , such as a bank , with a large collection of archival data , say , a copy of every bank transaction ever made . Data is stored in a data-warehouse facility and periodically accessed , albeit perhaps somewhat unfrequently . Storing the data requires a representation that is succinct , amenable to arbitrary searches , and supports efficient random access. Aside from savings in storage , a no less important advantage of a succinct representation of archival data is a resulting improvement in performance of sorting and searching operations . This improvement is twofold: First , in general , almost any sorting and", "label": ["order-preserving compression", "string sorting", "random access", "order-preserving compression scheme", "linear time algorithm", "sorting algorithms", "word-ram sorting algorithm", "ram", "sorting", "unit-cost", "compression scheme", "compression ratio", "word-ram", "data collection", "keys of bounded length"], "stemmed_label": ["order-preserv compress", "string sort", "random access", "order-preserv compress scheme", "linear time algorithm", "sort algorithm", "word-ram sort algorithm", "ram", "sort", "unit-cost", "compress scheme", "compress ratio", "word-ram", "data collect", "key of bound length"]}
{"doc": "FBRAM , a new form of dynamic random access memory that greatly accelerates the rendering of Z-buffered primitives , is presented . Two key concepts make this acceleration possible . The first is to convert the read-modify-write Z-buffer compare and RGBα blend into a single write only operation . The second is to support two levels of rectangularly shaped pixel caches internal to the memory chip . The result is a 10 megabit part that , for 3D graphics , performs read-modify-write cycles ten times faster than conventional 60 ns VRAMs . A four-way interleaved 100 MHz FBRAM frame buffer can Z-buffer up to 400 million pixels per second . Working FBRAM prototypes have been fabricated . INTRODUCTION One of the traditional bottlenecks of 3D graphics hardware has been the rate at which pixels can be rendered into a frame buffer . Modern interactive 3D graphics applications require rendering platforms that can support 30 Hz animation of richly detailed 3D scenes . But existing memory technologies cannot deliver the desired rendering performance at desktop price points. The performance of hidden surface elimination algorithms has been limited by the pixel fill rate of 2D projections of 3D primitives. While a number of exotic architectures have been proposed to improve rendering speed beyond that achievable with conventional DRAM or VRAM , to date all commercially available workstation 3D accelerators have been based on these types of memory chips. This paper describes a new form of specialized memory , Frame Buffer RAM (FBRAM) . FBRAM increases the speed of Z-buffer operations by an order of magnitude , and at a lower system cost than conventional VRAM . This speedup is achieved through two architectural changes: moving the Z compare and RGB blend operations inside the memory chip , and using two levels of appropri-ately shaped and interleaved on-chip pixel caches. PREVIOUS WORK After the Z-buffer algorithm was invented 3 , the first Z-buffered hardware systems were built in the 1970's from conventional DRAM memory chips . Over time , the density of DRAMs increased exponentially , but without corresponding increases in I/O bandwidth . Eventually , video output bandwidth requirements alone exceeded the total DRAM I/O bandwidth. Introduced in the early 1980's , VRAM 18 20 solved the video output bandwidth problem by adding a separate video port to a DRAM . This allowed graphics frame buffers to continue to benefit from improving bit densities , but did nothing directly to speed rendering operations . More recently , rendering architectures have bumped up against a new memory chip bandwidth limitation: faster rendering engines have surpassed VRAM's input bandwidth . As a result , recent generations of VRAM have been forced to increase the width of their I/O busses just to keep up . For the last five years, the pixel fill (i.e . write) rates of minimum chip count VRAM frame buffers have increased by less than 30%. Performance gains have mainly been achieved in commercially available systems by brute force . Contemporary mid-range systems have employed 10-way and 20-way interleaved VRAM designs 1 14", "label": ["fbram", "video output bandwidth", "dynamic random access memory", "rgba blend", "dynamic memory", "dram", "rendering rate", "z buffer", "rendering", "graphics", "caching", "memory", "dynamic memory chips", "pixel processing", "3d graphics hardware", "acceleration", "3d graphics", "z-buffer", "optimisation", "z-compare", "pixel caching", "vram", "fbram", "z-buffering", "parallel graphics algorithms", "video buffers", "pixel processing", "pixel cache", "frame buffer", "sram", "caches"], "stemmed_label": ["fbram", "video output bandwidth", "dynam random access memori", "rgba blend", "dynam memori", "dram", "render rate", "z buffer", "render", "graphic", "cach", "memori", "dynam memori chip", "pixel process", "3d graphic hardwar", "acceler", "3d graphic", "z-buffer", "optimis", "z-compar", "pixel cach", "vram", "fbram", "z-buffer", "parallel graphic algorithm", "video buffer", "pixel process", "pixel cach", "frame buffer", "sram", "cach"]}
{"doc": "In this paper we study the problem of finding most topical named entities among all entities in a document , which we refer to as focused named entity recognition . We show that these focused named entities are useful for many natural language processing applications , such as document summarization , search result ranking , and entity detection and tracking . We propose a statistical model for focused named entity recognition by converting it into a classification problem . We then study the impact of various linguistic features and compare a number of classification algorithms . From experiments on an annotated Chinese news corpus , we demonstrate that the proposed method can achieve near human-level accuracy . INTRODUCTION With the rapid growth of online electronic documents, many technologies have been developed to deal with the enormous amount of information , such as automatic summarization , topic detection and tracking , and information retrieval . Among these technologies , a key component is to Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. SIGIR'04 , July 2529 , 2004 , Sheffield , South Yorkshire , UK. Copyright 2004 ACM 1-58113-881-4/04/0007 ... $ 5.00. identify the main topics of a document , where topics can be represented by words , sentences , concepts , and named entities . A number of techniques for this purpose have been proposed in the literature , including methods based on position 3 , cue phrases 3 , word frequency , lexical chains 1 and discourse segmentation 13 . Although word frequency is the easiest way to representing the topics of a document, it was reported in 12 that position methods produce better results than word counting based methods. Important sentence extraction is the most popular method studied in the literature . A recent trend in topic sentence extraction is to employ machine learning methods . For example , trainable classifiers have been used in 8 , 21 , 5 , 11 to select sentences based on features such as cue phrase, location , sentence length , word frequency and title , etc. All of the above methods share the same goal of extracting important sentences from documents . However , for topic representation , sentence-level document summaries may still contain redundant information . For this reason , other representations have also been suggested . For example , in 17 , the authors used structural features of technical papers to identify important concepts rather than sentences . The authors of 9 presented an efficient algorithm to choose topic terms for hierarchical summarization according to a proba-bilistic language model . Another hybrid system , presented in 7 , generated summarizations with the help of", "label": ["named entities", "information retrieval", "naive bayes", "topic identification", "classification model", "sentence extraction", "summarization", "entity recognition", "natural language processing applications", "ranking", "information retrieval", "linguistic features", "automatic summarization", "classification methods", "robust risk minimization", "decision tree", "electronic documents", "machine learning", "focused named entity recognition", "statistical model", "features", "machine learning approach", "text summarization", "main topics", "natural language processing"], "stemmed_label": ["name entiti", "inform retriev", "naiv bay", "topic identif", "classif model", "sentenc extract", "summar", "entiti recognit", "natur languag process applic", "rank", "inform retriev", "linguist featur", "automat summar", "classif method", "robust risk minim", "decis tree", "electron document", "machin learn", "focus name entiti recognit", "statist model", "featur", "machin learn approach", "text summar", "main topic", "natur languag process"]}
{"doc": "Starting from P . Sestoft semantics for lazy evaluation , we define a new semantics in which normal forms consist of variables pointing to lambdas or constructions . This is in accordance with the more recent changes in the Spineless Tagless G-machine (STG) machine , where constructions only appear in closures (lambdas only appeared in closures already in previous versions) . We prove the equivalence between the new semantics and Sestoft's . Then , a sequence of STG machines are derived , formally proving the correctness of each derivation . The last machine consists of a few imperative instructions and its distance to a conventional language is minimal . The paper also discusses the differences between the final machine and the actual STG machine implemented in the Glasgow Haskell Compiler . INTRODUCTION The Spineless Tagless G-machine (STG) 6 is at the heart of the Glasgow Haskell Compiler (GHC) 7 which is perhaps the Haskell compiler generating the most efficient code . For a description of Haskell language see 8 . Part of the secret for that is the set of analysis and transformations carried out at the intermediate representation level . Another part of the explanation is the efficient design and implementation of the STG machine. A high level description of the STG can be found in 6 . If the reader is interested in a more detailed view , then the only available information is the Haskell code of GHC (about 80.000 lines , 12.000 of which are devoted to the implementation of the STG machine) and the C code of its different runtime systems (more than 40.000 lines) 1 . In this paper we provide a step-by-step derivation of the STG machine , starting from a description higher-level than that of 6 and arriving at a description lower-level than that. Our starting point is a commonly accepted operational semantics for lazy evaluation provided by Peter Sestoft in 10 as an improvement of John Launchbury's well-known definition in 4 . Then , we present the following refinements: 1 . A new operational semantics , which we call semantics S3 --acknowledging that semantics 1 and 2 were defined by Mountjoy in a previous attempt 5 -- , where normal forms may appear only in bindings. 2 . A first machine , called STG-1 , derived from S3 in which explicit replacement of pointers for variables is done in expressions. 3 . A second machine STG-2 introducing environments in closures , case alternatives , and in the control expression . 4 . A third machine , called ISTG (I stands for imperative) with a very small set of elementary instructions , each one very easy to be implemented in a conventional language such as C. 5 . A translation from the language of STG-2 to the language of ISTG in which the data structures of STG-2 are represented (or implemented) by the ISTG data structures. 102 e x -- variable | x.e -- lambda abstraction | e x -- application | letrec x i = e i in e -- recursive let |", "label": ["lazy evaluation", "operational semantics", "functional programming", "stg machine", "compiler verification", "closures", "translation scheme", "abstract machine", "stepwise derivation", "operational semantics", "abstract machines", "haskell compiler"], "stemmed_label": ["lazi evalu", "oper semant", "function program", "stg machin", "compil verif", "closur", "translat scheme", "abstract machin", "stepwis deriv", "oper semant", "abstract machin", "haskel compil"]}
{"doc": "Web query classification (QC) aims to classify Web users' queries , which are often short and ambiguous , into a set of target categories . QC has many applications including page ranking in Web search , targeted advertisement in response to queries , and personalization . In this paper , we present a novel approach for QC that outperforms the winning solution of the ACM KDDCUP 2005 competition , whose objective is to classify 800,000 real user queries . In our approach , we first build a bridging classifier on an intermediate taxonomy in an offline mode . This classifier is then used in an online mode to map user queries to the target categories via the above intermediate taxonomy . A major innovation is that by leveraging the similarity distribution over the intermediate taxonomy , we do not need to retrain a new classifier for each new set of target categories , and therefore the bridging classifier needs to be trained only once . In addition , we introduce category selection as a new method for narrowing down the scope of the intermediate taxonomy based on which we classify the queries . Category selection can improve both efficiency and effectiveness of the online classification . By combining our algorithm with the winning solution of KDDCUP 2005 , we made an improvement by 9.7% and 3.8% in terms of precision and F1 respectively compared with the best results of KDDCUP 2005 . INTRODUCTION With exponentially increasing information becoming available on the Internet , Web search has become an indispensable tool for Web users to gain desired information . Typi-cally , Web users submit a short Web query consisting of a few words to search engines . Because these queries are short and ambiguous , how to interpret the queries in terms of a set of target categories has become a major research issue. In this paper , we call the problem of generating a ranked list of target categories from user queries the query classification problem , or QC for short. The importance of QC is underscored by many services provided by Web search . A direct application is to provide better search result pages for users with interests of different categories . For example , the users issuing a Web query \"apple\" might expect to see Web pages related to the fruit apple , or they may prefer to see products or news related to the computer company . Online advertisement services can rely on the QC results to promote different products more accurately . Search result pages can be grouped according to the categories predicted by a QC algorithm . However, the computation of QC is non-trivial , since the queries are usually short in length , ambiguous and noisy (e.g. , wrong spelling) . Direct matching between queries and target categories often produces no result . In addition , the target categories can often change , depending on the new Web contents as the Web evolves , and as the intended services change as well. KDDCUP 2005 ( http://www.acm.org/sigkdd/kddcup", "label": ["bridging classifier", "category selection", "ensemble classifier", "bridging classifier", "target categories", "similarity distribution", "mapping functions", "search engine", "matching approaches", "intermediate categories", "taxonomy", "query enrichment", "kddcup 2005", "query classification", "category selection", "web query classification"], "stemmed_label": ["bridg classifi", "categori select", "ensembl classifi", "bridg classifi", "target categori", "similar distribut", "map function", "search engin", "match approach", "intermedi categori", "taxonomi", "queri enrich", "kddcup 2005", "queri classif", "categori select", "web queri classif"]}
{"doc": "A collaborative crawler is a group of crawling nodes , in which each crawling node is responsible for a specific portion of the web . We study the problem of collecting geographically -aware pages using collaborative crawling strategies . We first propose several collaborative crawling strategies for the geographically focused crawling , whose goal is to collect web pages about specified geographic locations , by considering features like URL address of page , content of page , extended anchor text of link , and others . Later , we propose various evaluation criteria to qualify the performance of such crawling strategies . Finally , we experimentally study our crawling strategies by crawling the real web data showing that some of our crawling strategies greatly outperform the simple URL-hash based partition collaborative crawling , in which the crawling assignments are determined according to the hash-value computation over URLs . More precisely , features like URL address of page and extended anchor text of link are shown to yield the best overall performance for the geographically focused crawling . INTRODUCTION While most of the current search engines are effective for pure keyword-oriented searches , these search engines are not fully effective for geographic-oriented keyword searches . For instance , queries like \"restaurants in New York , NY\" or \"good plumbers near 100 milam street , Houston , TX\" or \"romantic hotels in Las Vegas , NV\" are not properly man-aged by traditional web search engines . Therefore , in recent This work was done while the author was visiting Genieknows .com Copyright is held by the International World Wide Web Conference Committee (IW3C2) . Distribution of these papers is limited to classroom use, and personal use by others. WWW 2006 , May 2326 , 2006 , Edinburgh , Scotland. ACM 1-59593-323-9/06/0005. years , there has been surge of interest within the search industry on the search localization (e.g. , Google Local 1 , Yahoo Local 2 ) . The main aim of such search localization is to allow the user to perform the search according his/her keyword input as well as the geographic location of his/her interest. Due to the current size of the Web and its dynamical nature , building a large scale search engine is challenging and it is still active area of research . For instance , the design of efficient crawling strategies and policies have been exten-sively studied in recent years (see 9 for the overview of the field) . While it is possible to build geographically sensitive search engines using the full web data collected through a standard web crawling , it would rather be more attractive to build such search engines over a more focused web data collection which are only relevant to the targeted geographic locations . Focusing on the collection of web pages which are relevant to the targeted geographic location would leverage the overall processing time and efforts for building such search engines . For instance , if we want to build a search engine targeting those users in New York , NY , then", "label": ["geographical nodes", "crawling strategies", "collaborative crawler", "evaluation criteria", "url based", "geographic locality", "normalization and disambiguation of city names", "search engine", "geographically focused crawling", "scalability", "geographic locality", "collaborative crawling", "anchor text", "collaborative crawling", "problems of aliasing and ambiguity", "search localization", "ip address based", "focused crawler", "geo-focus", "geographic entities", "full content based", "extracted url", "pattern matching", "crawling strategies", "geo-coverage", "hash based collaboration", "geographically focused crawling", "quality issue"], "stemmed_label": ["geograph node", "crawl strategi", "collabor crawler", "evalu criteria", "url base", "geograph local", "normal and disambigu of citi name", "search engin", "geograph focus crawl", "scalabl", "geograph local", "collabor crawl", "anchor text", "collabor crawl", "problem of alias and ambigu", "search local", "ip address base", "focus crawler", "geo-focu", "geograph entiti", "full content base", "extract url", "pattern match", "crawl strategi", "geo-coverag", "hash base collabor", "geograph focus crawl", "qualiti issu"]}
{"doc": "In this paper we consider implementations of embedded 3D graphics and provide evidence indicating that 3D benchmarks employed for desktop computers are not suitable for mobile environments . Consequently , we present GraalBench , a set of 3D graphics workloads representative for contemporary and emerging mobile devices . In addition , we present detailed simulation results for a typical rasterization pipeline . The results show that the proposed benchmarks use only a part of the resources offered by current 3D graphics libraries . For instance , while each benchmark uses the texturing unit for more than 70% of the generated fragments , the alpha unit is employed for less than 13% of the fragments . The Fog unit was used for 84% of the fragments by one benchmark , but the other benchmarks did not use it at all . Our experiments on the proposed suite suggest that the texturing , depth and blending units should be implemented in hardware , while , for instance , the dithering unit may be omitted from a hardware implementation . Finally , we discuss the architectural implications of the obtained results for hardware implementations . INTRODUCTION In recent years , mobile computing devices have been used for a broader spectrum of applications than mobile telephony or personal digital assistance . Several companies expect that 3D graphics applications will become an important workload of wireless devices. For example , according to 10 , the number of users of interactive 3D graphics applications (in particular games) is expected to increase drastically in the future: it is predicted that the global wireless games market will grow to 4 billion dollars in 2006 . Because current wireless devices do not have sufficient computational power to support 3D graphics in real time and because present accelerators consume too much power , several companies and universities have started to develop a low-power 3D graphics accelerator . However, to the best of our knowledge , there is no publicly available benchmark suite that can be used to guide the architectural exploration of such devices. This paper presents GraalBench , a 3D graphics benchmark suite suitable for 3D graphics on low-power , mobile systems , in particular mobile phones . These benchmarks were collected to facilitate our studies on low-power 3D graphics accelerators in the Graal (GRAphics AcceLerator) project 5 . It includes several games as well as virtual reality applications such as 3D museum guides . Applications were selected on the basis of several criteria . For example , CAD/CAM applications , such as contained in the Viewperf package 18 , were excluded because it is unlikely that they will be offered on mobile devices . Other characteristics we considered are resolution and polygon count. A second goal of this paper is to provide a detailed quantitative workload characterization of the collected benchmarks . For each rasterization unit , we determine if it is used by the benchmark , and collect several statistics such as the number of fragments that bypass the unit , fragments that are processed by the unit and", "label": ["mechanism", "workload characterization", "api", "mobile devices", "embedded 3d graphics", "accelerators", "3d graphics benchmarking", "real-time", "bottlenecks", "rasterization", "graalbench", "architecture", "embedded systems", "workload", "benchmark", "3d graphics", "pipeline", "mobile environments", "3d graphics applications", "mobile phones", "triangles", "opengl", "unit", "graalbench", "performance", "measurement", "statistics", "opengl", "transform and lighting", "embedded 3d graphics architectures", "3d graphics benchmarks"], "stemmed_label": ["mechan", "workload character", "api", "mobil devic", "embed 3d graphic", "acceler", "3d graphic benchmark", "real-tim", "bottleneck", "raster", "graalbench", "architectur", "embed system", "workload", "benchmark", "3d graphic", "pipelin", "mobil environ", "3d graphic applic", "mobil phone", "triangl", "opengl", "unit", "graalbench", "perform", "measur", "statist", "opengl", "transform and light", "embed 3d graphic architectur", "3d graphic benchmark"]}
{"doc": "Vertical handoff is a switching process between heterogeneous wireless networks in a hybrid 3G/WLAN network . Vertical handoffs fromWLAN to 3G network often fail due to the abrupt degrade of the WLAN signal strength in the transition areas . In this paper , a Handoff Trigger Table is introduced to improve the performance of vertical handoff . Based on this table , a proactive handoff scheme is proposed . Simulation results show that with the proposed scheme , the vertical handoff decisions will be more efficient so that dropping probability can be decreased dramatically . INTRODUCTION With the emergence of different wireless technologies , which are developed for different purposes , the integration of these wireless networks has attracted much attention from both academia and industry . Among them , the integration of 3G cellular networks and wireless local access networks (WLAN) has become a very active area in the development toward the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee. IWCMC'06 , July 36 , 2006 , Vancouver , British Columbia , Canada. Copyright 2006 ACM 1-59593-306-9/06/0007 ... $ 5.00. next generation wireless networks . WLAN access technology can offer high-speed data connections in a small coverage with relatively low cost . On the other hand , cellular networks can offer connectivity over several square kilometers but with relatively low data rate . Taking advantages of both networks will bring great benefits to both service providers and users. One of the desired features of such a heterogeneous wireless network is to support seamless global roaming or vertical handoff . Traditionally , handoff is performed within the same wireless system , which is called horizontal handoff. In contrast , a vertical handoff takes place between different wireless networks 2 . In an integrated 3G/WLAN network , there are two directions in vertical handoff , one is from WLANs to 3G networks and the other is from3G Networks to WLANs . In the first direction , the objective for handoff is to maintain the connectivity , i.e. , switching to the cellular network before the WLAN link breaks while trying to stay in the WLAN as long as possible because of its relatively high bandwidth and low cost . Since a WLAN has smaller coverage and is usually covered by a 3G network , when the mobile terminal (MT) steps out of the WLAN area , the decay of the signal fromthe WLAN should be accurately detected . A timely decision of handoff should be made properly , and the MT should switch the connection to the appropriate 3G network successfully . In the second direction , the objective of handoff is usually to improve QoS", "label": ["wlan", "integrated networks", "vertical handoff", "cellular network", "3g", "wireless communications", "handoff trigger table", "wireles networks"], "stemmed_label": ["wlan", "integr network", "vertic handoff", "cellular network", "3g", "wireless commun", "handoff trigger tabl", "wirel network"]}
