{"doc": "Synthetic simultaneity - natural and artificial In control loops, each element introduces time delays. If those time delays are larger than the critical times for control of the system, a problem exists. I show a simple approach to mitigating this problem by basing the controller's decisions not on the observations themselves but on our projections as to what the observations will be at the time our controls reach the controlled system. Finally, I argue that synthetic simultaneity explains Libet's (1993) results better than Libet's explanation", "label": ["control loops", "time delays", "critical times", "controller decisions", "observations", "synthetic simultaneity"], "stemmed_label": ["control loop", "time delay", "critic time", "control decis", "observ", "synthet simultan"]}
{"doc": "An inverse problem for a model of a hierarchical structure We consider the inverse problem for the identification of the coefficient in a parabolic equation. The model is applied to describe the functioning of a hierarchical structure; it is also relevant for heat-conduction theory. Unique solvability of the inverse problem is proved", "label": ["inverse problem", "hierarchical structure", "parabolic equation", "heat-conduction theory", "unique solvability"], "stemmed_label": ["invers problem", "hierarch structur", "parabol equat", "heat-conduct theori", "uniqu solvabl"]}
{"doc": "Operational phase-space probability distribution in quantum communication theory Operational phase-space probability distributions are useful tools for describing quantum mechanical systems, including quantum communication and quantum information processing systems. It is shown that quantum communication channels with Gaussian noise and quantum teleportation of continuous variables are described by operational phase-space probability distributions. The relation of operational phase-space probability distribution to the extended phase-space formalism proposed by Chountasis and Vourdas (1998) is discussed", "label": ["operational phase-space probability distribution", "quantum communication theory", "quantum mechanical systems", "quantum information processing systems", "gaussian noise", "quantum teleportation", "continuous variables", "extended phase-space formalism"], "stemmed_label": ["oper phase-spac probabl distribut", "quantum commun theori", "quantum mechan system", "quantum inform process system", "gaussian nois", "quantum teleport", "continu variabl", "extend phase-spac formal"]}
{"doc": "Exploring developments in Web based relationship marketing within the hotel industry This paper provides a content analysis study of the application of World Wide Web marketing by the hotel industry. There is a lack of historical perspective on industry related Web marketing applications and this paper attempts to resolve this with a two-year follow-up case study of the changing use of the Web to develop different types of relationships. Specifically, the aims are: (1) to identify key changes in the way hotels are using the Web; (2) to look for evidence of the adoption of a relationship marketing (RM) model as a strategy for the development of hotel Web sites and the use of new technologies; and, (3) To investigate the use of multimedia in hotel Web sites. The development and strategic exploitation of the Internet has transformed the basis of marketing. Using the evidence from a Web content survey this study reveals the way relationships are being created and managed within the hotel industry by its use of the Web as a marketing tool. The authors have collected evidence by means of a descriptive study on the way hotels build and create relationships with their Web presence delivering multimedia information as well as channel and interactive means of communication. In addition a strategic framework is offered as the means to describe the mechanism and orientation of Web based marketing by hotels. The study utilizes a model by Gilbert (1996) as a means of developing a measurement instrument to allow a content analysis of the current approach by hotels to the development of Web sites. The results indicate hotels are aware of the new uses of Web technology and are promoting hotel products in the global electronic market in new and sophisticated ways", "label": ["web based relationship marketing", "hotel industry", "world wide web marketing", "hotel web sites", "multimedia", "web content survey", "global electronic market"], "stemmed_label": ["web base relationship market", "hotel industri", "world wide web market", "hotel web site", "multimedia", "web content survey", "global electron market"]}
{"doc": "Fast frequency acquisition phase-frequency detectors for Gsamples/s phase-locked loops This paper describes two techniques for designing phase-frequency detectors (PFDs) with higher operating frequencies periods of less than 8* the delay of a fan-out-4 inverter (FO-4) and faster frequency acquisition. Prototypes designed in 0.25- mu m CMOS process exhibit operating frequencies of 1.25 GHz =1/(8.FO-4) and 1.5 GHz =1/(6.7.FO-4) for two techniques, respectively, whereas a conventional PFD operates at 1 GHz =1/(10.FO-4) . The two proposed PFDs achieve a capture range of 1.7* and 1.4* the conventional design, respectively", "label": ["phase-frequency detectors", "fast frequency acquisition", "cmos process", "clock generator", "latch-based pfd architecture", "phase-locked loop", "gsamples/s pll", "pass-transistor dff pfd architecture", "1.25 ghz", "1.5 ghz", "0.25 micron"], "stemmed_label": ["phase-frequ detector", "fast frequenc acquisit", "cmo process", "clock gener", "latch-bas pfd architectur", "phase-lock loop", "gsamples/ pll", "pass-transistor dff pfd architectur", "1.25 ghz", "1.5 ghz", "0.25 micron"]}
{"doc": "Quantitative speed control for SRM drive using fuzzy adapted inverse model Quantitative and robust speed control for a switched reluctance motor (SRM) drive is considered to be rather difficult and challenging owing to its highly nonlinear dynamic behavior. A speed control scheme having two-degree-of-freedom (2DOF) structure is developed here to improve the speed dynamic response of an SRM drive. In the proposed control scheme, the feedback controller is quantitatively designed to meet the desired regulation control requirements first. Then a reference model and a command feedforward controller based on an inverse plant model are employed to yield the desired tracking response at nominal case. As the variations of system parameters and operating conditions occur, the prescribed control specifications may not be satisfied any more. To improve this, the inverse model is adaptively tuned by a fuzzy control scheme so that the model-following tracking error is significantly reduced. In addition, a simple disturbance cancellation robust controller is added to improve the tracking and regulation control performances further", "label": ["quantitative speed control", "srm drive", "fuzzy adapted inverse model", "switched reluctance motor", "nonlinear dynamic behavior", "two-degree-of-freedom structure", "speed dynamic response", "regulation control requirements", "reference model", "command feedforward controller", "inverse plant model", "tracking response", "system parameters", "operating conditions", "control specifications", "fuzzy control scheme", "model-following tracking error", "disturbance cancellation controller"], "stemmed_label": ["quantit speed control", "srm drive", "fuzzi adapt invers model", "switch reluct motor", "nonlinear dynam behavior", "two-degree-of-freedom structur", "speed dynam respons", "regul control requir", "refer model", "command feedforward control", "invers plant model", "track respons", "system paramet", "oper condit", "control specif", "fuzzi control scheme", "model-follow track error", "disturb cancel control"]}
{"doc": "Stock market dynamics We elucidate on several empirical statistical observations of stock market returns. Moreover, we find that these properties are recurrent and are also present in invariant measures of low-dimensional dynamical systems. Thus, we propose that the returns are modeled by the first Poincare return time of a low-dimensional chaotic trajectory. This modeling, which captures the recurrent properties of the return fluctuations, is able to predict well the evolution of the observed statistical quantities. In addition, it explains the reason for which stocks present simultaneously dynamical properties and high uncertainties. In our analysis, we use data from the S&P 500 index and the Brazilian stock Telebras", "label": ["stock market returns", "empirical statistical observations", "invariant measures", "low-dimensional dynamical systems", "first poincare return time", "low-dimensional chaotic trajectory", "statistical quantities", "brazilian stock", "econophysics"], "stemmed_label": ["stock market return", "empir statist observ", "invari measur", "low-dimension dynam system", "first poincar return time", "low-dimension chaotic trajectori", "statist quantiti", "brazilian stock", "econophys"]}
{"doc": "ePsych: interactive demonstrations and experiments in psychology ePsych (http://epsych.msstate.edu), a new Web site currently under active development, is intended to teach students about the discipline of psychology. The site presumes little prior knowledge about the field and so may be used in introductory classes, but it incorporates sufficient depth of coverage to be useful in more advanced classes as well. Numerous interactive and dynamic elements are incorporated into various modules, orientations, and guidebooks. These elements include Java-based experiments and demonstrations, video clips, and animated diagrams. Rapid access to all material is provided through a layer-based navigation system that allows users to visit various \"Worlds of the Mind.\" Active learning is encouraged, by challenging students with puzzles and problems and by providing the opportunity to \"dig deeper\" to learn more about the phenomena at hand", "label": ["epsych", "interactive demonstrations", "psychology experiments", "web site", "teaching", "java-based experiments", "video clips", "animated diagrams", "layer-based navigation system", "worlds of the mind", "active learning"], "stemmed_label": ["epsych", "interact demonstr", "psycholog experi", "web site", "teach", "java-bas experi", "video clip", "anim diagram", "layer-bas navig system", "world of the mind", "activ learn"]}
{"doc": "Collective action in the age of the Internet: mass communication and online mobilization This article examines how the Internet transforms collective action. Current practices on the Web bear witness to thriving collective action ranging from persuasive to confrontational, individual to collective, undertakings. Even more influential than direct calls for action is the indirect mobilizing influence of the Internet's powers of mass communication, which is boosted by an antiauthoritarian ideology on the Web. Theoretically, collective action through the otherwise socially isolating computer is possible because people rely on internalized group memberships and social identities to achieve social involvement. Empirical evidence from an online survey among environmental activists and nonactivists confirms that online action is considered an equivalent alternative to offline action by activists and nonactivists alike. However, the Internet may slightly alter the motives underlying collective action and thereby alter the nature of collective action and social movements. Perhaps more fundamental is the reverse influence that successful collective action will have on the nature and function of the Internet", "label": ["internet", "mass communication", "online mobilization", "collective action", "world wide web", "antiauthoritarian ideology", "group memberships", "social identities", "online survey", "anonymity", "politics"], "stemmed_label": ["internet", "mass commun", "onlin mobil", "collect action", "world wide web", "antiauthoritarian ideolog", "group membership", "social ident", "onlin survey", "anonym", "polit"]}
{"doc": "There is no optimal routing policy for the torus A routing policy is the method used to select a specific output channel for a message from among a number of acceptable output channels. An optimal routing policy is a policy that maximizes the probability of a message reaching its destination without delays. Optimal routing policies have been proposed for several regular networks, including the mesh and the hypercube. An open problem in interconnection network research has been the identification of an optimal routing policy for the torus. In this paper, we show that there is no optimal routing policy for the torus. Our result is demonstrated by presenting a detailed example in which the best choice of output channel is dependent on the probability of each channel being available. This result settles, in the negative, a conjecture by J. Wu (1996) concerning an optimal routing policy for the torus", "label": ["optimal routing policy", "torus", "hypercube"], "stemmed_label": ["optim rout polici", "toru", "hypercub"]}
{"doc": "Integrating virtual and physical context to support knowledge workers The Kimura system augments and integrates independent tools into a pervasive computing system that monitors a user's interactions with the computer, an electronic whiteboard, and a variety of networked peripheral devices and data sources", "label": ["pervasive computing", "knowledge workers", "networked peripheral devices", "electronic whiteboard", "kimura system", "data sources"], "stemmed_label": ["pervas comput", "knowledg worker", "network peripher devic", "electron whiteboard", "kimura system", "data sourc"]}
{"doc": "Prospective on computer applications in power The so-called \"deregulation\" and restructuring of the electric power industry have made it very difficult to keep up with industry changes and have made it much more difficult to envision the future. In this article, current key issues and major developments of the past few years are reviewed to provide perspective, and prospects for future computer applications in power are suggested. Technology changes are occurring at an exponential rate. The interconnected bulk electric systems are becoming integrated with vast networked information systems. This article discusses the skills that will be needed by future power engineers to keep pace with these developments and trends", "label": ["electric power industry deregulation", "computer applications", "electricity industry restructuring", "technology changes", "interconnected bulk electric systems", "networked information systems"], "stemmed_label": ["electr power industri deregul", "comput applic", "electr industri restructur", "technolog chang", "interconnect bulk electr system", "network inform system"]}
{"doc": "Evolution of the high-end computing market in the USA This paper focuses on the technological change in the high-end computing market. The discussion combines historical analysis with strategic analysis to provide a framework to analyse a key component of the computer industry. This analysis begins from the perspective of government research and development spending; then examines the confusion around the evolution of the high-end computing market in the context of standard theories of technology strategy and new product innovation. Rather than the high-end market being 'dead', one should view the market as changing due to increased capability and competition from the low-end personal computer market. The high-end market is also responding to new product innovation from the introduction of new parallel computing architectures. In the conclusion, key leverage points in the market are identified and the trends in high-end computing are highlighted with implications", "label": ["high-end computing market evolution", "usa", "historical analysis", "strategic analysis", "computer industry", "government research", "development spending", "technology strategy", "new product innovation", "competition", "low-end personal computer market", "parallel computing architectures", "supercomputing"], "stemmed_label": ["high-end comput market evolut", "usa", "histor analysi", "strateg analysi", "comput industri", "govern research", "develop spend", "technolog strategi", "new product innov", "competit", "low-end person comput market", "parallel comput architectur", "supercomput"]}
{"doc": "Inverse problems for a mathematical model of ion exchange in a compressible ion exchanger A mathematical model of ion exchange is considered, allowing for ion exchanger compression in the process of ion exchange. Two inverse problems are investigated for this model, unique solvability is proved, and numerical solution methods are proposed. The efficiency of the proposed methods is demonstrated by a numerical experiment", "label": ["inverse problems", "mathematical model", "ion exchange", "compressible ion exchanger", "ion exchanger compression", "unique solvability", "numerical solution methods"], "stemmed_label": ["invers problem", "mathemat model", "ion exchang", "compress ion exchang", "ion exchang compress", "uniqu solvabl", "numer solut method"]}
{"doc": "On-line Homework/Quiz/Exam applet: freely available Java software for evaluating performance on line The Homework/Quiz/Exam applet is a freely available Java program that can be used to evaluate student performance on line for any content authored by a teacher. It has database connectivity so that student scores are automatically recorded. It allows several different types of questions. Each question can be linked to images and detailed story problems. Three levels of feedback are provided to student responses. It allows teachers to randomize the sequence of questions and to randomize which of several options is the correct answer in multiple-choice questions. The creation and editing of questions involves menu selections, button presses, and the typing of content; no programming knowledge is required. The code is open source in order to encourage modifications that will meet individual pedagogical needs", "label": ["online homework/quiz/exam applet", "freely available java software", "online student performance evaluation", "teacher authored content", "database connectivity", "automatic student score recording", "images", "detailed story problems", "feedback", "randomized question sequence", "multiple-choice questions", "question editing", "question creation", "menu selections", "button presses", "typing content", "individual pedagogical needs"], "stemmed_label": ["onlin homework/quiz/exam applet", "freeli avail java softwar", "onlin student perform evalu", "teacher author content", "databas connect", "automat student score record", "imag", "detail stori problem", "feedback", "random question sequenc", "multiple-choic question", "question edit", "question creation", "menu select", "button press", "type content", "individu pedagog need"]}
{"doc": "Diagnosis of the technical state of heat systems A step-by-step approach to the diagnosis of the technical state of heat systems is stated. The class of physical defects is supplemented by the behavioral defects of objects, which are related to the disturbance of the modes of their operation. The implementation of the approach is illustrated by an example of the solution of a specific problem of the diagnosis of a closed heat consumption system", "label": ["heat system technical state diagnosis", "step-by-step diagnosis", "operational mode disturbance", "closed heat consumption system diagnosis"], "stemmed_label": ["heat system technic state diagnosi", "step-by-step diagnosi", "oper mode disturb", "close heat consumpt system diagnosi"]}
{"doc": "Presentation media, information complexity, and learning outcomes Multimedia computing provides a variety of information presentation modality combinations. Educators have observed that visuals enhance learning which suggests that multimedia presentations should be superior to text-only and text with static pictures in facilitating optimal human information processing and, therefore, comprehension. The article reports the findings from a 3 (text-only, overhead slides, and multimedia presentation)*2 (high and low information complexity) factorial experiment. Subjects read a text script, viewed an acetate overhead slide presentation, or viewed a multimedia presentation depicting the greenhouse effect (low complexity) or photocopier operation (high complexity). Multimedia was superior to text-only and overhead slides for comprehension. Information complexity diminished comprehension and perceived presentation quality. Multimedia was able to reduce the negative impact of information complexity on comprehension and increase the extent of sustained attention to the presentation. These findings suggest that multimedia presentations invoke the use of both the verbal and visual working memory channels resulting in a reduction of the cognitive load imposed by increased information complexity. Moreover, multimedia superiority in facilitating comprehension goes beyond its ability to increase sustained attention; the quality and effectiveness of information processing attained (i.e., use of verbal and visual working memory) is also significant", "label": ["presentation media", "information complexity", "learning outcomes", "cognitive processing limitations", "human working memory", "verbal working memory channel", "visual working memory channel", "multimedia computing", "information presentation modality combinations", "educators", "multimedia presentations", "static pictures", "optimal human information processing", "overhead slides", "text script", "acetate overhead slide presentation", "multimedia presentation", "greenhouse effect", "photocopier operation", "cognitive load", "multimedia superiority", "sustained attention"], "stemmed_label": ["present media", "inform complex", "learn outcom", "cognit process limit", "human work memori", "verbal work memori channel", "visual work memori channel", "multimedia comput", "inform present modal combin", "educ", "multimedia present", "static pictur", "optim human inform process", "overhead slide", "text script", "acet overhead slide present", "multimedia present", "greenhous effect", "photocopi oper", "cognit load", "multimedia superior", "sustain attent"]}
{"doc": "Pattern recognition strategies for molecular surfaces. I. Pattern generation using fuzzy set theory A new method for the characterization of molecules based on the model approach of molecular surfaces is presented. We use the topographical properties of the surface as well as the electrostatic potential, the local lipophilicity/hydrophilicity, and the hydrogen bond density on the surface for characterization. The definition and the calculation method for these properties are reviewed. The surface is segmented into overlapping patches with similar molecular properties. These patches can be used to represent the characteristic local features of the molecule in a way that is beyond the atomistic resolution but can nevertheless be applied for the analysis of partial similarities of different molecules as well as for the identification of molecular complementarity in a very general sense. The patch representation can be used for different applications, which will be demonstrated in subsequent articles", "label": ["pattern recognition strategies", "molecular surfaces", "pattern generation", "fuzzy set theory", "model approach", "topographical properties", "electrostatic potential", "local lipophilicity/hydrophilicity", "hydrogen bond density", "segmented surface", "overlapping patches", "molecular properties", "local features", "atomistic resolution", "partial similarities", "molecular complementarity", "patch representation", "lipophilicity", "hydrophilicity"], "stemmed_label": ["pattern recognit strategi", "molecular surfac", "pattern gener", "fuzzi set theori", "model approach", "topograph properti", "electrostat potenti", "local lipophilicity/hydrophil", "hydrogen bond densiti", "segment surfac", "overlap patch", "molecular properti", "local featur", "atomist resolut", "partial similar", "molecular complementar", "patch represent", "lipophil", "hydrophil"]}
{"doc": "Down up IT projects Despite the second quarter's gloomy GDP report, savvy CIOs are forging ahead with big IT projects that will position their companies to succeed when the economy soars again", "label": ["strategic technology projects", "walgreen", "ford", "caterpillar", "victoria's secret", "morgan stanley", "staples"], "stemmed_label": ["strateg technolog project", "walgreen", "ford", "caterpillar", "victoria' secret", "morgan stanley", "stapl"]}
{"doc": "Deriving model parameters from field test measurements generator control simulation A major component of any power system simulation is the generating plant. The purpose of DeriveAssist is to speed up the parameter derivation process and to allow engineers less versed in parameter matching and identification to get involved in the process of power plant electric generator modelling", "label": ["deriveassist", "parameter derivation process", "parameter matching", "parameter identification", "power system simulation", "turbine/governor", "power system stability analysis", "computer simulation", "generator parameter derivation process", "steady-state parameters derivation", "control simulation"], "stemmed_label": ["deriveassist", "paramet deriv process", "paramet match", "paramet identif", "power system simul", "turbine/governor", "power system stabil analysi", "comput simul", "gener paramet deriv process", "steady-st paramet deriv", "control simul"]}
{"doc": "Adaptive state feedback control for a class of linear systems with unknown bounds of uncertainties The problem of adaptive robust stabilization for a class of linear time-varying systems with disturbance and nonlinear uncertainties is considered. The bounds of the disturbance and uncertainties are assumed to be unknown, being even arbitrary. For such uncertain dynamical systems, the adaptive robust state feedback controller is obtained. And the resulting closed-loop systems are asymptotically stable in theory. Moreover, an adaptive robust state feedback control scheme is given. The scheme ensures the closed-loop systems exponentially practically stable and can be used in practical engineering. Finally, simulations show that the control scheme is effective", "label": ["robust stabilization", "adaptive stabilization", "linear time-varying systems", "nonlinear uncertainties", "closed-loop systems", "uncertain dynamical systems", "state feedback", "adaptive controller", "robust control", "uncertain systems"], "stemmed_label": ["robust stabil", "adapt stabil", "linear time-vari system", "nonlinear uncertainti", "closed-loop system", "uncertain dynam system", "state feedback", "adapt control", "robust control", "uncertain system"]}
{"doc": "A new method of systemological analysis coordinated with the procedure of object-oriented design. II For pt.I. see Vestn. KhGPU, no.81, p.15-18 (2000). The paper presents the results of development of an object-oriented systemological method used to design complex systems. A formal system representation, as well as an axiomatics of the calculus of systems as functional flow-type objects based on a Node-Function-Object class hierarchy are proposed. A formalized NFO/UFO analysis algorithm and CASE tools used to support it are considered", "label": ["systemological analysis", "object-oriented design", "complex systems design", "formal system representation", "axiomatics", "functional flow-type objects", "formalized nfo/ufo analysis algorithm", "case tools"], "stemmed_label": ["systemolog analysi", "object-ori design", "complex system design", "formal system represent", "axiomat", "function flow-typ object", "formal nfo/ufo analysi algorithm", "case tool"]}
{"doc": "Unsafe at any speed? While Sun prides itself on Java's secure sandbox programming model, Microsoft takes a looser approach. Its C# language incorporates C-like concepts, including pointers and memory management. But is unsafe code really a boon to programmers, or is it a step backward?", "label": ["microsoft c# language", "c-like concepts", "pointers", "memory management", "sun java secure sandbox programming model"], "stemmed_label": ["microsoft c# languag", "c-like concept", "pointer", "memori manag", "sun java secur sandbox program model"]}
{"doc": "Estimating long-range dependence: finite sample properties and confidence intervals A major issue in financial economics is the behavior of asset returns over long horizons. Various estimators of long-range dependence have been proposed. Even though some have known asymptotic properties, it is important to test their accuracy by using simulated series of different lengths. We test R/S analysis, detrended fluctuation analysis and periodogram regression methods on samples drawn from Gaussian white noise. The DFA statistics turns out to be the unanimous winner. Unfortunately, no asymptotic distribution theory has been derived for this statistics so far. We were able, however, to construct empirical (i.e. approximate) confidence intervals for all three methods. The obtained values differ largely from heuristic values proposed by some authors for the R/S statistics and are very close to asymptotic values for the periodogram regression method", "label": ["long-range dependence", "finite sample properties", "confidence intervals", "financial economics", "asset returns", "long horizons", "asymptotic properties", "detrended fluctuation analysis", "periodogram regression methods", "gaussian white noise", "heuristic values"], "stemmed_label": ["long-rang depend", "finit sampl properti", "confid interv", "financi econom", "asset return", "long horizon", "asymptot properti", "detrend fluctuat analysi", "periodogram regress method", "gaussian white nois", "heurist valu"]}
{"doc": "Nonlinear extrapolation algorithm for realization of a scalar random process A method of construction of a nonlinear extrapolation algorithm is proposed. This method makes it possible to take into account any nonlinear random dependences that exist in an investigated process and are described by mixed central moment functions. The method is based on the V. S. Pugachev canonical decomposition apparatus. As an example, the problem of nonlinear extrapolation is solved for a moment function of third order", "label": ["nonlinear extrapolation algorithm", "scalar random process", "nonlinear random dependences", "mixed central moment functions", "canonical decomposition apparatus", "moment function"], "stemmed_label": ["nonlinear extrapol algorithm", "scalar random process", "nonlinear random depend", "mix central moment function", "canon decomposit apparatu", "moment function"]}
{"doc": "Accelerated simulation of the steady-state availability of non-Markovian systems A general accelerated simulation method for evaluation of the steady-state availability of non-Markovian systems is proposed. It is applied to the investigation of a class of systems with repair. Numerical examples are given", "label": ["accelerated simulation", "steady-state availability", "non-markovian systems", "general accelerated simulation method", "numerical examples"], "stemmed_label": ["acceler simul", "steady-st avail", "non-markovian system", "gener acceler simul method", "numer exampl"]}
{"doc": "A new approach to the problem of structural identification. II The subject under discussion is a new approach to the problem of structural identification, which relies on the recognition of a decisive role of the human factor in the process of structural identification. Potential possibilities of the suggested approach are illustrated by the statement of a new mathematical problem of structural identification", "label": ["structural identification", "human factor", "mathematical equations", "decision-maker"], "stemmed_label": ["structur identif", "human factor", "mathemat equat", "decision-mak"]}
{"doc": "Comprehensive encoding and decoupling solution to problems of decoherence and design in solid-state quantum computing Proposals for scalable quantum computing devices suffer not only from decoherence due to the interaction with their environment, but also from severe engineering constraints. Here we introduce a practical solution to these major concerns, addressing solid-state proposals in particular. Decoherence is first reduced by encoding a logical qubit into two qubits, then completely eliminated by an efficient set of decoupling pulse sequences. The same encoding removes the need for single-qubit operations, which pose a difficult design constraint. We further show how the dominant decoherence processes can be identified empirically, in order to optimize the decoupling pulses", "label": ["solid-state quantum computing", "decoherence", "logical qubit encoding", "pulse sequence decoupling", "engineering constraints", "decoupling pulse optimization", "scalable quantum computing devices", "exchange hamiltonian"], "stemmed_label": ["solid-st quantum comput", "decoher", "logic qubit encod", "puls sequenc decoupl", "engin constraint", "decoupl puls optim", "scalabl quantum comput devic", "exchang hamiltonian"]}
{"doc": "Flexibility analysis of complex technical systems under uncertainty An important problem in designing technical systems under partial uncertainty of the initial physical, chemical, and technological data is the determination of a design in which the technical system is flexible, i.e., its control system is capable of guaranteeing that the constraints hold even under changes in external and internal factors and application of fuzzy mathematical models in its design. Three flexibility problems, viz., the flexibility of a technical system of given structure, structural flexibility of a technical system, and the optimal design guaranteeing the flexibility of a technical system, are studied. Two approaches to these problems are elaborated. Results of a computation experiment are given", "label": ["flexibility analysis", "complex technical systems", "partial uncertainty", "control system", "fuzzy mathematical models", "structural flexibility", "optimal design"], "stemmed_label": ["flexibl analysi", "complex technic system", "partial uncertainti", "control system", "fuzzi mathemat model", "structur flexibl", "optim design"]}
{"doc": "Nonlockability in multirings and hypercubes at serial transmission of data blocks For the multiring and hypercube, a method of conflictless realization of an arbitrary permutation of \"large\" data items that can be divided into many \"smaller\" data blocks was considered, and its high efficiency was demonstrated", "label": ["nonlockability", "multirings", "hypercubes", "data block serial transmission", "multiprocessor computer systems"], "stemmed_label": ["nonlock", "multir", "hypercub", "data block serial transmiss", "multiprocessor comput system"]}
{"doc": "Model predictive control helps to regulate slow processes-robust barrel temperature control Slow temperature control is a challenging control problem. The problem becomes even more challenging when multiple zones are involved, such as in barrel temperature control for extruders. Often, strict closed-loop performance requirements (such as fast startup with no overshoot and maintaining tight temperature control during production) are given for such applications. When characteristics of the system are examined, it becomes clear that a commonly used proportional plus integral plus derivative (PID) controller cannot meet such performance specifications for this kind of system. The system either will overshoot or not maintain the temperature within the specified range during the production run. In order to achieve the required performance, a control strategy that utilizes techniques such as model predictive control, autotuning, and multiple parameter PID is formulated. This control strategy proves to be very effective in achieving the desired specifications, and is very robust", "label": ["model predictive control", "slow processes regulation", "robust barrel temperature control", "extruders", "autotuning", "multiple parameter pid"], "stemmed_label": ["model predict control", "slow process regul", "robust barrel temperatur control", "extrud", "autotun", "multipl paramet pid"]}
{"doc": "Debugging Web applications The author considers how one can save time tracking down bugs in Web-based applications by arming yourself with the right tools and programming practices. A wide variety of debugging tools have been written with Web developers in mind", "label": ["web application debugging tools", "programming"], "stemmed_label": ["web applic debug tool", "program"]}
{"doc": "Knowledge-based structures and organisational commitment Organisational commitment, the emotional attachment of an employee to the employing organisation, has attracted a substantial body of literature, relating the concept to various antecedents, including organisational structure, and to a range of consequences, including financially important performance factors such as productivity and staff turnover. The new areas of knowledge management and learning organisations offer substantial promise as imperatives for the organisation of business enterprises. As organisations in the contemporary environment adopt knowledge-based structures to improve their competitive position, there is value in examining these structures against other performance related factors. Theoretical knowledge-based structures put forward by R. Miles et al. (1997) and J. Quinn et al. (1996) and an existing implementation are examined to determine common features inherent in these approaches. These features are posited as a typical form and their impact on organisational commitment and hence on individual and organisational performance is examined", "label": ["knowledge-based structures", "emotional attachment", "performance factors", "productivity", "staff turnover", "earning organisations", "organisational commitment"], "stemmed_label": ["knowledge-bas structur", "emot attach", "perform factor", "product", "staff turnov", "earn organis", "organis commit"]}
{"doc": "Perspectives on scholarly online books: the Columbia University Online Books Evaluation Project The Online Books Evaluation Project at Columbia University studied the potential for scholarly online books from 1995 to 1999. Issues included scholars' interest in using online books, the role they might play in scholarly life, features that scholars and librarians sought in online books, the costs of producing and owning print and online books, and potential marketplace arrangements. Scholars see potential for online books to make their research, learning, and teaching more efficient and effective. Librarians see potential to serve their scholars better. Librarians may face lower costs if they can serve their scholars with online books instead of print books. Publishers may be able to offer scholars greater opportunities to use their books while enhancing their own profitability", "label": ["columbia university online books evaluation project", "scholarly online books", "print books", "costs", "marketplace arrangements", "research", "learning"], "stemmed_label": ["columbia univers onlin book evalu project", "scholarli onlin book", "print book", "cost", "marketplac arrang", "research", "learn"]}
{"doc": "Social percolation and the influence of mass media In the marketing model of Solomon and Weisbuch, people buy a product only if their neighbours tell them of its quality, and if this quality is higher than their own quality expectations. Now we introduce additional information from the mass media, which is analogous to the ghost field in percolation theory. The mass media shift the percolative phase transition observed in the model, and decrease the time after which the stationary state is reached", "label": ["social percolation", "mass media influence", "solomon-weisbuch marketing model", "quality expectations", "ghost field", "percolative phase transition", "stationary state", "customers", "cinema", "external field"], "stemmed_label": ["social percol", "mass media influenc", "solomon-weisbuch market model", "qualiti expect", "ghost field", "percol phase transit", "stationari state", "custom", "cinema", "extern field"]}
{"doc": "Adaptive and efficient mutual exclusion The paper presents adaptive algorithms for mutual exclusion using only read and write operations; the performance of the algorithms depends only on the point contention, i.e., the number of processes that are concurrently active during algorithm execution (and not on n, the total number of processes). Our algorithm has O(k) remote step complexity and O(log k) system response time, where k is the point contention. The remote step complexity is the maximal number of steps performed by a process where a wait is counted as one step. The system response time is the time interval between subsequent entries to the critical section, where one time unit is the minimal interval in which every active process performs at least one step. The space complexity of this algorithm is O(N log n), where N is the range of process names. We show how to make the space complexity of our algorithm depend solely on n, while preserving the other performance measures of the algorithm", "label": ["adaptive mutual exclusion", "adaptive algorithms", "read operations", "write operations", "point contention", "algorithm execution", "remote step complexity", "system response time", "critical section", "minimal interval", "active process", "space complexity", "performance measures"], "stemmed_label": ["adapt mutual exclus", "adapt algorithm", "read oper", "write oper", "point content", "algorithm execut", "remot step complex", "system respons time", "critic section", "minim interv", "activ process", "space complex", "perform measur"]}
{"doc": "Integration is key - an introduction to enterprise application integration (EAI) technology Over the past few years, numerous organisations have invested in the latest software applications to drive their business forward. But many are now finding that these systems are becoming redundant on their own. The key to staying ahead of the competition in today's current climate is now to integrate all of these systems, says Justin Opie, Portfolio Director at Imark Communications", "label": ["enterprise application integration", "imark communications"], "stemmed_label": ["enterpris applic integr", "imark commun"]}
{"doc": "Modeling daily realized futures volatility with singular spectrum analysis Using singular spectrum analysis (SSA), we model the realized volatility and logarithmic standard deviations of two important futures return series. The realized volatility and logarithmic standard deviations are constructed following the methodology of Andersen et al. J. Am. Stat. Ass. 96 (2001) 42-55 using intra-day transaction data. We find that SSA decomposes the volatility series quite well and effectively captures both the market trend (accounting for about 34-38 of the total variance in the series) and, more importantly, a number of underlying market periodicities. Reliable identification of any periodicities is extremely important for options pricing and risk management and we believe that SSA can be a useful addition to the financial practitioners' toolbox", "label": ["daily realized futures volatility", "singular spectrum analysis", "ssa", "logarithmic standard deviations", "return series", "intraday transaction data", "market trend", "market periodicities", "risk management", "options pricing", "financial practitioners", "econophysics", "asset return"], "stemmed_label": ["daili realiz futur volatil", "singular spectrum analysi", "ssa", "logarithm standard deviat", "return seri", "intraday transact data", "market trend", "market period", "risk manag", "option price", "financi practition", "econophys", "asset return"]}
{"doc": "Compatibility of systems of linear constraints over the set of natural numbers Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types", "label": ["linear constraints", "set of natural numbers", "linear diophantine equations", "strict inequations", "nonstrict inequations", "upper bounds", "minimal generating sets"], "stemmed_label": ["linear constraint", "set of natur number", "linear diophantin equat", "strict inequ", "nonstrict inequ", "upper bound", "minim gener set"]}
{"doc": "Much ado about nothing: Win32.Perrun JPEG files do not contain any executable code and it is impossible to infect such files. The author takes a look at the details surrounding the Win32.Perrun virus and make clear exactly what it does. The main virus feature is its ability to affect JPEG image files (compressed graphic images) and to spread via affected JPEG files. The virus affects, or modifies, or alters JPEG files but does not \"infect\" them", "label": ["win32.perrun", "jpeg files", "virus", "compressed graphic images"], "stemmed_label": ["win32.perrun", "jpeg file", "viru", "compress graphic imag"]}
{"doc": "Performance comparison between PID and dead-time compensating controllers This paper is intended to answer the question: \"When can a simple dead-time compensator be expected to perform better than a PID?\". The performance criterion used is the integrated absolute error (IAE). It is compared for PI and PID controllers and a simple dead-time compensator (DTC) when a step load disturbance is applied at the plant input. Both stable and integrating processes are considered. For a fair comparison the controllers should provide equal robustness in some sense. Here, as a measure of robustness, the H/sub infinity / norm of the sum of the absolute values of the sensitivity function and the complementary sensitivity function is used. Performance of the DTC's is given also as a function of dead-time margin (D/sub M/)", "label": ["performance comparison", "pid controllers", "dead-time compensating controllers", "performance criterion", "integrated absolute error", "iae", "pi controllers", "dead-time compensator", "dtc", "step load disturbance", "stable processes", "integrating processes", "equal robustness", "complementary sensitivity function", "dead-time margin", "absolute value sum h/sub infinity / norm"], "stemmed_label": ["perform comparison", "pid control", "dead-tim compens control", "perform criterion", "integr absolut error", "iae", "pi control", "dead-tim compens", "dtc", "step load disturb", "stabl process", "integr process", "equal robust", "complementari sensit function", "dead-tim margin", "absolut valu sum h/sub infin / norm"]}
{"doc": "A friction compensator for pneumatic control valves A procedure that compensates for static friction (stiction) in pneumatic control valves is presented. The compensation is obtained by adding pulses to the control signal. The characteristics of the pulses are determined from the control action. The compensator is implemented in industrial controllers and control systems, and the industrial experiences show that the procedure reduces the control error during stick-slip motion significantly compared to standard control without stiction compensation", "label": ["friction compensator", "pneumatic control valves", "static friction compensation", "stiction compensation", "industrial controllers", "control error reduction", "stick-slip motion", "standard control"], "stemmed_label": ["friction compens", "pneumat control valv", "static friction compens", "stiction compens", "industri control", "control error reduct", "stick-slip motion", "standard control"]}
{"doc": "Alien Rescue: a problem-based hypermedia learning environment for middle school science The article describes an innovative hypermedia product for sixth graders in space science: Alien Rescue. Using a problem-based learning approach that is highly interactive, Alien Rescue engages students in scientific investigations aimed at finding solutions to complex and meaningful problems. Problem-based learning (PBL) is an instructional strategy proven to be effective in medical and business fields, and it is increasingly popular in education. However, using PBL in K-12 classrooms is challenging and requires access to rich knowledge bases and cognitive tools. Alien Rescue is designed to provide such cognitive support for successful use of PBL in sixth-grade classrooms. The design and development of Alien Rescue is guided by current educational research. Research is an integral part of this project. Results of formative evaluation and research studies are being integrated into the development and improvement of the program. Alien Rescue is designed in accordance with the National Science Standards and the Texas Essential Knowledge and Skills (TEKS) for science. So far Alien Rescue has been field-tested by approximately 1400 sixth graders. More use in middle schools is in progress and more research on its use is planned", "label": ["alien rescue", "problem-based hypermedia learning environment", "middle school science", "space science", "sixth graders", "scientific investigations", "pbl", "instructional strategy", "medical fields", "business fields", "k-12 classrooms", "rich knowledge bases", "cognitive tools", "cognitive support", "educational research", "formative evaluation", "middle schools"], "stemmed_label": ["alien rescu", "problem-bas hypermedia learn environ", "middl school scienc", "space scienc", "sixth grader", "scientif investig", "pbl", "instruct strategi", "medic field", "busi field", "k-12 classroom", "rich knowledg base", "cognit tool", "cognit support", "educ research", "form evalu", "middl school"]}
{"doc": "Managing system risk Companies are increasingly required to provide assurance that their systems are secure and conform to commercial security standards. Senior business managers are ultimately responsible for the security of their corporate systems and for the implications in the event of a failure. Businesses will be exposed to unquantified security risks unless they have a formal risk management framework in place to enable risks to be identified, evaluated and managed. Failure to assess and manage risks can lead to a business suffering serious financial impacts, commercial embarrassment and fines or sanctions from regulators. This is both a key responsibility and opportunity for Management Services Practitioners", "label": ["risk management framework", "commercial security standards", "it projects"], "stemmed_label": ["risk manag framework", "commerci secur standard", "it project"]}
{"doc": "Multidimensional data visualization Historically, data visualization has been limited primarily to two dimensions (e.g., histograms or scatter plots). Available software packages (e.g., Data Desk 6.1, MatLab 6.1, SAS-JMP 4.04, SPSS 10.0) are capable of producing three-dimensional scatter plots with (varying degrees of) user interactivity. We constructed our own data visualization application with the Visualization Toolkit (Schroeder et al., 1998) and Tcl/Tk to display multivariate data through the application of glyphs (Ware, 2000). A glyph is a visual object onto which many data parameters may be mapped, each with a different visual attribute (e.g., size or color). We used our multi-dimensional data viewer to explore data from several psycholinguistic experiments. The graphical interface provides flexibility when users dynamically explore the multidimensional image rendered from raw experimental data. We highlight advantages of multidimensional data visualization and consider some potential limitations", "label": ["multidimensional data visualization", "3d scatter plots", "user interactivity", "visualization toolkit", "tcl/tk", "multivariate data display", "glyphs", "visual object", "data parameters", "visual attribute", "multi-dimensional data viewer", "psycholinguistic experiments", "graphical interface", "multidimensional image rendering"], "stemmed_label": ["multidimension data visual", "3d scatter plot", "user interact", "visual toolkit", "tcl/tk", "multivari data display", "glyph", "visual object", "data paramet", "visual attribut", "multi-dimension data viewer", "psycholinguist experi", "graphic interfac", "multidimension imag render"]}
{"doc": "Synchronizing experiments with linear interval systems Concerns generalized control problems without exact information. P A method of constructing a minimal synchronizing sequence for a linear interval system over the field of real numbers is developed. This problem is reduced to a system of linear inequalities", "label": ["synchronizing experiments", "linear interval systems", "minimal synchronizing sequence construction", "real numbers", "linear inequalities", "generalized control problems", "controllability"], "stemmed_label": ["synchron experi", "linear interv system", "minim synchron sequenc construct", "real number", "linear inequ", "gener control problem", "control"]}
{"doc": "Evolution of litigation support systems For original paper see ibid., vol. 12, no. 6: \"The E-mail of the Species\". The author responds to that paper and argues that printing, scanning and imaging E-mails or other electronic (rather than paper) documents prior to listing and disclosure seems to be unnecessary, not 'proportionate' (from a costs point of view) and not particularly helpful, to either side. He asks how litigation support systems might evolve to help and support the legal team in their task", "label": ["litigation support systems", "e-mail", "legal team"], "stemmed_label": ["litig support system", "e-mail", "legal team"]}
{"doc": "Electronic signatures - much ado? Whilst the market may be having a crisis of confidence regarding the prospects for e-commerce, the EU and the Government continue apace to develop the legal framework. Most recently, this has resulted in the Electronic Signatures Regulations 2002. These Regulations were made on 13 February 2002 and came into force on 8 March 2002. The Regulations implement the European Electronic Signatures Directive (1999/93/EC). Critics may say that the Regulations were implemented too late (they were due to have been implemented by 19 July 2001), with too short a consultation period (25 January 2002 to 12 February 2002) and with an unconvincing case as to what they add to English law (as to which, read on). The author explains the latest development on e-signatures and the significance of Certification Service Providers (CSPs)", "label": ["e-commerce", "legal framework", "electronic signatures regulations 2002", "european electronic signatures directive"], "stemmed_label": ["e-commerc", "legal framework", "electron signatur regul 2002", "european electron signatur direct"]}
{"doc": "A PID standard: What, why, how? The paper is written for all who develop and use P&IDs. It will aid in solving the long existing and continuing problem of confusing information on P&IDs. The acronym P&ID is widely understood to mean the principal document used to define the details of how a process works and how it is controlled. The ISA Dictionary definition for P&ID tells what they do, \"show the interconnection of process equipment and the instrumentation used to control the process. In the process industry a standard set of symbols is used to prepare drawings of processes. The instrument symbols used in these drawings are generally based on ISA-S5.1.\" In the paper the ISA standard is referred to as ISA-5.1. The article develops the concept of the \"standard\" and poses some of the questions that the \"standard\" can answer", "label": ["p&id standard", "principal document", "process controlled", "isa-5.1", "isa standard"], "stemmed_label": ["p&id standard", "princip document", "process control", "isa-5.1", "isa standard"]}
{"doc": "Four factors influencing the fair market value of out-of-print books.1 Four factors (edition, condition, dust jacket, and autograph) that are hypothesized to influence the value of books are identified and linked to basic economic principles, which are explained. A sample of fifty-six titles is qualitatively examined to test the hypothesis", "label": ["fair market value", "out-of-print books", "economic principles", "pricing"], "stemmed_label": ["fair market valu", "out-of-print book", "econom principl", "price"]}
{"doc": "A fuzzy logic adaptation circuit for control systems of deformable space vehicles: its design A fuzzy-logic adaptation algorithm is designed for adjusting the discreteness period of a control system for ensuring the stability and quality of control process with regard to the elastic structural vibrations of a deformable space vehicle. Its performance is verified by digital modeling of a discrete control system with two objects", "label": ["fuzzy logic adaptation circuit", "control systems", "deformable space vehicles", "discreteness period", "stability", "elastic structural vibrations", "digital modeling"], "stemmed_label": ["fuzzi logic adapt circuit", "control system", "deform space vehicl", "discret period", "stabil", "elast structur vibrat", "digit model"]}
{"doc": "Brightness-independent start-up routine for star trackers Initial attitude acquisition by a modern star tracker is investigated here. Criteria for efficient organization of the on-board database are discussed with reference to a brightness-independent initial acquisition algorithm. Star catalog generation preprocessing is described, with emphasis on the identification of minimum star brightness for detection by a sensor based on a charge coupled device (CCD) photodetector. This is a crucial step for proper evaluation of the attainable sky coverage when selecting the stars to be included in the on-board catalog. Test results are also reported, both for reliability and accuracy, even if the former is considered to be the primary target. Probability of erroneous solution is 0.2 in the case of single runs of the procedure, while attitude determination accuracy is in the order of 0.02 degrees in the average for the computation of the inertial pointing of the boresight axis", "label": ["brightness-independent start-up routine", "star trackers", "initial attitude acquisition", "on-board database", "star catalog generation preprocessing", "gyroless spacecraft", "minimum star brightness", "charge coupled device photodetector", "reliability", "boresight axis"], "stemmed_label": ["brightness-independ start-up routin", "star tracker", "initi attitud acquisit", "on-board databas", "star catalog gener preprocess", "gyroless spacecraft", "minimum star bright", "charg coupl devic photodetector", "reliabl", "boresight axi"]}
{"doc": "Labscape: a smart environment for the cell biology laboratory Labscape is a smart environment that we designed to improve the experience of people who work in a cell biology laboratory. Our goal in creating it was to simplify, laboratory work by making information available where it is needed and by collecting and organizing data where and when it is created into a formal representation that others can understand and process. By helping biologists produce a more complete record of their work with less effort, Labscape is designed to foster improved collaboration in conjunction with increased individual efficiency and satisfaction. A user-driven system, although technologically conservative, embraces a central goal of ubiquitous computing: to enhance the ability to perform domain tasks through fluid interaction with computational resources. Smart environments could soon replace the pen and paper commonly used in the laboratory setting", "label": ["cell biology", "labscape", "laboratory work", "ubiquitous computing", "smart environment", "experimental technologies", "biochemical procedure"], "stemmed_label": ["cell biolog", "labscap", "laboratori work", "ubiquit comput", "smart environ", "experiment technolog", "biochem procedur"]}
{"doc": "Online auctions: dynamic pricing and the lodging industry The traditional channels of distribution for overnight accommodation are rapidly being displaced by Web site scripting, online intermediaries, and specialty brokers. Businesses that pioneered Internet usage relied on it as a sales and marketing alternative to predecessor product distribution channels. As such, Web sites replace the traditional trading model to the Internet. Web-enabled companies are popular because the medium renders the process faster, less costly, highly reliable, and secure. Auction-based models impact business models by converting the price setting mechanism from supplier-centric to market-centric and transforming the trading model from \"one to many\" to \"many to many.\" Historically, pricing was based on the cost of production plus a margin of profit. Traditionally, as products and services move through the supply chain, from the producer to the consumer, various intermediaries added their share of profit to the price. As Internet based mediums of distribution become more prevalent, traditional pricing models are being supplanted with dynamic pricing. A dynamic pricing model represents a flexible system that changes prices not only from product to product, but also from customer to customer and transaction to transaction. Many industry leaders are skeptical of the long run impact of online auctions on lodging industry profit margins, despite the fact pricing theory suggests that an increase in the flow of information results in efficient market pricing. The future of such endeavors remains promising, but controversial", "label": ["online auctions", "dynamic pricing", "lodging industry", "overnight accommodations", "web site scripting", "online intermediaries", "specialty brokers", "internet usage", "sales", "marketing", "trading model", "business models", "price setting mechanism", "supply chain"], "stemmed_label": ["onlin auction", "dynam price", "lodg industri", "overnight accommod", "web site script", "onlin intermediari", "specialti broker", "internet usag", "sale", "market", "trade model", "busi model", "price set mechan", "suppli chain"]}
{"doc": "Design of PID-type controllers using multiobjective genetic algorithms The design of a PID controller is a multiobjective problem. A plant and a set of specifications to be satisfied are given. The designer has to adjust the parameters of the PID controller such that the feedback interconnection of the plant and the controller satisfies the specifications. These specifications are usually competitive and any acceptable solution requires a tradeoff among them. An approach for adjusting the parameters of a PID controller based on multiobjective optimization and genetic algorithms is presented in this paper. The MRCD (multiobjective robust control design) genetic algorithm has been employed. The approach can be easily generalized to design multivariable coupled and decentralized PID loops and has been successfully validated for a large number of experimental cases", "label": ["pid-type controllers", "multiobjective genetic algorithms", "feedback interconnection", "multiobjective robust control design", "multivariable coupled pid loops", "decentralized pid loops", "tuning methods"], "stemmed_label": ["pid-typ control", "multiobject genet algorithm", "feedback interconnect", "multiobject robust control design", "multivari coupl pid loop", "decentr pid loop", "tune method"]}
{"doc": "Robust wavelet neuro control for linear brushless motors Design, simulation and experimental implementation of a wavelet basis function network learning controller for linear brushless dc motors (LBDCM) are considered. Stability robustness with position tracking is the primary concern. The proposed controller deals mainly with external disturbances, e.g. nonlinear friction force and payload variation in motion control of linear motors. It consists of two parts, one is a state feedback component, and the other one is a learning feedback component. The state feedback controller is designed on the basis of a simple linear model, and the learning feedback component is a wavelet neural controller. The attenuation effect of wavelet neural networks on friction force is first verified by the numerical method. The learning effect of wavelet neural networks on friction force is also shown in the numerical results. Then, a wavelet neural network is applied on a real LBDCM to on-line suppress the friction force, which may be variable due to the different lubrication. The effectiveness of the proposed control schemes is demonstrated by simulated and experimental results", "label": ["robust wavelet neuro control", "linear brushless motors", "wavelet basis function network", "lbdcm", "stability robustness", "position tracking", "external disturbances", "nonlinear friction force", "payload variation", "motion control", "state feedback component", "learning feedback component", "attenuation effect", "friction force", "lubrication"], "stemmed_label": ["robust wavelet neuro control", "linear brushless motor", "wavelet basi function network", "lbdcm", "stabil robust", "posit track", "extern disturb", "nonlinear friction forc", "payload variat", "motion control", "state feedback compon", "learn feedback compon", "attenu effect", "friction forc", "lubric"]}
{"doc": "An identity-based society oriented signature scheme with anonymous signers In this paper, we propose a new society oriented scheme, based on the Guillou-Quisquater (1989) signature scheme. The scheme is identity-based and the signatures are verified with respect to only one identity. That is, the verifier does not have to know the identity of the co-signers, but just that of the organization they represent", "label": ["identity-based society oriented signature scheme", "anonymous signers", "signature verification"], "stemmed_label": ["identity-bas societi orient signatur scheme", "anonym signer", "signatur verif"]}
{"doc": "Extinction cross sections of realistic raindrops: data-bank established using T-matrix method and nonlinear fitting technique A new computer program is developed based on the T-matrix method to generate a large number of total (extinction) cross sections (TCS) values of the realistic raindrops that are deformed due to a balance of the forces that act on a drop failing under gravity, and were described in shape by Pruppacher and Pitter (1971). These data for various dimensions of the raindrops (mean effective radius from 0 to 3.25 mm), frequencies (10 to 80 GHz), (horizontal and vertical) polarizations, and temperatures (0, 10 and 20 degrees C) are stored to establish a data bank. Furthermore, a curve fitting technique, i.e., interpolation of order 3, is implemented for the TCS values in the data bank. Therefore, the interpolated TCS results can be obtained readily from the interpolation process with negligible or even null computational time and efforts. Error analysis is carried out to show the high accuracy of the present analysis and applicability of the interpolation. At three operating frequencies of 15, 21.225, and 38 GHz locally used in Singapore, some new TCS values are obtained from the new fast and efficient interpolation with a good accuracy", "label": ["extinction cross sections", "realistic raindrops", "data-bank", "t-matrix method", "total cross sections", "temperature", "error analysis", "mean effective radius", "gravity", "horizontal polarization", "vertical polarization", "interpolation", "nonlinear curve fitting technique", "operating frequencies", "singapore", "shf", "ehf", "electromagnetic wave scattering", "em wave scattering", "computer program", "15 ghz", "21.225 ghz", "38 ghz", "10 to 80 ghz", "0 to 3.25 mm", "0 c", "10 c", "20 c"], "stemmed_label": ["extinct cross section", "realist raindrop", "data-bank", "t-matrix method", "total cross section", "temperatur", "error analysi", "mean effect radiu", "graviti", "horizont polar", "vertic polar", "interpol", "nonlinear curv fit techniqu", "oper frequenc", "singapor", "shf", "ehf", "electromagnet wave scatter", "em wave scatter", "comput program", "15 ghz", "21.225 ghz", "38 ghz", "10 to 80 ghz", "0 to 3.25 mm", "0 c", "10 c", "20 c"]}
{"doc": "CAD/CAE software aids converter design DC/DC power conversion Typically, power supply design involves electronic and magnetic components. In this paper, the authors describe, using a flyback converter example, how CAD/CAE tools can aid the power supply engineer in both areas, reducing prototyping costs and providing insights into system performance", "label": ["dc/dc power convertor design", "power supply design", "electronic components", "magnetic components", "cad/cae software", "flyback power convertor topology", "prototyping costs"], "stemmed_label": ["dc/dc power convertor design", "power suppli design", "electron compon", "magnet compon", "cad/ca softwar", "flyback power convertor topolog", "prototyp cost"]}
{"doc": "Complexity transitions in global algorithms for sparse linear systems over finite fields We study the computational complexity of a very basic problem, namely that of finding solutions to a very large set of random linear equations in a finite Galois field modulo q. Using tools from statistical mechanics we are able to identify phase transitions in the structure of the solution space and to connect them to the changes in the performance of a global algorithm, namely Gaussian elimination. Crossing phase boundaries produces a dramatic increase in memory and CPU requirements necessary for the algorithms. In turn, this causes the saturation of the upper bounds for the running time. We illustrate the results on the specific problem of integer factorization, which is of central interest for deciphering messages encrypted with the RSA cryptosystem", "label": ["complexity transitions", "global algorithms", "sparse linear systems", "finite fields", "random linear equations", "finite galois field", "statistical mechanics", "gaussian elimination", "phase boundaries", "integer factorization", "message deciphering", "encryption", "rsa cryptosystem", "disordered systems"], "stemmed_label": ["complex transit", "global algorithm", "spars linear system", "finit field", "random linear equat", "finit galoi field", "statist mechan", "gaussian elimin", "phase boundari", "integ factor", "messag deciph", "encrypt", "rsa cryptosystem", "disord system"]}
{"doc": "Project-based learning: teachers learning and using high-tech to preserve Cajun culture Using project-based learning pedagogy in EdTc 658 Advances in Educational Technology, the author has trained inservice teachers in Southwestern Louisiana with an advanced computer multimedia program called Director(R) (Macromedia, Inc.). The content of this course focused on modeling the project-based learning pedagogy and researching Acadian's traditions and legacy. With the multi-functions of microcomputers, new technologies were used to preserve and celebrate the local culture with superiority of text, graphics, animation, sound, and video. The article describes how several groups of school teachers in the surrounding areas of a regional state university of Louisiana learned computer multimedia using project-based learning and integrated their learning into local cultural heritage", "label": ["project-based learning", "teachers", "cajun culture", "project-based learning pedagogy", "edtc 658 advances in educational technology", "inservice teachers", "advanced computer multimedia program", "director", "acadian traditions", "macromedia", "new technologies", "local culture", "school teachers", "regional state university", "computer multimedia", "local cultural heritage"], "stemmed_label": ["project-bas learn", "teacher", "cajun cultur", "project-bas learn pedagogi", "edtc 658 advanc in educ technolog", "inservic teacher", "advanc comput multimedia program", "director", "acadian tradit", "macromedia", "new technolog", "local cultur", "school teacher", "region state univers", "comput multimedia", "local cultur heritag"]}
{"doc": "Solutions for cooperative games A new concept of the characteristic function is defined. It matches cooperative games far better than the classical characteristic function and is useful in reducing the number of decisions that can be used as the unique solution of a game", "label": ["cooperative games", "characteristic function", "decisions", "unique solution", "transferrable utility"], "stemmed_label": ["cooper game", "characterist function", "decis", "uniqu solut", "transferr util"]}
{"doc": "The eyes have it hotel security CCTV systems can help lodging establishments accomplish a range of objectives, from deterring criminals to observing staff interactions with clientele. But pitfalls can arise if the CCTV system has not been properly integrated into the overall hotel security plan. CCTV system designs at new hotel properties are often too sophisticated, too complicated, and too costly, and do not take into consideration the security realities of site management. These problems arise when the professionals designing or installing the system, including architects, construction engineers, integrators, and consultants, are not familiar with a hotel's operating strategies or security standards", "label": ["hotel security", "cctv system", "site management", "operating strategies"], "stemmed_label": ["hotel secur", "cctv system", "site manag", "oper strategi"]}
{"doc": "Binocular model for figure-ground segmentation in translucent and occluding images A Fourier-based solution to the problem of figure-ground segmentation in short baseline binocular image pairs is presented. Each image is modeled as an additive composite of two component images that exhibit a spatial shift due to the binocular parallax. The segmentation is accomplished by decoupling each Fourier component in one of the resultant additive images into its two constituent phasors, allocating each to its appropriate object-specific spectrum, and then reconstructing the foreground and background using the inverse Fourier transform. It is shown that the foreground and background shifts can be computed from the differences of the magnitudes and phases of the Fourier transform of the binocular image pair. While the model is based on translucent objects, it also works with occluding objects", "label": ["binocular model", "figure-ground segmentation", "translucent images", "occluding images", "images", "image segmentation", "fourier-based solution", "short baseline binocular image pairs", "component images", "spatial shift", "binocular parallax", "fourier component decoupling", "phasors", "object-specific spectrum", "foreground", "background", "inverse fourier transform", "binocular image pair", "translucent objects", "occluding objects"], "stemmed_label": ["binocular model", "figure-ground segment", "transluc imag", "occlud imag", "imag", "imag segment", "fourier-bas solut", "short baselin binocular imag pair", "compon imag", "spatial shift", "binocular parallax", "fourier compon decoupl", "phasor", "object-specif spectrum", "foreground", "background", "invers fourier transform", "binocular imag pair", "transluc object", "occlud object"]}
{"doc": "Student consulting projects benefit faculty and industry Student consulting projects require students to apply OR/MS tools to obtain insight into the activities of firms in the community. These projects benefit faculty by providing clear feedback on the real capabilities of students, a broad connection to local industry, and material for case studies and research. They benefit companies by stimulating new thinking regarding their activities and delivering results they can use. Projects provide insights into the end-user modeling mode of OR/MS practice. Projects support continuous improvement as the lessons gained from a crop of projects enable better teaching during the next course offering, which in turn leads to better projects and further insights into teaching", "label": ["student consulting projects", "or/ms tools", "student placements", "student capability feedback", "case study material"], "stemmed_label": ["student consult project", "or/m tool", "student placement", "student capabl feedback", "case studi materi"]}
{"doc": "Multispectral color image capture using a liquid crystal tunable filter We describe the experimental setup of a multispectral color image acquisition system consisting of a professional monochrome CCD camera and a tunable filter in which the spectral transmittance can be controlled electronically. We perform a spectral characterization of the acquisition system taking into account the acquisition noise. To convert the camera output signals to device-independent color data, two main approaches are proposed and evaluated. One consists in applying regression methods to convert from the K camera outputs to a device-independent color space such as CIEXYZ or CIELAB. Another method is based on a spectral model of the acquisition system. By inverting the model using a principal eigenvector approach, we estimate the spectral reflectance of each pixel of the imaged surface", "label": ["multispectral color image capture", "liquid crystal tunable filter", "multispectral color image acquisition system", "monochrome ccd camera", "tunable filter", "spectral transmittance", "spectral characterization", "acquisition system", "acquisition noise", "camera output signals", "device-independent color data", "regression methods", "camera outputs", "independent color space", "ciexyz", "cielab", "spectral model", "principal eigenvector approach", "spectral reflectance", "imaged surface", "pixel"], "stemmed_label": ["multispectr color imag captur", "liquid crystal tunabl filter", "multispectr color imag acquisit system", "monochrom ccd camera", "tunabl filter", "spectral transmitt", "spectral character", "acquisit system", "acquisit nois", "camera output signal", "device-independ color data", "regress method", "camera output", "independ color space", "ciexyz", "cielab", "spectral model", "princip eigenvector approach", "spectral reflect", "imag surfac", "pixel"]}
{"doc": "A method for solution of systems of linear algebraic equations with m-dimensional lambda -matrices A system of linear algebraic equations with m-dimensional lambda -matrices is considered. The proposed method of searching for the solution of this system lies in reducing it to a numerical system of a special kind", "label": ["linear algebraic equations", "numerical system", "m-dimensional lambda -matrices"], "stemmed_label": ["linear algebra equat", "numer system", "m-dimension lambda -matric"]}
{"doc": "On batch-constructing B/sup +/-trees: algorithm and its performance evaluation Efficient construction of indexes is very important in bulk-loading a database or adding a new index to an existing database since both of them should handle an enormous volume of data. In this paper, we propose an algorithm for batch-constructing the B/sup +/-tree, the most widely used index structure in database systems. The main characteristic of our algorithm is to simultaneously process all the key values to be placed on each B+-tree page when accessing the page. This avoids the overhead due to accessing the same page multiple times, which results from applying the B+-tree insertion algorithm repeatedly. For performance evaluation, we have analyzed our algorithm in terms of the number of disk accesses. The results show that the number of disk accesses excluding those in the relocation process is identical to the number of pages belonging to the B/sup +/-tree. Considering that the relocation process is an unavoidable preprocessing step for batch-constructing of B/sup +/-trees, our algorithm requires just one disk access per B+-tree page, and therefore turns out to be optimal. We also present the performance tendency in relation with different parameter values via simulation. Finally, we show the performance enhancement effect of our algorithm, compared with the one using repeated insertions through experiments", "label": ["b+-tree batch construction", "algorithm performance evaluation", "database bulk loading", "index structure", "b+-tree page", "page access", "b+-tree insertion algorithm", "disk accesses", "relocation process", "simulation"], "stemmed_label": ["b+-tree batch construct", "algorithm perform evalu", "databas bulk load", "index structur", "b+-tree page", "page access", "b+-tree insert algorithm", "disk access", "reloc process", "simul"]}
{"doc": "Note on \"Deterministic inventory lot-size models under inflation with shortages and deterioration for fluctuating demand\" by Yang et al For original paper see H.-L. Yang et al., ibid., vol.48, p.144-58 (2001). Yang et al. extended the lot-size models to allow for inflation and fluctuating demand. For this model they proved that the optimal replenishment schedule exists and is unique. They also proposed an algorithm to find the optimal policy. The present paper provides examples, which show that the optimal replenishment schedule and consequently the overall optimal policy may not exist", "label": ["deterministic inventory lot-size models", "inflation", "fluctuating demand", "optimal replenishment schedule", "optimal policy algorithm", "optimal scheduling parameters"], "stemmed_label": ["determinist inventori lot-siz model", "inflat", "fluctuat demand", "optim replenish schedul", "optim polici algorithm", "optim schedul paramet"]}
{"doc": "Cyberobscenity and the ambit of English criminal law The author looks at a recent case and questions the Court of Appeal's approach. In the author's submission, the Court of Appeal's decision in Perrin was wrong. P published no material in England and Wales, and should not have been convicted of any offence under English law, even if it were proved that he sought to attract English subscribers to his site. That may be an unpalatable conclusion but, if the content of foreign-hosted Internet sites is to be controlled, the only sensible way forward is through international agreement and cooperation. The Council of Europe's Cybercrime Convention provides some indication of the limited areas over which widespread international agreement might be achieved", "label": ["cyberobscenity", "criminal law", "court of appeal", "internet sites", "cybercrime convention", "council of europe", "international agreement", "england"], "stemmed_label": ["cyberobscen", "crimin law", "court of appeal", "internet site", "cybercrim convent", "council of europ", "intern agreement", "england"]}
{"doc": "Automation of the recovery of efficiency of complex structure systems Basic features are set forth of the method for automation of the serviceability recovery of systems of complex structures in real time without the interruption of operation. Specific features of the method are revealed in an important example of the system of control of hardware components of ships", "label": ["efficiency recovery", "serviceability recovery", "complex structure systems", "ships", "hardware components"], "stemmed_label": ["effici recoveri", "servic recoveri", "complex structur system", "ship", "hardwar compon"]}
{"doc": "IT as a key enabler to law firm competitiveness Professional services firms have traditionally been able to thrive in virtually any market conditions. They have been consistently successful for several decades without ever needing to reexamine or change their basic operating model. However, gradual but inexorable change in client expectations and the business environment over recent years now means that more of the same is no longer enough. In future, law firms will increasingly need to exploit IT more effectively in order to remain competitive. To do this, they will need to ensure that all their information systems function as an integrated whole and are available to their staff, clients and business partners. The authors set out the lessons to be learned for law firms in the light of the recent PA Consulting survey", "label": ["professional services firms", "client expectations", "business environment", "information systems", "law firms"], "stemmed_label": ["profession servic firm", "client expect", "busi environ", "inform system", "law firm"]}
{"doc": "Design and implementation of a brain-computer interface with high transfer rates This paper presents a brain-computer interface (BCI) that can help users to input phone numbers. The system is based on the steady-state visual evoked potential (SSVEP). Twelve buttons illuminated at different rates were displayed on a computer monitor. The buttons constituted a virtual telephone keypad, representing the ten digits 0-9, BACKSPACE, and ENTER. Users could input phone number by gazing at these buttons. The frequency-coded SSVEP was used to judge which button the user desired. Eight of the thirteen subjects succeeded in ringing the mobile phone using the system. The average transfer rate over all subjects was 27.15 bits/min. The attractive features of the system are noninvasive signal recording, little training required for use, and high information transfer rate. Approaches to improve the performance of the system are discussed", "label": ["brain-computer interface with high transfer rates", "phone numbers input", "steady-state visual evoked potential", "illuminated buttons", "system performance improvement", "virtual telephone keypad", "frequency-coded ssvep", "mobile phone ringing", "computer monitor"], "stemmed_label": ["brain-comput interfac with high transfer rate", "phone number input", "steady-st visual evok potenti", "illumin button", "system perform improv", "virtual telephon keypad", "frequency-cod ssvep", "mobil phone ring", "comput monitor"]}
{"doc": "Global stability of the attracting set of an enzyme-catalysed reaction system The essential feature of enzymatic reactions is a nonlinear dependency of reaction rate on metabolite concentration taking the form of saturation kinetics. Recently, it has been shown that this feature is associated with the phenomenon of \"loss of system coordination\" (Liu, 1999). In this paper, we study a system of ordinary differential equations representing a branched biochemical system of enzyme-mediated reactions. We show that this system can become very sensitive to changes in certain maximum enzyme activities. In particular, we show that the system exhibits three distinct responses: a unique, globally-stable steady-state, large amplitude oscillations, and asymptotically unbounded solutions, with the transition between these states being almost instantaneous. It is shown that the appearance of large amplitude, stable limit cycles occurs due to a \"false\" bifurcation or canard explosion. The subsequent disappearance of limit cycles corresponds to the collapse of the domain of attraction of the attracting set for the system and occurs due to a global bifurcation in the flow, namely, a saddle connection. Subsequently, almost all nonnegative data become unbounded under the action of the dynamical system and correspond exactly to loss of system coordination. We discuss the relevance of these results to the possible consequences of modulating such systems", "label": ["enzymatic reactions", "nonlinear dependency", "metabolite concentration", "saturation kinetics", "biochemical system", "ordinary differential equations", "enzyme-mediated reactions", "saddle connection", "stable limit cycles", "bifurcation"], "stemmed_label": ["enzymat reaction", "nonlinear depend", "metabolit concentr", "satur kinet", "biochem system", "ordinari differenti equat", "enzyme-medi reaction", "saddl connect", "stabl limit cycl", "bifurc"]}
{"doc": "A Web-accessible database of characteristics of the 1,945 basic Japanese kanji In 1981, the Japanese government published a list of the 1,945 basic Japanese kanji (Jooyoo Kanji-hyo), including specifications of pronunciation. This list was established as the standard for kanji usage in print. The database for 1,945 basic Japanese kanji provides 30 cells that explain in detail the various characteristics of kanji. Means, standard deviations, distributions, and information related to previous research concerning these kanji are provided in this paper. The database is saved as a Microsoft Excel 2000 file for Windows. This kanji database is accessible on the Web site of the Oxford Text Archive, Oxford University (http://ota.ahds.ac.uk). Using this database, researchers and educators will be able to conduct planned experiments and organize classroom instruction on the basis of the known characteristics of selected kanji", "label": ["web-accessible database", "basic japanese kanji", "jooyoo kanji-hyo", "pronunciation", "kanji usage print", "cells", "means", "standard deviations", "distributions", "microsoft excel 2000 file for windows", "oxford text archive web site", "classroom instruction"], "stemmed_label": ["web-access databas", "basic japanes kanji", "jooyoo kanji-hyo", "pronunci", "kanji usag print", "cell", "mean", "standard deviat", "distribut", "microsoft excel 2000 file for window", "oxford text archiv web site", "classroom instruct"]}
{"doc": "Modeling self-consistent multi-class dynamic traffic flow In this study, we present a systematic self-consistent multiclass multilane traffic model derived from the vehicular Boltzmann equation and the traffic dispersion model. The multilane domain is considered as a two-dimensional space and the interaction among vehicles in the domain is described by a dispersion model. The reason we consider a multilane domain as a two-dimensional space is that the driving behavior of road users may not be restricted by lanes, especially motorcyclists. The dispersion model, which is a nonlinear Poisson equation, is derived from the car-following theory and the equilibrium assumption. Under the concept that all kinds of users share the finite section, the density is distributed on a road by the dispersion model. In addition, the dynamic evolution of the traffic flow is determined by the systematic gas-kinetic model derived from the Boltzmann equation. Multiplying Boltzmann equation by the zeroth, first- and second-order moment functions, integrating both side of the equation and using chain rules, we can derive continuity, motion and variance equation, respectively. However, the second-order moment function, which is the square of the individual velocity, is employed by previous researches does not have physical meaning in traffic flow", "label": ["self-consistent multiclass dynamic traffic flow modeling", "multilane traffic model", "vehicular boltzmann equation", "traffic dispersion model", "road users", "nonlinear poisson equation", "car-following theory", "dynamic evolution", "variance equation", "motion equation", "poisson equation"], "stemmed_label": ["self-consist multiclass dynam traffic flow model", "multilan traffic model", "vehicular boltzmann equat", "traffic dispers model", "road user", "nonlinear poisson equat", "car-follow theori", "dynam evolut", "varianc equat", "motion equat", "poisson equat"]}
{"doc": "Standards for service discovery and delivery For the past five years, competing industries and standards developers have been hotly pursuing automatic configuration, now coined the broader term service discovery. Jini, Universal Plug and Play (UPnP), Salutation, and Service Location Protocol are among the front-runners in this new race. However, choosing service discovery as the topic of the hour goes beyond the need for plug-and-play solutions or support for the SOHO (small office/home office) user. Service discovery's potential in mobile and pervasive computing environments motivated my choice", "label": ["service discovery", "jini", "universal plug and play", "salutation", "service location protocol", "mobile computing", "pervasive computing"], "stemmed_label": ["servic discoveri", "jini", "univers plug and play", "salut", "servic locat protocol", "mobil comput", "pervas comput"]}
{"doc": "Decisions, decisions, decisions: a tale of special collections in the small academic library A case study of a special collections department in a small academic library and how its collections have been acquired and developed over the years is described. It looks at the changes that have occurred in the academic environment and what effect, if any, these changes may have had on the department and how it has adapted to them. It raises questions about development and acquisitions policies and procedures", "label": ["special collections", "small academic library", "case study", "acquisitions policies", "out-of-print books", "university library"], "stemmed_label": ["special collect", "small academ librari", "case studi", "acquisit polici", "out-of-print book", "univers librari"]}
{"doc": "Web-based experiments controlled by JavaScript: an example from probability learning JavaScript programs can be used to control Web experiments. This technique is illustrated by an experiment that tested the effects of advice on performance in the classic probability-learning paradigm. Previous research reported that people tested via the Web or in the lab tended to match the probabilities of their responses to the probabilities that those responses would be reinforced. The optimal strategy, however, is to consistently choose the more frequent event; probability matching produces suboptimal performance. We investigated manipulations we reasoned should improve performance. A horse race scenario in which participants predicted the winner in each of a series of races between two horses was compared with an abstract scenario used previously. Ten groups of learners received different amounts of advice, including all combinations of (1) explicit instructions concerning the optimal strategy, (2) explicit instructions concerning a monetary sum to maximize, and (3) accurate information concerning the probabilities of events. The results showed minimal effects of horse race versus abstract scenario. Both advice concerning the optimal strategy and probability information contributed significantly to performance in the task. This paper includes a brief tutorial on JavaScript, explaining with simple examples how to assemble a browser-based experiment", "label": ["web-based experiments", "javascript", "probability learning", "advice", "explicit instructions", "probability", "browser-based experiment", "internet-based research"], "stemmed_label": ["web-bas experi", "javascript", "probabl learn", "advic", "explicit instruct", "probabl", "browser-bas experi", "internet-bas research"]}
{"doc": "Twenty years of the literature on acquiring out-of-print materials This article reviews the last two-and-a-half decades of literature on acquiring out-of-print materials to assess recurring issues and identify changing practices. The out-of-print literature is uniform in its assertion that libraries need to acquire o.p. materials to replace worn or damaged copies, to replace missing copies, to duplicate copies of heavily used materials, to fill gaps in collections, to strengthen weak collections, to continue to develop strong collections, and to provide materials for new courses, new programs, and even entire new libraries", "label": ["out-of-print materials", "recurring issues", "changing practices", "out-of-print books", "library materials", "acquisition"], "stemmed_label": ["out-of-print materi", "recur issu", "chang practic", "out-of-print book", "librari materi", "acquisit"]}
{"doc": "Personality research on the Internet: a comparison of Web-based and traditional instruments in take-home and in-class settings Students, faculty, and researchers have become increasingly comfortable with the Internet, and many of them are interested in using the Web to collect data. Few published studies have investigated the differences between Web-based data and data collected with more traditional methods. In order to investigate these potential differences, two important factors were crossed in this study: whether the data were collected on line or not and whether the data were collected in a group setting at a fixed time or individually at a time of the respondent's choosing. The Visions of Morality scale (Shelton and McAdams, 1990) was used, and the participants were assigned to one of four conditions: in-class Web survey, in-class paper-and-pencil survey; take-home Web survey, and take-home paper-and-pencil survey. No significant differences in scores were found for any condition; however, response rates were affected by the type of survey administered, with the take-home Web-based instrument having the lowest response rate. Therefore, researchers need to be aware that different modes of administration may affect subject attrition and may, therefore, confound investigations of other independent variables", "label": ["personality research", "internet", "web-based instruments", "data collection", "visions of morality scale", "in-class web survey", "in-class paper-and-pencil survey", "take-home web survey", "take-home paper-and-pencil survey", "response rates", "administration", "subject attrition"], "stemmed_label": ["person research", "internet", "web-bas instrument", "data collect", "vision of moral scale", "in-class web survey", "in-class paper-and-pencil survey", "take-hom web survey", "take-hom paper-and-pencil survey", "respons rate", "administr", "subject attrit"]}
{"doc": "Noise effect on memory recall in dynamical neural network model of hippocampus We investigate some noise effect on a neural network model proposed by Araki and Aihara (1998) for the memory recall of dynamical patterns in the hippocampus and the entorhinal cortex; the noise effect is important since the release of transmitters at synaptic clefts, the operation of gate of ion channels and so on are known as stochastic phenomena. We consider two kinds of noise effect due to a deterministic noise and a stochastic noise. By numerical simulations, we find that reasonable values of noise give better performance on the memory recall of dynamical patterns. Furthermore we investigate the effect of the strength of external inputs on the memory recall", "label": ["hippocampus", "dynamical neural network model", "noise effect", "memory recall", "dynamical patterns", "entorhinal cortex", "synaptic clefts", "gate of ion channels", "stochastic phenomena", "deterministic noise", "stochastic noise", "brain functions", "numerical simulations", "synaptic strength", "inhibitory connection"], "stemmed_label": ["hippocampu", "dynam neural network model", "nois effect", "memori recal", "dynam pattern", "entorhin cortex", "synapt cleft", "gate of ion channel", "stochast phenomena", "determinist nois", "stochast nois", "brain function", "numer simul", "synapt strength", "inhibitori connect"]}
{"doc": "Convolution-based global simulation technique for millimeter-wave photodetector and photomixer circuits A fast convolution-based time-domain approach to global photonic-circuit simulation is presented that incorporates a physical device model in the complete detector or mixer circuit. The device used in the demonstration of this technique is a GaAs metal-semiconductor-metal (MSM) photodetector that offers a high response speed for the detection and generation of millimeter waves. Global simulation greatly increases the accuracy in evaluating the complete circuit performance because it accounts for the effects of the millimeter-wave embedding circuit. Device and circuit performance are assessed by calculating optical responsivity and bandwidth. Device-only simulations using GaAs MSMs are compared with global simulations that illustrate the strong interdependence between device and external circuit", "label": ["convolution-based time-domain approach", "global photonic-circuit simulation", "physical device model", "gaas msm photodetector", "millimeter-wave photodetector", "photomixer", "mm-wave embedding circuit", "optical responsivity", "bandwidth", "convolution-based global simulation", "gaas"], "stemmed_label": ["convolution-bas time-domain approach", "global photonic-circuit simul", "physic devic model", "gaa msm photodetector", "millimeter-wav photodetector", "photomix", "mm-wave embed circuit", "optic respons", "bandwidth", "convolution-bas global simul", "gaa"]}
{"doc": "Recursive state estimation for multiple switching models with unknown transition probabilities This work considers hybrid systems with continuous-valued target states and discrete-valued regime variable. The changes (switches) of the regime variable are modeled by a finite state Markov chain with unknown and random transition probabilities following Dirichlet distributions. Our work analytically derives the marginal posterior distribution of the states and regime variables, the transition probabilities being integrated out. This leads to a variety of recursive hybrid state estimation schemes which are an appealing intuitive and straightforward extension of standard algorithms. Their performance is illustrated by a maneuvering target tracking example", "label": ["recursive state estimation", "multiple switching models", "unknown transition probabilities", "hybrid systems", "continuous-valued target states", "discrete-valued regime variable", "finite state markov chain", "random transition probabilities", "dirichlet distributions", "marginal posterior distribution", "maneuvering target tracking"], "stemmed_label": ["recurs state estim", "multipl switch model", "unknown transit probabl", "hybrid system", "continuous-valu target state", "discrete-valu regim variabl", "finit state markov chain", "random transit probabl", "dirichlet distribut", "margin posterior distribut", "maneuv target track"]}
{"doc": "Statistical inference with partial prior information based on a Gauss-type inequality Potter and Anderson (1983) have developed a Bayesian decision procedure requiring the specification of a class of prior distributions restricted to have a minimal probability content for a given subset of the parameter space. They do not, however, provide a method for the selection of that subset. We show how a generalization of Gauss' inequality can be used to determine the relevant parameter subset", "label": ["bayesian decision procedure", "prior distributions", "minimal probability content", "parameter space", "gauss inequality", "prior-to-posterior sensitivity", "partial prior information"], "stemmed_label": ["bayesian decis procedur", "prior distribut", "minim probabl content", "paramet space", "gauss inequ", "prior-to-posterior sensit", "partial prior inform"]}
{"doc": "Quality image metrics for synthetic images based on perceptual color differences Due to the improvement of image rendering processes, and the increasing importance of quantitative comparisons among synthetic color images, it is essential to define perceptually based metrics which enable to objectively assess the visual quality of digital simulations. In response to this need, this paper proposes a new methodology for the determination of an objective image quality metric, and gives an answer to this problem through three metrics. This methodology is based on the LLAB color space for perception of color in complex images, a modification of the CIELab1976 color space. The first metric proposed is a pixel by pixel metric which introduces a local distance map between two images. The second metric associates, to a pair of images, a global value. Finally, the third metric uses a recursive subdivision of the images to obtain an adaptative distance map, rougher but less expensive to compute than the first method", "label": ["quality image metrics", "synthetic images", "perceptual color differences", "image rendering", "color images", "perceptually based metrics", "visual quality", "digital simulations", "llab color space", "cielab1976 color space", "pixel by pixel metric", "local distance map", "global value", "recursive subdivision", "adaptative distance map"], "stemmed_label": ["qualiti imag metric", "synthet imag", "perceptu color differ", "imag render", "color imag", "perceptu base metric", "visual qualiti", "digit simul", "llab color space", "cielab1976 color space", "pixel by pixel metric", "local distanc map", "global valu", "recurs subdivis", "adapt distanc map"]}
{"doc": "Activity and location recognition using wearable sensors Using measured acceleration and angular velocity data gathered through inexpensive, wearable sensors, this dead-reckoning method can determine a user's location, detect transitions between preselected locations, and recognize and classify sitting, standing, and walking behaviors. Experiments demonstrate the proposed method's effectiveness", "label": ["measured acceleration", "angular velocity", "wearable sensors", "dead-reckoning method", "user's location", "preselected locations", "transitions", "sitting", "standing", "walking"], "stemmed_label": ["measur acceler", "angular veloc", "wearabl sensor", "dead-reckon method", "user' locat", "preselect locat", "transit", "sit", "stand", "walk"]}
{"doc": "Trusted...or...trustworthy: the search for a new paradigm for computer and network security This paper sets out a number of major questions and challenges which include: (a) just what is meant by `trusted' or `trustworthy' systems after 20 years of experience, or more likely, lack of business level experience, with the 'trusted computer system' criteria anyway; (b) does anyone really care about the adoption of international standards for computer system security evaluation by IT product and system manufacturers and suppliers (IS 15408) and, if so, how does it all relate to business risk management anyway (IS 17799); (c) with the explosion of adoption of the microcomputer and personal computer some 20 years ago, has the industry abandoned all that it learnt about security during the `mainframe era'; or - `whatever happened to MULTICS' and its lessons; (d) has education kept up with security requirements by industry and government alike in the need for safe and secure operation of large scale and networked information systems on national and international bases, particularly where Web or Internet-based information services are being proposed as the major `next best thing' in the IT industry; (e) has the `fourth generation' of computer professionals inherited the spirit of information systems management and control that resided by necessity with the last `generation', the professionals who developed and created the applications for shared mainframe and minicomputer systems?", "label": ["computer security", "network security", "trusted systems", "trustworthy systems", "international standards", "is 15408", "business risk management", "is 17799", "it manufacturers", "microcomputer", "personal computer", "multics", "education", "large scale information systems", "web", "internet-based information services", "fourth generation computer professionals", "information systems management", "information systems control"], "stemmed_label": ["comput secur", "network secur", "trust system", "trustworthi system", "intern standard", "is 15408", "busi risk manag", "is 17799", "it manufactur", "microcomput", "person comput", "multic", "educ", "larg scale inform system", "web", "internet-bas inform servic", "fourth gener comput profession", "inform system manag", "inform system control"]}
{"doc": "An automated parallel image registration technique based on the correlation of wavelet features With the increasing importance of multiple multiplatform remote sensing missions, fast and automatic integration of digital data from disparate sources has become critical to the success of these endeavors. Our work utilizes maxima of wavelet coefficients to form the basic features of a correlation-based automatic registration algorithm. Our wavelet-based registration algorithm is tested successfully with data from the National Oceanic and Atmospheric Administration (NOAA) Advanced Very High Resolution Radiometer (AVHRR) and the Landsat Thematic Mapper (TM), which differ by translation and/or rotation. By the choice of high-frequency wavelet features, this method is similar to an edge-based correlation method, but by exploiting the multiresolution nature of a wavelet decomposition, our method achieves higher computational speeds for comparable accuracies. This algorithm has been implemented on a single-instruction multiple-data (SIMD) massively parallel computer, the MasPar MP-2, as well as on the CrayT3D, the Cray T3E, and a Beowulf cluster of Pentium workstations", "label": ["geophysical measurement technique", "land surface", "terrain mapping", "optical imaging", "microwave radiometry", "image processing", "automated parallel image registration", "correlation", "wavelet feature", "remote sensing", "automatic registration algorithm", "avhrr", "landsat thematic mapper", "wavelet decomposition", "simd massively parallel computing"], "stemmed_label": ["geophys measur techniqu", "land surfac", "terrain map", "optic imag", "microwav radiometri", "imag process", "autom parallel imag registr", "correl", "wavelet featur", "remot sens", "automat registr algorithm", "avhrr", "landsat themat mapper", "wavelet decomposit", "simd massiv parallel comput"]}
{"doc": "Supervisory control design based on hybrid systems and fuzzy events detection. Application to an oxichlorination reactor This paper presents a supervisory control scheme based on hybrid systems theory and fuzzy events detection. The fuzzy event detector is a linguistic model, which synthesizes complex relations between process variables and process events incorporating experts' knowledge about the process operation. This kind of detection allows the anticipation of appropriate control actions, which depend upon the selected membership functions used to characterize the process under scrutiny. The proposed supervisory control scheme was successfully implemented for an oxichlorination reactor in a vinyl monomer plant", "label": ["supervisory control design", "hybrid systems", "events detection. fuzzy", "oxichlorination reactor", "linguistic model", "complex relations", "process variables", "process events", "expert knowledge", "process operation", "control actions", "membership functions", "vinyl monomer plant", "reactor stability", "raw material consumption", "discrete events systems", "reactive systems", "finite state machines"], "stemmed_label": ["supervisori control design", "hybrid system", "event detection. fuzzi", "oxichlorin reactor", "linguist model", "complex relat", "process variabl", "process event", "expert knowledg", "process oper", "control action", "membership function", "vinyl monom plant", "reactor stabil", "raw materi consumpt", "discret event system", "reactiv system", "finit state machin"]}
{"doc": "Estimating populations for collective dose calculations The collective dose provides an estimate of the effects of facility operations on the public based on an estimate of the population in the area. Geographic information system software, electronic population data resources, and a personal computer were used to develop estimates of population within 80 km radii of two sites", "label": ["collective dose calculations", "facility operations", "public", "geographic information system software", "electronic population data resources", "personal computer"], "stemmed_label": ["collect dose calcul", "facil oper", "public", "geograph inform system softwar", "electron popul data resourc", "person comput"]}
{"doc": "Efficient computation of local geometric moments Local moments have attracted attention as local features in applications such as edge detection and texture segmentation. The main reason for this is that they are inherently integral-based features, so that their use reduces the effect of uncorrelated noise. The computation of local moments, when viewed as a neighborhood operation, can be interpreted as a convolution of the image with a set of masks. Nevertheless, moments computed inside overlapping windows are not independent and convolution does not take this fact into account. By introducing a matrix formulation and the concept of accumulation moments, this paper presents an algorithm which is computationally much more efficient than convolving and yet as simple", "label": ["local geometric moments computation", "local features", "edge detection", "texture segmentation", "integral-based features", "neighborhood operation", "image convolution", "overlapping windows", "matrix formulation", "accumulation moments", "computationally efficient algorithm", "image analysis"], "stemmed_label": ["local geometr moment comput", "local featur", "edg detect", "textur segment", "integral-bas featur", "neighborhood oper", "imag convolut", "overlap window", "matrix formul", "accumul moment", "comput effici algorithm", "imag analysi"]}
{"doc": "Adaptive image denoising using scale and space consistency This paper proposes a new method for image denoising with edge preservation, based on image multiresolution decomposition by a redundant wavelet transform. In our approach, edges are implicitly located and preserved in the wavelet domain, whilst image noise is filtered out. At each resolution level, the image edges are estimated by gradient magnitudes (obtained from the wavelet coefficients), which are modeled probabilistically, and a shrinkage function is assembled based on the model obtained. Joint use of space and scale consistency is applied for better preservation of edges. The shrinkage functions are combined to preserve edges that appear simultaneously at several resolutions, and geometric constraints are applied to preserve edges that are not isolated. The proposed technique produces a filtered version of the original image, where homogeneous regions appear separated by well-defined edges. Possible applications include image presegmentation, and image denoising", "label": ["adaptive image denoising", "scale consistency", "space consistency", "edge preservation", "image multiresolution decomposition", "redundant wavelet transform", "image edges", "gradient magnitudes", "shrinkage function", "geometric constraints", "edge enhancement"], "stemmed_label": ["adapt imag denois", "scale consist", "space consist", "edg preserv", "imag multiresolut decomposit", "redund wavelet transform", "imag edg", "gradient magnitud", "shrinkag function", "geometr constraint", "edg enhanc"]}
{"doc": "High-voltage transistor scaling circuit techniques for high-density negative-gate channel-erasing NOR flash memories In order to scale high-voltage transistors for high-density negative-gate channel-erasing NOR flash memories, two circuit techniques were developed. A proposed level shifter with low operating voltage is composed of three parts, a latch holding the negative erasing voltage, two coupling capacitors connected with the latched nodes in the latch, and high-voltage drivers inverting the latch, resulting in reduction of the maximum internal voltage by 0.5 V. A proposed high-voltage generator adds a path-gate logic to a conventional high-voltage generator to realize both low noise and low ripple voltage, resulting in a reduction of the maximum internal voltage by 0.5 V. As a result, these circuit techniques along with high coupling-ratio cell technology can scale down the high-voltage transistors by 15 and can realize higher density negative-gate channel-erase NOR flash memories in comparison with the source-erase NOR flash memories", "label": ["hv transistor scaling circuit techniques", "high-density nor flash memories", "negative-gate channel-erasing flash memories", "level shifter", "low operating voltage shifter", "high-voltage drivers", "high-voltage generator", "path-gate logic", "hv generator", "low noise", "low ripple voltage", "high coupling-ratio cell technology"], "stemmed_label": ["hv transistor scale circuit techniqu", "high-dens nor flash memori", "negative-g channel-eras flash memori", "level shifter", "low oper voltag shifter", "high-voltag driver", "high-voltag gener", "path-gat logic", "hv gener", "low nois", "low rippl voltag", "high coupling-ratio cell technolog"]}
{"doc": "Who wants to see a $million error? Inspired by the popular television show \"Who Wants to Be a Millionaire?\", this case discusses the monetary decisions contestants face on a game consisting of 15 increasingly difficult multiple choice questions. Since the game continues as long as a contestant answers correctly, this case, at its core, is one of sequential decision analysis, amenable to analysis via stochastic dynamic programming. The case is also suitable for a course dealing with single decision analysis, allowing for discussion of utility theory and Bayesian probability revision. In developing a story line for the case, the author has sprinkled in much background material on probability and statistics. This material is placed in a historical context, illuminating some of the influential scholars involved in the development of these subjects as well as the birth of operations research and the management sciences", "label": ["operations research", "game theory", "decision analysis", "stochastic dynamic programming", "educational course", "statistics", "probabilistic models"], "stemmed_label": ["oper research", "game theori", "decis analysi", "stochast dynam program", "educ cours", "statist", "probabilist model"]}
{"doc": "E-mail and the legal profession The widespread use of E-mail can be found in all areas of commerce, and the legal profession is one that has embraced this new medium of communication. E-mail is not without its drawbacks, however. Due to the nature of the technologies behind the medium, it is a less secure form of communication than many of those traditionally used by the legal profession, including DX, facsimile, and standard and registered post. There are a number of ways in which E-mails originating from the practice may be protected, including software encryption, hardware encryption and various methods of controlling and administering access to the E-mails", "label": ["e-mail", "legal profession", "secure communication", "software encryption", "hardware encryption", "access control"], "stemmed_label": ["e-mail", "legal profess", "secur commun", "softwar encrypt", "hardwar encrypt", "access control"]}
{"doc": "Optimize/sup IT/ robot condition monitoring tool As robots have gained more and more 'humanlike' capability, users have looked increasingly to their builders for ways to measure the critical variables-the robotic equivalent of a physical check-up-in order to monitor their condition and schedule maintenance more effectively. This is all the more essential considering the tremendous pressure there is to improve productivity in today's global markets. Developed for ABB robots with an S4-family controller and based on the company's broad process know-how, Optimize/sup IT/ robot condition monitoring offers maintenance routines with embedded checklists that give a clear indication of a robot's operating condition. It performs semi-automatic measurements that support engineers during trouble-shooting and enable action to be taken to prevent unplanned stops. By comparing these measurements with reference data, negative trends can be detected early and potential breakdowns predicted. Armed with all these features, Optimize/sup IT/ robot condition monitoring provides the ideal basis for reliability-centered maintenance (RCM) for robots", "label": ["optimize/sup it/ robot condition monitoring tool", "maintenance scheduling", "condition monitoring", "abb robots", "s4-family controller", "semi-automatic measurements", "reliability-centered maintenance"], "stemmed_label": ["optimize/sup it/ robot condit monitor tool", "mainten schedul", "condit monitor", "abb robot", "s4-famili control", "semi-automat measur", "reliability-cent mainten"]}
{"doc": "Java portability put to the test Sun Microsystems' recently launched Java Verification Program aims to enable companies to assess the cross-platform portability of applications written in Java, and to help software vendors ensure that their solutions can run in heterogenous J2EE application server environments", "label": ["sun microsystems", "java verification program", "cross-platform portability"], "stemmed_label": ["sun microsystem", "java verif program", "cross-platform portabl"]}
{"doc": "The Open Archives Initiative: realizing simple and effective digital library interoperability The Open Archives Initiative (OAI) is dedicated to solving problems of digital library interoperability. Its focus has been on defining simple protocols, most recently for the exchange of metadata from archives. The OAI evolved out of a need to increase access to scholarly publications by supporting the creation of interoperable digital libraries. As a first step towards such interoperability, a metadata harvesting protocol was developed to support the streaming of metadata from one repository to another, ultimately to a provider of user services such as browsing, searching, or annotation. This article provides an overview of the mission, philosophy, and technical framework of the OAI", "label": ["open archives initiative", "digital library interoperability", "protocols", "exchange metadata", "scholarly publications", "metadata harvesting protocol", "streaming metadata", "annotation", "searching", "browsing", "user services"], "stemmed_label": ["open archiv initi", "digit librari interoper", "protocol", "exchang metadata", "scholarli public", "metadata harvest protocol", "stream metadata", "annot", "search", "brows", "user servic"]}
{"doc": "Rapid microwell polymerase chain reaction with subsequent ultrathin-layer gel electrophoresis of DNA Large-scale genotyping, mapping and expression profiling require affordable, fully automated high-throughput devices enabling rapid, high-performance analysis using minute quantities of reagents. In this paper, we describe a new combination of microwell polymerase chain reaction (PCR) based DNA amplification technique with automated ultrathin-layer gel electrophoresis analysis of the resulting products. This technique decreases the reagent consumption (total reaction volume 0.75-1 mu L), the time requirement of the PCR (15-20 min) and subsequent ultrathin-layer gel electrophoresis based fragment analysis (5 min) by automating the current manual procedure and reducing the human intervention using sample loading robots and computerized real time data analysis. Small aliquots (0.2 mu L) of the submicroliter size PCR reaction were transferred onto loading membranes and analyzed by ultrathin-layer gel electrophoresis which is a novel, high-performance and automated microseparation technique. This system employs integrated scanning laser-induced fluorescence-avalanche photodiode detection and combines the advantages of conventional slab and capillary gel electrophoresis. Visualization of the DNA fragments was accomplished by \"in migratio\" complexation with ethidium bromide during the electrophoresis process also enabling real time imaging and data analysis", "label": ["rapid microwell polymerase chain reaction", "ultrathin-layer gel electrophoresis", "dna amplification", "large-scale genotyping", "expression profiling", "rapid high-performance analysis", "automated electrophoresis analysis", "reagent consumption", "sample loading robots", "computerized real time data analysis", "automated microseparation", "integrated scanning lif apd detection", "complexation with ethidium bromide", "real time imaging"], "stemmed_label": ["rapid microwel polymeras chain reaction", "ultrathin-lay gel electrophoresi", "dna amplif", "large-scal genotyp", "express profil", "rapid high-perform analysi", "autom electrophoresi analysi", "reagent consumpt", "sampl load robot", "computer real time data analysi", "autom microsepar", "integr scan lif apd detect", "complex with ethidium bromid", "real time imag"]}
{"doc": "Fuzzy control of multivariable process by modified error decoupling In this paper, a control concept for the squared (equal number of inputs and outputs) multivariable process systems is given. The proposed control system consists of two parts, single loop fuzzy controllers in each loop and a centralized decoupling unit. The fuzzy control system uses feedback control to minimize the error in the loop and the decoupler uses an adaptive technique to mitigate loop interactions. The decoupler predicts the interacting loop changes and modifies the input (error) of the loop controller. The controller was tested on the simulation model of \"single component vaporizer\" process", "label": ["multivariable process", "modified error decoupling", "squared multivariable process systems", "square multivariable process systems", "single-loop fuzzy controllers", "centralized decoupling unit", "feedback control", "error minimization", "loop interaction mitigation", "single component vaporizer process", "set point changes", "load changes"], "stemmed_label": ["multivari process", "modifi error decoupl", "squar multivari process system", "squar multivari process system", "single-loop fuzzi control", "central decoupl unit", "feedback control", "error minim", "loop interact mitig", "singl compon vapor process", "set point chang", "load chang"]}
{"doc": "Lossy to lossless object-based coding of 3-D MRI data We propose a fully three-dimensional (3-D) object-based coding system exploiting the diagnostic relevance of the different regions of the volumetric data for rate allocation. The data are first decorrelated via a 3-D discrete wavelet transform. The implementation via the lifting steps scheme allows to map integer-to-integer values, enabling lossless coding, and facilitates the definition of the object-based inverse transform. The coding process assigns disjoint segments of the bitstream to the different objects, which can be independently accessed and reconstructed at any up-to-lossless quality. Two fully 3-D coding strategies are considered: embedded zerotree coding (EZW-3D) and multidimensional layered zero coding (MLZC), both generalized for region of interest (ROI)-based processing. In order to avoid artifacts along region boundaries, some extra coefficients must be encoded for each object. This gives rise to an overheading of the bitstream with respect to the case where the volume is encoded as a whole. The amount of such extra information depends on both the filter length and the decomposition depth. The system is characterized on a set of head magnetic resonance images. Results show that MLZC and EZW-3D have competitive performances. In particular, the best MLZC mode outperforms the others state-of-the-art techniques on one of the datasets for which results are available in the literature", "label": ["lossy object-based coding", "lossless object-based coding", "3-d mri data", "diagnostic relevance", "volumetric data", "rate allocation", "3-d discrete wavelet transform", "lifting steps scheme", "integer-to-integer values", "object-based inverse transform", "disjoint segments", "bitstream", "embedded zerotree coding", "ezw-3d", "multidimensional layered zero coding", "nmzq", "region of interest-based processing", "roi-based processing", "region boundaries", "filter length", "decomposition depth", "head magnetic resonance images"], "stemmed_label": ["lossi object-bas code", "lossless object-bas code", "3-d mri data", "diagnost relev", "volumetr data", "rate alloc", "3-d discret wavelet transform", "lift step scheme", "integer-to-integ valu", "object-bas invers transform", "disjoint segment", "bitstream", "embed zerotre code", "ezw-3d", "multidimension layer zero code", "nmzq", "region of interest-bas process", "roi-bas process", "region boundari", "filter length", "decomposit depth", "head magnet reson imag"]}
{"doc": "In search of strategic operations research/management science We define strategic OR/MS as \"OR/MS work that leads to a sustainable competitive advantage.\" We found evidence of strategic OR/MS in the literature of strategic information systems (SIS) and OR/MS. We examined 30 early examples of SIS, many of which contained OR/MS work. Many of the most successful had high OR/MS content, while the least successful contained none. The inclusion of OR/MS work may be a key to sustaining an advantage from information technology. We also examined the Edelman Prize finalist articles published between 1990 and 1999. We found that 13 of the 42 private sector applications meet our definition of strategic OR/MS", "label": ["operations research", "management science", "strategic or/ms", "strategic information systems", "sis"], "stemmed_label": ["oper research", "manag scienc", "strateg or/m", "strateg inform system", "si"]}
{"doc": "Location of transport nets on a heterogeneous territory The location of transport routes on a heterogeneous territory is studied. The network joins a given set of terminal points and a certain number of additional (branch) points. The problem is formulated, properties of the optimal solution for a. tree-like network, and the number of branch points are studied. A stepwise optimization algorithm for a. network with given adjacency matrix based on an algorithm for constructing minimal-cost routes is designed", "label": ["transport nets", "heterogeneous territory", "transport routes", "terminal points", "branch points", "tree-like network", "stepwise optimization algorithm", "adjacency matrix"], "stemmed_label": ["transport net", "heterogen territori", "transport rout", "termin point", "branch point", "tree-lik network", "stepwis optim algorithm", "adjac matrix"]}
{"doc": "Low-voltage DRAM sensing scheme with offset-cancellation sense amplifier A novel bitline sensing scheme is proposed for low-voltage DRAM to achieve low power dissipation and compatibility with low-voltage CMOS. One of the major obstacles in low-voltage DRAM is the degradation of data-retention time due to low signal level at the memory cell, which requires power-consuming refresh operations more frequently. This paper proposes an offset-cancellation sense-amplifier scheme (OCSA) that improves data-retention time significantly even at low supply voltage. It also improves die efficiency, because the proposed scheme reduces the number of sense amplifiers by supporting more cells in each sense amplifier. Measurements show that the data-retention time of the proposed scheme at 1.5-V supply voltage is 2.4 times of the conventional scheme at 2.0 V", "label": ["lv dram sensing scheme", "low-voltage sensing scheme", "offset-cancellation sense amplifier scheme", "bitline sensing scheme", "low power dissipation", "low-voltage cmos compatibility", "data-retention time", "memory cell", "differential amplifier configuration", "power-consuming refresh operations", "sensing margin", "1.5 v"], "stemmed_label": ["lv dram sens scheme", "low-voltag sens scheme", "offset-cancel sens amplifi scheme", "bitlin sens scheme", "low power dissip", "low-voltag cmo compat", "data-retent time", "memori cell", "differenti amplifi configur", "power-consum refresh oper", "sens margin", "1.5 v"]}
{"doc": "Strain contouring using Gabor filters: principle and algorithm Moire interferometry is a powerful technique for high sensitivity in-plane deformation contouring. However, from an engineering viewpoint, the derivatives of displacement, i.e., strain, are the desired parameter. Thus there is a need to differentiate the displacement field. Optical and digital methods have been proposed for this differentiation. Optical methods provide contours that still need to be quantified, while digital methods suffer from drawbacks inherent in the digital differentiation process. We describe a novel approach of strain segmentation for the moire pattern using a multichannel Gabor filter. Appropriate filter design allows for user-specific segmentation, which is essentially in engineering design and analysis", "label": ["strain contouring", "gabor filters", "algorithm", "moire interferometry", "high sensitivity in-plane deformation contouring", "displacement", "displacement field", "digital methods", "optical methods", "differentiation", "digital differentiation process", "strain segmentation", "multichannel gabor filter", "filter design", "user-specific segmentation", "engineering design", "engineering analysis", "image segmentation", "spatial filters"], "stemmed_label": ["strain contour", "gabor filter", "algorithm", "moir interferometri", "high sensit in-plan deform contour", "displac", "displac field", "digit method", "optic method", "differenti", "digit differenti process", "strain segment", "multichannel gabor filter", "filter design", "user-specif segment", "engin design", "engin analysi", "imag segment", "spatial filter"]}
{"doc": "A method for correlations analysis of coordinates: applications for molecular conformations We describe a new method to analyze multiple correlations between subsets of coordinates that represent a sample. The correlation is established only between specific regions of interest at the coordinates. First, the region(s) of interest are selected at each molecular coordinate. Next, a correlation matrix is constructed for the selected regions. The matrix is subject to further analysis, illuminating the multidimensional structural characteristics that exist in the conformational space. The method's abilities are demonstrated in several examples: it is used to analyze the conformational space of complex molecules, it is successfully applied to compare related conformational spaces, and it is used to analyze a diverse set of protein folding trajectories", "label": ["multiple correlation analysis", "regions of interest", "correlation matrix", "molecular coordinate", "multidimensional structural characteristics", "complex molecules", "conformational spaces", "protein folding trajectories", "molecular conformations"], "stemmed_label": ["multipl correl analysi", "region of interest", "correl matrix", "molecular coordin", "multidimension structur characterist", "complex molecul", "conform space", "protein fold trajectori", "molecular conform"]}
{"doc": "Temelin casts its shadow nuclear power plant Reservations about Temelin nuclear plant in the Czech Republic are political rather than technical. This paper discusses the problems of turbogenerator vibrations and how they were diagnosed. The paper also discusses some of the other problems of commissioning the power plant. The simulator used for training new staff is also mentioned", "label": ["temelin nuclear plant", "czech republic", "turbogenerator vibrations", "power plant commissioning", "training simulator"], "stemmed_label": ["temelin nuclear plant", "czech republ", "turbogener vibrat", "power plant commiss", "train simul"]}
{"doc": "Pane relief. Robotic solutions for car windshield assembly Just looking through a car's windshield doesn't give us much reason to wonder about how it's made. The idea that special manufacturing expertise might be required can hardly occur to anyone, but that's exactly what is needed to ensure crystal-clear visibility, not to mention a perfect fit every time one is pressed into place on a car production line. Comprising two thin glass sheets joined by a vinyl interlayer, windshields are assembled-usually manually-to very precise product and environmental specifications. To make sure this is done as perfectly as possible, the industry invests heavily in the equipment used for their fabrication. ABB has now developed a robot-based Compact Assembling System for the automatic assembly of laminated windshields that speeds up production and increases cost efficiency", "label": ["car windshield assembly robots", "manufacturing expertise", "car production line", "compact assembling system", "laminated windshields assembly automation", "production", "cost efficiency", "abb"], "stemmed_label": ["car windshield assembl robot", "manufactur expertis", "car product line", "compact assembl system", "lamin windshield assembl autom", "product", "cost effici", "abb"]}
{"doc": "Teaching modeling in management science This essay discusses how we can most effectively teach Management Science to students in MBA or similar programs who will be, at best, part-time practitioners of these arts. I take as a working hypothesis the radical proposition that the heart of Management Science itself is not the impressive array of tools that have been built up over the years (optimization, simulation, decision analysis, queuing, and so on) but rather the art of reasoning logically with formal models. I believe it is necessary with this group of students to teach basic modeling skills, and in fact it is only when such students have these basic skills as a foundation that they are prepared to acquire the more sophisticated skills needed to employ Management Science. In this paper I present a hierarchy of modeling skills, from numeracy skills through sophisticated Management Science skills, as a framework within which to plan courses for the occasional practitioner", "label": ["management science", "modeling", "numeracy skills", "formal models", "decision analysis"], "stemmed_label": ["manag scienc", "model", "numeraci skill", "formal model", "decis analysi"]}
{"doc": "Lossy SPICE models produce realistic averaged simulations In previous averaged models, the state-space averaging technique or switch waveforms analysis were usually applied over perfect elements, non-inclusive of the ohmic losses. However, if these elements play an active role in the DC transfer function, they affect the small-signal AC analysis by introducing various damping effects. A model is introduced in a boost voltage-mode application", "label": ["lossy spice models", "realistic averaged simulations", "state-space averaging technique", "switch waveforms analysis", "damping effects", "boost voltage-mode application", "ohmic losses", "dc transfer function"], "stemmed_label": ["lossi spice model", "realist averag simul", "state-spac averag techniqu", "switch waveform analysi", "damp effect", "boost voltage-mod applic", "ohmic loss", "dc transfer function"]}
{"doc": "BioOne: a new model for scholarly publishing This article describes a unique electronic journal publishing project involving the University of Kansas, the Big 12 Plus Libraries Consortium, the American Institute of Biological Sciences, Allen Press, and SPARC, the Scholarly Publishing and Academic Resources Coalition. This partnership has created BioOne, a database of 40 full-text society journals in the biological and environmental sciences, which was launched in April, 2001. The genesis and development of the project is described and financial, technical, and intellectual property models for the project are discussed. Collaborative strategies for the project are described", "label": ["bioone full-text society journal database", "electronic journal publishing project", "scholarly publishing model", "university of kansas", "big 12 plus libraries consortium", "american institute of biological sciences", "allen press", "sparc", "scholarly publishing and academic resources coalition", "biological sciences", "environmental sciences", "intellectual property models", "technical models", "financial models", "collaborative strategies"], "stemmed_label": ["bioon full-text societi journal databas", "electron journal publish project", "scholarli publish model", "univers of kansa", "big 12 plu librari consortium", "american institut of biolog scienc", "allen press", "sparc", "scholarli publish and academ resourc coalit", "biolog scienc", "environment scienc", "intellectu properti model", "technic model", "financi model", "collabor strategi"]}
{"doc": "Limits for computational electromagnetics codes imposed by computer architecture The algorithmic complexity of the innermost loops that determine the complexity of algorithms in computational electromagnetics (CEM) codes are analyzed according to their operation count and the impact of the underlying computer hardware. As memory chips are much slower than arithmetic processors, codes that involve a high data movement compared to the number of arithmetic operations are executed comparatively slower. Hence, matrix-matrix multiplications are much faster than matrix-vector multiplications. It is seen that it is not sufficient to compare only the complexity, but also the actual performance of algorithms to judge on faster execution. Implications involve FDTD loops, LU factorizations, and iterative solvers for dense matrices. Run times on two reference platforms, namely an Athlon 900 MHz and an HP PA 8600 processor, verify the findings", "label": ["computational electromagnetics codes", "computer architecture", "algorithmic complexity", "innermost loops", "cem codes", "operation count", "computer hardware", "memory chips", "data movement", "matrix-matrix multiplications", "matrix-vector multiplications", "fdtd loops", "lu factorizations", "iterative solvers", "dense matrices"], "stemmed_label": ["comput electromagnet code", "comput architectur", "algorithm complex", "innermost loop", "cem code", "oper count", "comput hardwar", "memori chip", "data movement", "matrix-matrix multipl", "matrix-vector multipl", "fdtd loop", "lu factor", "iter solver", "dens matric"]}
{"doc": "Matching PET and CT scans of the head and neck area: Development of method and validation Positron emission tomography (PET) provides important information on tumor biology, but lacks detailed anatomical information. Our aim in the present study was to develop and validate an automatic registration method for matching PET and CT scans of the head and neck. Three difficulties in achieving this goal are (1) nonrigid motions of the neck can hamper the use of automatic ridged body transformations; (2) emission scans contain too little anatomical information to apply standard image fusion methods; and (3) no objective way exists to quantify the quality of the match results. These problems are solved as follows: accurate and reproducible positioning of the patient was achieved by using a radiotherapy treatment mask. The proposed method makes use of the transmission rather than the emission scan. To obtain sufficient (anatomical) information for matching, two bed positions for the transmission scan were included in the protocol. A mutual information-based algorithm was used as a registration technique. PET and CT data were obtained in seven patients. Each patient had two CT scans and one PET scan. The datasets were used to estimate the consistency by matching PET to CT/sub 1/, CT/sub 1/ to CT/sub 2/, and CT/sub 2/ to PET using the full circle consistency test. It was found that using our method, consistency could be obtained of 4 mm and 1.3 degrees on average. The PET voxels used for registration were 5.15 mm, so the errors compared quite favorably with the voxel size. Cropping the images (removing the scanner bed from images) did not improve the consistency of the algorithm. The transmission scan, however, could potentially be reduced to a single position using this approach. In conclusion, the represented algorithm and validation technique has several features that are attractive from both theoretical and practical point of view, it is a user-independent, automatic validation technique for matching CT and PET scans of the head and neck, which gives the opportunity to compare different image enhancements", "label": ["positron emission tomography scans", "tumor biology", "anatomical information", "automatic registration method", "computerised tomography scans", "head", "neck", "nonrigid motions", "automatic ridged body transformations", "standard image fusion methods", "radiotherapy treatment mask", "bed positions", "transmission scan", "mutual information-based algorithm", "registration technique", "patients", "full circle consistency test", "errors", "scanner bed", "user-independent automatic validation technique", "image enhancements"], "stemmed_label": ["positron emiss tomographi scan", "tumor biolog", "anatom inform", "automat registr method", "computeris tomographi scan", "head", "neck", "nonrigid motion", "automat ridg bodi transform", "standard imag fusion method", "radiotherapi treatment mask", "bed posit", "transmiss scan", "mutual information-bas algorithm", "registr techniqu", "patient", "full circl consist test", "error", "scanner bed", "user-independ automat valid techniqu", "imag enhanc"]}
{"doc": "Acquiring materials in the history of science, technology, and medicine This article provides detailed advice on acquiring new, out-of-print, and rare materials in the history of science, technology, and medicine for the beginner in these fields. The focus is on the policy formation, basic reference tools, and methods of collection development and acquisitions that are the necessary basis for success in this endeavor", "label": ["out-of-print books", "special collections", "library acquisitions", "science", "technology", "medicine", "rare materials", "policy formation", "basic reference tools", "collection development"], "stemmed_label": ["out-of-print book", "special collect", "librari acquisit", "scienc", "technolog", "medicin", "rare materi", "polici format", "basic refer tool", "collect develop"]}
{"doc": "Incremental motion control of linear synchronous motor In this study a particular incremental motion control problem, which is specified by the trapezoidal velocity profile using multisegment sliding mode control (MSSMC), is proposed to control a permanent magnet linear synchronous motor (PMLSM) servo drive system. First, the structure and operating principle of the PMLSM are described in detail. Second, a field-oriented control PMLSM servo drive is introduced. Then, each segment of the multisegment switching surfaces is designed to match the corresponding part of the trapezoidal velocity profile, thus the motor dynamics on the specified-segment switching surface have the desired velocity or acceleration corresponding part of the trapezoidal velocity profile. In addition, the proposed control system is implemented in a PC-based computer control system. Finally, the effectiveness of the proposed PMLSM servo drive system is demonstrated by some simulated and experimental results", "label": ["incremental motion control", "linear synchronous motor", "trapezoidal velocity profile", "multisegment sliding mode control", "permanent magnet motor", "servo drive system", "field-oriented control", "multisegment switching surfaces", "motor dynamics"], "stemmed_label": ["increment motion control", "linear synchron motor", "trapezoid veloc profil", "multiseg slide mode control", "perman magnet motor", "servo drive system", "field-ori control", "multiseg switch surfac", "motor dynam"]}
{"doc": "Self-calibration from image derivatives This study investigates the problem of estimating camera calibration parameters from image motion fields induced by a rigidly moving camera with unknown parameters, where the image formation is modeled with a linear pinhole-camera model. The equations obtained show the flow to be separated into a component due to the translation and the calibration parameters and a component due to the rotation and the calibration parameters. A set of parameters encoding the latter component is linearly related to the flow, and from these parameters the calibration can be determined. However, as for discrete motion, in general it is not possible to decouple image measurements obtained from only two frames into translational and rotational components. Geometrically, the ambiguity takes the form of a part of the rotational component being parallel to the translational component, and thus the scene can be reconstructed only up to a projective transformation. In general, for full calibration at least four successive image frames are necessary, with the 3D rotation changing between the measurements. The geometric analysis gives rise to a direct self-calibration method that avoids computation of optical flow or point correspondences and uses only normal flow measurements. New constraints on the smoothness of the surfaces in view are formulated to relate structure and motion directly to image derivatives, and on the basis of these constraints the transformation of the viewing geometry between consecutive images is estimated. The calibration parameters are then estimated from the rotational components of several flow fields. As the proposed technique neither requires a special set up nor needs exact correspondence it is potentially useful for the calibration of active vision systems which have to acquire knowledge about their intrinsic parameters while they perform other tasks, or as a tool for analyzing image sequences in large video databases", "label": ["camera calibration parameters", "image motion fields", "rigidly moving camera", "image formation", "linear pinhole-camera model", "calibration parameters", "image measurements", "translational components", "rotational components", "direct self-calibration method", "optical flow", "point correspondences", "normal flow measurements", "active vision systems", "image sequences", "large video databases", "depth distortion"], "stemmed_label": ["camera calibr paramet", "imag motion field", "rigidli move camera", "imag format", "linear pinhole-camera model", "calibr paramet", "imag measur", "translat compon", "rotat compon", "direct self-calibr method", "optic flow", "point correspond", "normal flow measur", "activ vision system", "imag sequenc", "larg video databas", "depth distort"]}
{"doc": "Computer-mediated communication and remote management: integration or isolation? The use of intranets and e-mails to communicate with remote staff is increasing rapidly within organizations. For many companies this is viewed as a speedy and cost-effective way of keeping in contact with staff and ensuring their continuing commitment to company goals. This article highlights the problems experienced by staff when managers use intranets and e-mails in an inappropriate fashion for these purposes. Issues of remoteness and isolation are discussed, along with the reports of frustration and disidentification experienced. However, it will be shown that when used appropriately, communication using these technologies can facilitate shared understanding and help remote staff to view their company as alive and exciting. Theoretical aspects are highlighted and the implications of these findings are discussed", "label": ["computer-mediated communication", "remote management", "intranets", "e-mails", "remote staff", "organizations", "companies", "cost-effective", "managers", "remoteness"], "stemmed_label": ["computer-medi commun", "remot manag", "intranet", "e-mail", "remot staff", "organ", "compani", "cost-effect", "manag", "remot"]}
{"doc": "A comparison of computational color constancy algorithms. I: Methodology and experiments with synthesized data We introduce a context for testing computational color constancy, specify our approach to the implementation of a number of the leading algorithms, and report the results of three experiments using synthesized data. Experiments using synthesized data are important because the ground truth is known, possible confounds due to camera characterization and pre-processing are absent, and various factors affecting color constancy can be efficiently investigated because they can be manipulated individually and precisely. The algorithms chosen for close study include two gray world methods, a limiting case of a version of the Retinex method, a number of variants of Forsyth's (1990) gamut-mapping method, Cardei et al.'s (2000) neural net method, and Finlayson et al.'s color by correlation method (Finlayson et al. 1997, 2001; Hubel and Finlayson 2000) . We investigate the ability of these algorithms to make estimates of three different color constancy quantities: the chromaticity of the scene illuminant, the overall magnitude of that illuminant, and a corrected, illumination invariant, image. We consider algorithm performance as a function of the number of surfaces in scenes generated from reflectance spectra, the relative effect on the algorithms of added specularities, and the effect of subsequent clipping of the data. All data is available on-line at http://www.cs.sfu.ca/~color/data, and implementations for most of the algorithms are also available (http://www.cs.sfu.ca/~color/code)", "label": ["computational color constancy algorithms", "synthesized data", "gray world methods", "retinex method", "gamut-mapping method", "neural net method", "color by correlation method", "chromaticity", "scene illuminant", "illumination invariant image", "algorithm performance", "reflectance spectra", "specularities", "clipping"], "stemmed_label": ["comput color constanc algorithm", "synthes data", "gray world method", "retinex method", "gamut-map method", "neural net method", "color by correl method", "chromat", "scene illumin", "illumin invari imag", "algorithm perform", "reflect spectra", "specular", "clip"]}
{"doc": "Modeling the labor market as an evolving institution: model ARTEMIS A stylized French labor market is modeled as an endogenously evolving institution. Boundedly rational firms and individuals strive to decrease the cost or increase utility. The labor market is coordinated by a search process and decentralized setting of hiring standards, but intermediaries can speed up matching. The model reproduces the dynamics of the gross flows and spectacular changes in mobility patterns of some demographic groups when the oil crisis in the 1970's occurred, notably the sudden decline of the integration in good jobs. The internal labor markets of large firms are shown to increase unemployment if the secondary (temporary or bad) jobs do not exist", "label": ["artemis model", "french labor market", "endogenously evolving institution", "simulation model", "jobs", "endogenous intermediary", "spectacular changes", "mobility patterns", "demographic groups"], "stemmed_label": ["artemi model", "french labor market", "endogen evolv institut", "simul model", "job", "endogen intermediari", "spectacular chang", "mobil pattern", "demograph group"]}
{"doc": "Mixture of experts classification using a hierarchical mixture model A three-level hierarchical mixture model for classification is presented that models the following data generation process: (1) the data are generated by a finite number of sources (clusters), and (2) the generation mechanism of each source assumes the existence of individual internal class-labeled sources (subclusters of the external cluster). The model estimates the posterior probability of class membership similar to a mixture of experts classifier. In order to learn the parameters of the model, we have developed a general training approach based on maximum likelihood that results in two efficient training algorithms. Compared to other classification mixture models, the proposed hierarchical model exhibits several advantages and provides improved classification performance as indicated by the experimental results", "label": ["hierarchical mixture model", "classification", "data generation process", "bayes classifier", "experts classifier", "posterior probability of class membership"], "stemmed_label": ["hierarch mixtur model", "classif", "data gener process", "bay classifi", "expert classifi", "posterior probabl of class membership"]}
{"doc": "Correction to construction of panoramic image mosaics with global and local alignment For original paper see ibid., vol. 36, no. 2, p. 101-30 (2000). The authors had given a method for the construction of panoramic image mosaics with global and local alignment. Unfortunately a mistake had led to an incorrect equation which whilst making little difference in many cases, for faster (and assured) convergence, the correct formulae given here should be used", "label": ["panoramic image mosaics", "global alignment", "local alignment", "resampled image"], "stemmed_label": ["panoram imag mosaic", "global align", "local align", "resampl imag"]}
{"doc": "Precoded OFDM with adaptive vector channel allocation for scalable video transmission over frequency-selective fading channels Orthogonal frequency division multiplexing (OFDM) has been applied in broadband wireline and wireless systems for high data rate transmission where severe intersymbol interference (ISI) always occurs. The conventional OFDM system provides advantages through conversion of an ISI channel into ISI-free subchannels at multiple frequency bands. However, it may suffer from channel spectral nulls and heavy data rate overhead due to cyclic prefix insertion. Previously, a new OFDM framework, the precoded OFDM, has been proposed to mitigate the above two problems through precoding and conversion of an ISI channel into ISI-free vector channels. In this paper, we consider the application of the precoded OFDM system to efficient scalable video transmission. We propose to enhance the precoded OFDM system with adaptive vector channel allocation to provide stronger protection against errors to more important layers in the layered bit stream structure of scalable video. The more critical layers, or equivalently, the lower layers, are allocated vector channels of higher transmission quality. The channel quality is characterized by Frobenius norm metrics; based on channel estimation at the receiver. The channel allocation information is fed back periodically to the transmitter through a control channel. Simulation results have demonstrated the robustness of the proposed scheme to noise and fading inherent in wireless channels", "label": ["precoded ofdm", "scalable video transmission", "frequency-selective fading channels", "orthogonal frequency division multiplexing", "channel spectral nulls", "heavy data rate overhead", "isi channel", "isi-free vector channels", "adaptive vector channel allocation", "layered bit stream structure", "lower layers", "critical layers", "channel quality", "frobenius norm metrics", "channel estimation", "channel allocation information", "control channel", "robustness"], "stemmed_label": ["precod ofdm", "scalabl video transmiss", "frequency-select fade channel", "orthogon frequenc divis multiplex", "channel spectral null", "heavi data rate overhead", "isi channel", "isi-fre vector channel", "adapt vector channel alloc", "layer bit stream structur", "lower layer", "critic layer", "channel qualiti", "frobeniu norm metric", "channel estim", "channel alloc inform", "control channel", "robust"]}
{"doc": "Quantum market games We propose a quantum-like description of markets and economics. The approach has roots in the recently developed quantum game theory", "label": ["quantum market games", "economics", "quantum game theory", "quantum strategies", "financial markets"], "stemmed_label": ["quantum market game", "econom", "quantum game theori", "quantum strategi", "financi market"]}
{"doc": "Using latent semantic analysis to assess reader strategies We tested a computer-based procedure for assessing reader strategies that was based on verbal protocols that utilized latent semantic analysis (LSA). Students were given self-explanation-reading training (SERT), which teaches strategies that facilitate self-explanation during reading, such as elaboration based on world knowledge and bridging between text sentences. During a computerized version of SERT practice, students read texts and typed self-explanations into a computer after each sentence. The use of SERT strategies during this practice was assessed by determining the extent to which students used the information in the current sentence versus the prior text or world knowledge in their self-explanations. This assessment was made on the basis of human judgments and LSA. Both human judgments and LSA were remarkably similar and indicated that students who were not complying with SERT tended to paraphrase the text sentences, whereas students who were compliant with SERT tended to explain the sentences in terms of what they knew about the world and of information provided in the prior text context. The similarity between human judgments and LSA indicates that LSA will be useful in accounting for reading strategies in a Web-based version of SERT", "label": ["latent semantic analysis", "reader strategy assessment", "computer-based procedure", "verbal protocols", "self-explanation-reading training", "elaboration", "world knowledge", "text sentence bridging", "human judgments"], "stemmed_label": ["latent semant analysi", "reader strategi assess", "computer-bas procedur", "verbal protocol", "self-explanation-read train", "elabor", "world knowledg", "text sentenc bridg", "human judgment"]}
{"doc": "A universal decomposition of the integration range for exponential functions The problem of determining the independent constants for decomposition of the integration range of exponential functions was solved on the basis of a similar approach to polynomials. The constants obtained enable one to decompose the integration range in two so that the integrals over them are equal independently of the function parameters. For the nontrigonometrical polynomials of even functions, an alternative approach was presented", "label": ["integration range universal decomposition", "exponential functions", "polynomials", "integration range decomposition", "nontrigonometrical polynomials", "even functions"], "stemmed_label": ["integr rang univers decomposit", "exponenti function", "polynomi", "integr rang decomposit", "nontrigonometr polynomi", "even function"]}
{"doc": "Effects of white space in learning via the Web This study measured the effect of specific white space features on learning from instructional Web materials. The study also measured learners' beliefs regarding Web-based instruction. Prior research indicated that small changes in the handling of presentation elements can affect learning. Achievement results from this study indicated that in on-line materials, when content and overall structure are sound, minor differences regarding table borders and vertical spacing in text do not hinder learning. Beliefs regarding Web-based instruction and instructors who use it did not differ significantly between treatment groups. Implications of the study and cautions regarding generalizing from the results are discussed", "label": ["white space features", "web-based instruction", "presentation", "online educational materials", "table borders", "text vertical spacing", "internet"], "stemmed_label": ["white space featur", "web-bas instruct", "present", "onlin educ materi", "tabl border", "text vertic space", "internet"]}
{"doc": "Embedding of level-continuous fuzzy sets on Banach spaces In this paper we present an extension of the Minkowski embedding theorem, showing the existence of an isometric embedding between the classF/sub c/(X) of compact-convex and level-continuous fuzzy sets on a real separable Banach space X and C( 0, 1 * B(X*)), the Banach space of real continuous functions defined on the cartesian product between 0, 1 and the unit ball B(X*) in the dual space X*. Also, by using this embedding, we give some applications to the characterization of relatively compact subsets of F/sub c/(X). In particular, an Ascoli-Arzela type theorem is proved and applied to solving the Cauchy problem x(t) = f(t, x(t)), x(t/sub 0/) = x/sub 0/ on F/sub c/(X)", "label": ["isometric embedding", "level-continuous fuzzy sets", "compact-convex fuzzy sets", "real separable banach space", "real continuous functions", "cartesian product", "unit ball", "dual space", "ascoli-arzela type theorem", "cauchy problem"], "stemmed_label": ["isometr embed", "level-continu fuzzi set", "compact-convex fuzzi set", "real separ banach space", "real continu function", "cartesian product", "unit ball", "dual space", "ascoli-arzela type theorem", "cauchi problem"]}
{"doc": "The creation of a high-fidelity finite element model of the kidney for use in trauma research A detailed finite element model of the human kidney for trauma research has been created directly from the National Library of Medicine Visible Human Female (VHF) Project data set. An image segmentation and organ reconstruction software package has been developed and employed to transform the 2D VHF images into a 3D polygonal representation. Nonuniform rational B-spline (NURBS) surfaces were then mapped to the polygonal surfaces, and were finally utilized to create a robust 3D hexahedral finite element mesh within a commercially available meshing software. The model employs a combined viscoelastic and hyperelastic material model to successfully simulate the behaviour of biological soft tissues. The finite element model was then validated for use in biomechanical research", "label": ["high-fidelity finite element model", "kidney", "trauma research", "national library of medicine", "visible human female project", "medical data set", "image segmentation", "organ reconstruction", "physically based animation", "software package", "3d polygonal representation", "2d vhf images", "nonuniform rational b-spline surfaces", "nurbs", "polygonal surfaces", "3d hexahedral finite element mesh", "viscoelastic model", "hyperelastic material model", "biological soft tissues", "biomechanical research"], "stemmed_label": ["high-fidel finit element model", "kidney", "trauma research", "nation librari of medicin", "visibl human femal project", "medic data set", "imag segment", "organ reconstruct", "physic base anim", "softwar packag", "3d polygon represent", "2d vhf imag", "nonuniform ration b-spline surfac", "nurb", "polygon surfac", "3d hexahedr finit element mesh", "viscoelast model", "hyperelast materi model", "biolog soft tissu", "biomechan research"]}
{"doc": "A comparison of the discounted utility model and hyperbolic discounting models in the case of social and private intertemporal preferences for health Whilst there is substantial evidence that hyperbolic discounting models describe intertemporal preferences for monetary outcomes better than the discounted utility (DU) model, there is only very limited evidence in the context of health outcomes. This study elicits private and social intertemporal preferences for non-fatal changes in health. Specific functional forms of the DU model and three hyperbolic models are fitted. The results show that the stationarity axiom is violated, and that the hyperbolic models fit the data better than the DU model. Intertemporal preferences for private and social decisions are found to be very similar", "label": ["discounted utility model", "hyperbolic discounting models", "intertemporal preferences", "health outcomes", "private decisions", "social decisions"], "stemmed_label": ["discount util model", "hyperbol discount model", "intertempor prefer", "health outcom", "privat decis", "social decis"]}
{"doc": "Bayesian nonstationary autoregressive models for biomedical signal analysis We describe a variational Bayesian algorithm for the estimation of a multivariate autoregressive model with time-varying coefficients that adapt according to a linear dynamical system. The algorithm allows for time and frequency domain characterization of nonstationary multivariate signals and is especially suited to the analysis of event-related data. Results are presented on synthetic data and real electroencephalogram data recorded in event-related desynchronization and photic synchronization scenarios", "label": ["photic synchronization scenarios", "event-related desynchronization", "frequency domain characterization", "time domain characterization", "biomedical signal analysis", "kalman smoother", "eeg analysis", "bayesian nonstationary autoregressive models", "linear dynamical system", "variational bayesian algorithm", "time-varying coefficients"], "stemmed_label": ["photic synchron scenario", "event-rel desynchron", "frequenc domain character", "time domain character", "biomed signal analysi", "kalman smoother", "eeg analysi", "bayesian nonstationari autoregress model", "linear dynam system", "variat bayesian algorithm", "time-vari coeffici"]}
{"doc": "An unconditionally stable extended (USE) finite-element time-domain solution of active nonlinear microwave circuits using perfectly matched layers This paper proposes an extension of the unconditionally stable finite-element time-domain (FETD) method for the global electromagnetic analysis of active microwave circuits. This formulation has two advantages. First, the time-step size is no longer governed by the spatial discretization of the mesh, but rather by the Nyquist sampling criterion. Second, the implementation of the truncation by the perfectly matched layers (PML) is straightforward. An anisotropic PML absorbing material is presented for the truncation of FETD lattices. Reflection less than -50 dB is obtained numerically over the entire propagation bandwidth in waveguide and microstrip line. A benchmark test on a microwave amplifier indicates that this extended FETD algorithm is not only superior to finite-difference time-domain-based algorithm in mesh flexibility and simulation accuracy, but also reduces computation time dramatically", "label": ["unconditionally stable fetd method", "finite-element time-domain method", "global electromagnetic analysis", "global em analysis", "active nonlinear microwave circuits", "nyquist sampling criterion", "time-step size", "pml truncation", "perfectly matched layers", "anisotropic pml absorbing material", "fetd lattices truncation", "waveguide", "microstrip line", "microwave amplifier", "mesh flexibility", "simulation accuracy", "computation time reduction"], "stemmed_label": ["uncondit stabl fetd method", "finite-el time-domain method", "global electromagnet analysi", "global em analysi", "activ nonlinear microwav circuit", "nyquist sampl criterion", "time-step size", "pml truncat", "perfectli match layer", "anisotrop pml absorb materi", "fetd lattic truncat", "waveguid", "microstrip line", "microwav amplifi", "mesh flexibl", "simul accuraci", "comput time reduct"]}
{"doc": "Iterative regularized least-mean mixed-norm image restoration We develop a regularized mixed-norm image restoration algorithm to deal with various types of noise. A mixed-norm functional is introduced, which combines the least mean square (LMS) and the least mean fourth (LMF) functionals, as well as a smoothing functional. Two regularization parameters are introduced: one to determine the relative importance of the LMS and LMF functionals, which is a function of the kurtosis, and another to determine the relative importance of the smoothing functional. The two parameters are chosen in such a way that the proposed functional is convex, so that a unique minimizer exists. An iterative algorithm is utilized for obtaining the solution, and its convergence is analyzed. The novelty of the proposed algorithm is that no knowledge of the noise distribution is required, and the relative contributions of the LMS, the LMF, and the smoothing functionals are adjusted based on the partially restored image. Experimental results demonstrate the effectiveness of the proposed algorithm", "label": ["iterative regularized least-mean mixed-norm image restoration", "noise", "mixed-norm functional", "least mean square functionals", "mean fourth functionals", "smoothing functional", "regularization parameters", "kurtosis", "convex functional", "unique minimizer", "iterative algorithm", "convergence", "noise distribution", "partially restored image"], "stemmed_label": ["iter regular least-mean mixed-norm imag restor", "nois", "mixed-norm function", "least mean squar function", "mean fourth function", "smooth function", "regular paramet", "kurtosi", "convex function", "uniqu minim", "iter algorithm", "converg", "nois distribut", "partial restor imag"]}
{"doc": "International library consortia: positive starts, promising futures Library consortia have grown substantially over the past ten years, both within North America and globally. As this resurgent consortial movement has begun to mature, and as publishers and vendors have begun to adapt to consortial purchasing models, consortia have expanded their agendas for action. The movement to globalize consortia is traced (including the development and current work of the International Coalition of Library Consortia-ICOLC). A methodology is explored to classify library consortia by articulating the key factors that affect and distinguish consortia as organizations within three major areas: strategic, tactical, and practical (or managerial) concerns. Common consortial values are examined, and a list of known international library consortia is presented", "label": ["consortial purchasing models", "international library consortia"], "stemmed_label": ["consorti purchas model", "intern librari consortia"]}
{"doc": "Modeling privacy control in context-aware systems Significant complexity issues challenge designers of context-aware systems with privacy control. Information spaces provide a way to organize information, resources, and services around important privacy-relevant contextual factors. In this article, we describe a theoretical model for privacy control in context-aware systems based on a core abstraction of information spaces. We have previously focused on deriving socially based privacy objectives in pervasive computing environments. Building on Ravi Sandhu's four-layer OM-AM (objectives, models, architectures, and mechanisms) idea, we aim to use information spaces to construct a model for privacy control that supports our socially based privacy objectives. We also discuss how we can introduce decentralization, a desirable property for many pervasive computing systems, into our information space model, using unified privacy tagging", "label": ["privacy", "pervasive computing", "context-aware systems", "privacy control", "smart office"], "stemmed_label": ["privaci", "pervas comput", "context-awar system", "privaci control", "smart offic"]}
{"doc": "The ubiquitous provisioning of internet services to portable devices Advances in mobile telecommunications and device miniaturization call for providing both standard and novel location- and context-dependent Internet services to mobile clients. Mobile agents are dynamic, asynchronous, and autonomous, making the MA programming paradigm suitable for developing novel middleware for mobility-enabled services", "label": ["mobile telecommunications", "device miniaturization", "internet services", "mobile clients", "mobile agents", "mobility-enabled services", "middleware"], "stemmed_label": ["mobil telecommun", "devic miniatur", "internet servic", "mobil client", "mobil agent", "mobility-en servic", "middlewar"]}
{"doc": "The influence of tollbooths on highway traffic We study the effects of tollbooths on the traffic flow. The highway traffic is simulated by the Nagel-Schreckenberg model. Various types of toll collection are examined, which can be characterized either by a waiting time or a reduced speed. A first-order phase transition is observed. The phase separation results a saturated flow, which is observed as a plateau region in the fundamental diagram. The effects of lane expansion near the tollbooth are examined. The full capacity of a highway can be restored. The emergence of vehicle queuing is studied. Besides the numerical results, we also obtain analytical expressions for various quantities. The numerical simulations can be well described by the analytical formulas. We also discuss the influence on the travel time and its variance. The tollbooth increases the travel time but decreases its variance. The differences between long- and short-distance travelers are also discussed", "label": ["highway traffic", "tollbooths", "nagel-schreckenberg model", "toll collection", "waiting time", "reduced speed", "first-order phase transition", "saturated flow", "lane expansion", "vehicle queuing", "numerical simulations"], "stemmed_label": ["highway traffic", "tollbooth", "nagel-schreckenberg model", "toll collect", "wait time", "reduc speed", "first-ord phase transit", "satur flow", "lane expans", "vehicl queu", "numer simul"]}
{"doc": "Optical recognition of three-dimensional objects with scale invariance using a classical convergent correlator We present a real-time method for recognizing three-dimensional (3-D) objects with scale invariance. The 3-D information of the objects is codified in deformed fringe patterns using the Fourier transform profilometry technique and is correlated using a classical convergent correlator. The scale invariance property is achieved using two different approaches: the Mellin radial harmonic decomposition and the logarithmic radial harmonic filter. Thus, the method is invariant for changes in the scale of the 3-D target within a defined interval of scale factors. Experimental results show the utility of the proposed method", "label": ["optical recognition", "3d object recognition", "scale invariance", "classical convergent correlator", "real-time method", "3-d information", "deformed fringe patterns", "fourier transform profilometry technique", "scale invariance property", "mellin radial harmonic decomposition", "logarithmic radial harmonic filter", "invariant", "scale factors"], "stemmed_label": ["optic recognit", "3d object recognit", "scale invari", "classic converg correl", "real-tim method", "3-d inform", "deform fring pattern", "fourier transform profilometri techniqu", "scale invari properti", "mellin radial harmon decomposit", "logarithm radial harmon filter", "invari", "scale factor"]}
{"doc": "Entrepreneurs in Action: a Web-case model Much of the traditional schooling in America is built around systems of compliance and control, characteristics which stifle the creative and entrepreneurial instincts of the children who are subjected to these tactics. The article explores a different approach to education, one that involves capturing the interest of the student through the use of problem and project-based instruction delivered via the Internet. Called Entrepreneurs in Action, this program seeks to involve students in a problem at the outset and to promote the learning of traditional subject areas as a process of the problem-solving activities that are undertaken. The program's details are explained, from elementary school through university level courses, and the authors outline their plans to test the efficacy of the program at each level", "label": ["entrepreneurs in action", "web-case model", "traditional schooling", "america", "entrepreneurial instincts", "project-based instruction", "internet", "traditional subject areas", "problem-solving activities", "elementary school", "university level courses"], "stemmed_label": ["entrepreneur in action", "web-cas model", "tradit school", "america", "entrepreneuri instinct", "project-bas instruct", "internet", "tradit subject area", "problem-solv activ", "elementari school", "univers level cours"]}
{"doc": "High-density remote storage: the Ohio State University Libraries depository The article describes a high-density off-site book storage facility operated by the Ohio State University Libraries. Opened in 1995, it has the capacity to house nearly 1.5 million items in only 9000 square feet by shelving books by size on 30-foot tall shelving. A sophisticated climate control system extends the life of stored materials up to 12 times. An online catalog record for each item informs patrons that the item is located in a remote location. Regular courier deliveries from the storage facility bring requested materials to patrons with minimal delay", "label": ["high-density remote storage", "ohio state university libraries", "high-density off-site book storage facility", "shelving", "climate control system", "stored materials", "patrons", "circulation", "online catalog record", "remote location", "courier deliveries"], "stemmed_label": ["high-dens remot storag", "ohio state univers librari", "high-dens off-sit book storag facil", "shelv", "climat control system", "store materi", "patron", "circul", "onlin catalog record", "remot locat", "courier deliveri"]}
{"doc": "Why your Web strategy is, err, wrong An awkward look at a few standard views from the author, who thinks that most people have got it, err, wrong. Like every other investment, when the time comes to sign the contract, the question that should be asked is not whether it is a good investment, but whether it is the best investment the firm can make with the money. the author argues that he would be surprised if any law firm Web site he has seen yet would jump that particular hurdle", "label": ["web strategy", "law firm web site"], "stemmed_label": ["web strategi", "law firm web site"]}
{"doc": "Two-step integral imaging for orthoscopic three-dimensional imaging with improved viewing resolution We present a two-step integral imaging system to obtain 3-D orthoscopic real images. By adopting a nonstationary micro-optics technique, we demonstrate experimentally the potential usefulness of two-step integral imaging", "label": ["two-step integral imaging", "resolution improved viewing", "two-step integral imaging system", "3-d orthoscopic real images", "nonstationary micro-optics technique", "3-d image reconstruction", "liquid crystal light valve", "display device", "lclv", "pickup lenslet array"], "stemmed_label": ["two-step integr imag", "resolut improv view", "two-step integr imag system", "3-d orthoscop real imag", "nonstationari micro-opt techniqu", "3-d imag reconstruct", "liquid crystal light valv", "display devic", "lclv", "pickup lenslet array"]}
{"doc": "Exact controllability of shells in minimal time We prove an exact controllability result for thin cups using the Fourier method and recent improvements of Ingham (1936) type theorems", "label": ["controllability", "shells", "minimal time", "thin cups", "partial differential equations", "young modulus", "hilbert space", "fourier method", "ingham type theorems"], "stemmed_label": ["control", "shell", "minim time", "thin cup", "partial differenti equat", "young modulu", "hilbert space", "fourier method", "ingham type theorem"]}
{"doc": "Re-examining the machining frictional boundary conditions using fractals Presents experimental evidence for the existence of non-Euclidean contact geometry at the tool-chip interface in the machining of aluminium alloy, which challenges conventional assumptions. The geometry of contact at the tool rake face is modelled using fractals and a dimension is computed for its description. The variation in the fractal dimension with the cutting speed is explored", "label": ["machining frictional boundary conditions", "fractals", "noneuclidean contact geometry", "tool-chip interface", "aluminium alloy", "contact geometry", "tool rake face", "cutting speed", "al"], "stemmed_label": ["machin friction boundari condit", "fractal", "noneuclidean contact geometri", "tool-chip interfac", "aluminium alloy", "contact geometri", "tool rake face", "cut speed", "al"]}
{"doc": "Improving computer security for authentication of users: influence of proactive password restrictions Entering a user name-password combination is a widely used procedure for identification and authentication in computer systems. However, it is a notoriously weak method, in that the passwords adopted by many users are easy to crack. In an attempt to, improve security, proactive password checking may be used, in which passwords must meet several criteria to be more resistant to cracking. In two experiments, we examined the influence of proactive password restrictions on the time that it took to generate an acceptable password and to use it subsequently to log in. The required length was a minimum of five characters in experiment I and eight characters in experiment 2. In both experiments, one condition had only the length restriction, and the other had additional restrictions. The additional restrictions greatly increased the time it took to generate the password but had only a small effect on the time it took to use it subsequently to log in. For the five-character passwords, 75 were cracked when no other restrictions were imposed, and this was reduced to 33 with the additional restrictions. For the eight-character passwords, 17 were cracked with no other restrictions, and 12.5 with restrictions. The results indicate that increasing the minimum character length reduces crackability and increases security, regardless of whether additional restrictions are imposed", "label": ["computer security", "user authentication", "proactive password checking", "proactive password restrictions", "length restriction", "five-character passwords", "eight-character passwords"], "stemmed_label": ["comput secur", "user authent", "proactiv password check", "proactiv password restrict", "length restrict", "five-charact password", "eight-charact password"]}
{"doc": "Computer program to generate operant schedules A computer program for programming schedules of reinforcement is described. Students can use the program to experience schedules of reinforcement that are typically used with nonhuman subjects. Accumulative recording of a student's response can be shown on the screen and/or printed with the computer's printer. The program can also be used to program operant schedules for animal subjects. The program was tested with human subjects experiencing fixed ratio, variable ratio, fixed interval, and variable interval schedules. Performance for human subjects on a given schedule was similar to performance for nonhuman subjects on the same schedule", "label": ["operant schedule generation", "computer program", "reinforcement schedule programming", "nonhuman subjects", "cumulative student response recording", "animal subjects", "fixed ratio schedules", "variable ratio schedules", "fixed interval schedules", "variable interval schedules", "human subjects"], "stemmed_label": ["oper schedul gener", "comput program", "reinforc schedul program", "nonhuman subject", "cumul student respons record", "anim subject", "fix ratio schedul", "variabl ratio schedul", "fix interv schedul", "variabl interv schedul", "human subject"]}
{"doc": "Using the Web to answer legal reference questions In an effort to help non-law librarians with basic legal reference questions, the author highlights three basic legal Web sites and outlines useful subject-specific Web sites that focus on statutes and regulations, case law and attorney directories", "label": ["world wide web", "legal reference questions", "nonlaw librarians", "case law", "attorney directories"], "stemmed_label": ["world wide web", "legal refer question", "nonlaw librarian", "case law", "attorney directori"]}
{"doc": "Novel ZE-isomerism descriptors derived from molecular topology and their application to QSAR analysis We introduce several series of novel ZE-isomerism descriptors derived directly from two-dimensional molecular topology. These descriptors make use of a quantity named ZE-isomerism correction, which is added to the vertex degrees of atoms connected by double bonds in Z and E configurations. This approach is similar to the one described previously for topological chirality descriptors (Golbraikh, A., et al. J. Chem. Inf. Comput. Sci. 2001, 41, 147-158). The ZE-isomerism descriptors include modified molecular connectivity indices, overall Zagreb indices, extended connectivity, overall connectivity, and topological charge indices. They can be either real or complex numbers. Mathematical properties of different subgroups of ZE-isomerism descriptors are discussed. These descriptors circumvent the inability of conventional topological indices to distinguish between Z and E isomers. The applicability of ZE-isomerism descriptors to QSAR analysis is demonstrated in the studies of a series of 131 anticancer agents inhibiting tubulin polymerization", "label": ["ze-isomerism descriptors", "two-dimensional molecular topology", "qsar analysis", "quantitative structure-activity relationship", "ze-isomerism correction", "vertex degrees", "double bond connected atoms", "modified molecular connectivity indices", "overall zagreb indices", "extended connectivity", "overall connectivity", "topological charge indices", "complex numbers", "anticancer agents", "tubulin polymerization", "descriptor pharmacophore", "chemical databases", "molecular graphs", "computer-assisted drug design", "toxicities", "combinatorial chemical libraries"], "stemmed_label": ["ze-isomer descriptor", "two-dimension molecular topolog", "qsar analysi", "quantit structure-act relationship", "ze-isomer correct", "vertex degre", "doubl bond connect atom", "modifi molecular connect indic", "overal zagreb indic", "extend connect", "overal connect", "topolog charg indic", "complex number", "anticanc agent", "tubulin polymer", "descriptor pharmacophor", "chemic databas", "molecular graph", "computer-assist drug design", "toxic", "combinatori chemic librari"]}
{"doc": "Three-dimensional optimum design of the cooling lines of injection moulds based on boundary element design sensitivity analysis A three-dimensional numerical simulation using the boundary element method is proposed, which can predict the cavity temperature distributions in the cooling stage of injection moulding. Then, choosing the radii and positions of cooling lines as design variables, the boundary integral sensitivity formulations are deduced. For the optimum design of cooling lines, the squared difference between the objective temperature and temperature of the cavity is taken as the objective function. Based on the optimization techniques with design sensitivity analysis, an iterative algorithm to reach the minimum value of the objective function is introduced, which leads to the optimum design of cooling lines at the same time", "label": ["injection moulding", "3d numerical simulation", "boundary element method", "cavity temperature distributions", "cooling stage", "boundary integral sensitivity analysis", "iterative algorithm", "heat conduction", "objective function", "optimization"], "stemmed_label": ["inject mould", "3d numer simul", "boundari element method", "caviti temperatur distribut", "cool stage", "boundari integr sensit analysi", "iter algorithm", "heat conduct", "object function", "optim"]}
{"doc": "Phase control of higher-order squeezing of a quantum field In a recent experiment Phys. Rev. Lett. 88 (2002) 023601 , phase-dependent photon statistics in a c.w. system has been observed in the mixing of a coherent field with a two-photon source. Their system has the advantage over other atomic transition-based fluorescent systems. In this paper, we examine further the squeezing properties of higher-order quantum fluctuations in one of the quadrature components of the combined field in this system. We demonstrate that efficient and lasting higher-order squeezing effects could be observed with proper choice of the relative phase between the pump and coherent fields. This nonclassical feature is attributed to a constructive two-photon interference. Relationship between the second- and higher-order squeezing of the field is discussed", "label": ["phase control", "higher-order squeezing", "quantum field", "phase-dependent photon statistics", "coherent field mixing", "atomic transition-based fluorescent systems", "quantum fluctuations", "two-photon interference"], "stemmed_label": ["phase control", "higher-ord squeez", "quantum field", "phase-depend photon statist", "coher field mix", "atom transition-bas fluoresc system", "quantum fluctuat", "two-photon interfer"]}
{"doc": "NuVox shows staying power with new cash, new market Who says you can't raise cash in today's telecom market? NuVox Communications positions itself for the long run with $78.5 million in funding and a new credit facility", "label": ["telecom", "competitive carrier market", "nuvox communications", "investors"], "stemmed_label": ["telecom", "competit carrier market", "nuvox commun", "investor"]}
{"doc": "Electronic data exchange for real estate With HM Land Registry's consultation now underway, no one denies that the property industry is facing a period of unprecedented change. PISCES (Property Information Systems Common Exchange) is a property-focused electronic data exchange standard. The standard is a set of definitions and rules to facilitate electronic transfer of data between key business areas and between different types of software packages that are used regularly by the property industry. It is not itself a piece of software but an enabling technology that allows software providers to prepare solutions within their own packages to transfer data between databases. This provides the attractive prospect of seamless transfer of data within and between systems and organisations", "label": ["hm land registry", "property industry", "pisces", "property information systems common exchange", "electronic data exchange", "standard", "software packages", "databases", "seamless transfer"], "stemmed_label": ["hm land registri", "properti industri", "pisc", "properti inform system common exchang", "electron data exchang", "standard", "softwar packag", "databas", "seamless transfer"]}
{"doc": "Nissan v. Nissan trademark dispute Is a trademark dispute a case of David v. Goliath or a corporation fending off a greedy opportunist? This paper discusses the case of Uzi Nissan, who is locked in a multimillion-dollar legal battle over whether or not his use of the nissan.com Internet domain name infringes upon Japan's Nissan Motor Co.'s trademark. At the heart of the matter is the impact of the global Internet on trademark law, which traditionally has been strongly influenced by geographic considerations. The paper discusses the background to the case from both sides and the issues involved", "label": ["trademark dispute", "uzi nissan", "nissan.com internet domain name", "nissan motor company trademark", "global internet", "trademark law"], "stemmed_label": ["trademark disput", "uzi nissan", "nissan.com internet domain name", "nissan motor compani trademark", "global internet", "trademark law"]}
{"doc": "Information security policy - what do international information security standards say? One of the most important information security controls, is the information security policy. This vital direction-giving document is, however, not always easy to develop and the authors thereof battle with questions such as what constitutes a policy. This results in the policy authors turning to existing sources for guidance. One of these sources is the various international information security standards. These standards are a good starting point for determining what the information security policy should consist of, but should not be relied upon exclusively for guidance. Firstly, they are not comprehensive in their coverage and furthermore, tending to rather address the processes needed for successfully implementing the information security policy. It is far more important the information security policy must fit in with the organisation's culture and must therefore be developed with this in mind", "label": ["information security policy", "international information security standards"], "stemmed_label": ["inform secur polici", "intern inform secur standard"]}
{"doc": "In search of a general enterprise model Many organisations, particularly SMEs, are reluctant to invest time and money in models to support decision making. Such reluctance could be overcome if a model could be used for several purposes rather than using a traditional \"single perspective\" model. This requires the development of a \"general enterprise model\" (GEM), which can be applied to a wide range of problem domains with unlimited scope. Current enterprise modelling frameworks only deal effectively with nondynamic modelling issues whilst dynamic modelling issues have traditionally only been addressed at the operational level. Although the majority of research in this area relates to manufacturing companies, the framework for a GEM must be equally applicable to service and public sector organisations. The paper identifies five key design issues that need to be considered when constructing a GEM. A framework for such a GEM is presented based on a \"plug and play\" methodology and demonstrated by a simple case study", "label": ["general enterprise model", "business process re-engineering", "smes", "decision making", "single perspective model", "gem", "problem domains", "enterprise modelling frameworks", "operational level", "dynamic modelling issues", "public sector organisations", "service sector organisations", "plug and play methodology", "case study"], "stemmed_label": ["gener enterpris model", "busi process re-engin", "sme", "decis make", "singl perspect model", "gem", "problem domain", "enterpris model framework", "oper level", "dynam model issu", "public sector organis", "servic sector organis", "plug and play methodolog", "case studi"]}
{"doc": "System embedding. Control with reduced observer Two interrelated problems-design of the reduced observer of plant state separately and together with its control system-were considered from the standpoint of designing the multivariable linear systems from the desired matrix transfer functions. The matrix equations defining the entire constructive class of solutions of the posed problems were obtained using the system embedding technology. As was demonstrated, control based on the reduced observer is capable to provide the desired response to the control input, as well as the response to the nonzero initial conditions, only for the directly measurable part of the components of the state vector. An illustrative example was presented", "label": ["system embedding", "reduced observer control", "reduced plant state observer design", "multivariable linear systems", "matrix transfer functions", "state vector"], "stemmed_label": ["system embed", "reduc observ control", "reduc plant state observ design", "multivari linear system", "matrix transfer function", "state vector"]}
{"doc": "Fresh tracks food processing Bar code labels and wireless terminals linked to a centralized database accurately track meat products from receiving to customers for Farmland Foods", "label": ["food processing", "bar code labels", "wireless terminals", "farmland foods", "automatic data capture", "intermec technologies"], "stemmed_label": ["food process", "bar code label", "wireless termin", "farmland food", "automat data captur", "intermec technolog"]}
{"doc": "Horizontal waypoint guidance design using optimal control A horizontal waypoint guidance algorithm is proposed by applying line-following guidance to waypoint line segments in sequence. The line-following guidance is designed using an LQR (linear quadratic regulator). Then, the optimal waypoint changing points are derived by minimizing the accelerations required for changing the waypoint line segments. Also derived is a sufficient condition for the stability bound of ground speed changes based on the Lyapunov stability theorem. Simulation results show that the proposed algorithm can effectively guide a vehicle along the sequence of waypoint line segments", "label": ["horizontal waypoint guidance algorithm", "line-following guidance", "waypoint line segments", "lqr", "linear quadratic regulator", "optimal waypoint changing points", "stability bound", "ground speed changes", "lyapunov stability theorem", "unmanned flying vehicle", "threat avoidance", "terrain masking", "attack directions", "target location arrival time"], "stemmed_label": ["horizont waypoint guidanc algorithm", "line-follow guidanc", "waypoint line segment", "lqr", "linear quadrat regul", "optim waypoint chang point", "stabil bound", "ground speed chang", "lyapunov stabil theorem", "unman fli vehicl", "threat avoid", "terrain mask", "attack direct", "target locat arriv time"]}
{"doc": "Geotensity: combining motion and lighting for 3D surface reconstruction This paper is about automatically reconstructing the full 3D surface of an object observed in motion by a single static camera. Based on the two paradigms, structure from motion and linear intensity subspaces, we introduce the geotensity constraint that governs the relationship between four or more images of a moving object. We show that it is possible in theory to solve for 3D Lambertian surface structure for the case of a single point light source and propose that a solution exists for an arbitrary number point light sources. The surface may or may not be textured. We then give an example of automatic surface reconstruction of a face under a point light source using arbitrary unknown object motion and a single fixed camera", "label": ["full 3d surface", "single static camera", "linear intensity subspaces", "geotensity constraint", "3d lambertian surface structure", "single point light source", "arbitrary number point light sources", "automatic surface reconstruction", "point light source", "linear image subspaces", "structure-from-motion"], "stemmed_label": ["full 3d surfac", "singl static camera", "linear intens subspac", "geotens constraint", "3d lambertian surfac structur", "singl point light sourc", "arbitrari number point light sourc", "automat surfac reconstruct", "point light sourc", "linear imag subspac", "structure-from-mot"]}
{"doc": "Innovative manufacture of impulse turbine blades for wave energy power conversion An innovative approach to the manufacture of impulse turbine blades using rapid prototyping, fused decomposition modelling (FDM), is presented. These blades were designed and manufactured by the Wave Energy Research Team (WERT) at the University of Limerick for the experimental analysis of a 0.6 m impulse turbine with fixed guide vanes for wave energy power conversion. The computer aided design/manufacture (CAD/CAM) package Pro-Engineer 2000i was used for three-dimensional solid modelling of the individual blades. A detailed finite element analysis of the blades under centrifugal loads was performed using Pro-Mechanica. based on this analysis and FDM machine capabilities, blades were redesigned. Finally, Pro-E data were transferred to an FDM machine for the manufacture of turbine blades. The objective of this paper is to present the innovative method used to design, modify and manufacture blades in a time and cost effective manner using a concurrent engineering approach", "label": ["cad/cam", "impulse turbine blades", "wave energy power conversion", "fused decomposition modelling", "rapid prototyping", "manufacturing", "concurrent engineering", "university of limerick", "solid modelling", "finite element analysis"], "stemmed_label": ["cad/cam", "impuls turbin blade", "wave energi power convers", "fuse decomposit model", "rapid prototyp", "manufactur", "concurr engin", "univers of limerick", "solid model", "finit element analysi"]}
{"doc": "Product and process innovations in the life cycle of an industry Filson (2001) uses industry-level data on firm numbers, price, quantity and quality along with an equilibrium model of industry evolution to estimate the nature and effects of quality and cost improvements in the personal computer industry and four other new industries. This paper studies the personal computer industry in more detail and shows that the model explains some peculiar patterns that cannot be explained by previous life-cycle models. The model estimates are evaluated using historical studies of the evolution of the personal computer industry and patterns that require further model development are described", "label": ["technological change", "life-cycle models", "industry dynamics", "personal computer market", "microelectronics", "equilibrium model", "industry evolution", "pc industry", "production cost"], "stemmed_label": ["technolog chang", "life-cycl model", "industri dynam", "person comput market", "microelectron", "equilibrium model", "industri evolut", "pc industri", "product cost"]}
{"doc": "Time-varying properties of renal autoregulatory mechanisms In order to assess the possible time-varying properties of renal autoregulation, time-frequency and time-scaling methods were applied to renal blood flow under broad-band forced arterial blood pressure fluctuations and single-nephron renal blood flow with spontaneous oscillations obtained from normotensive (Sprague-Dawley, Wistar, and Long-Evans) rats, and spontaneously hypertensive rats. Time-frequency analyses of normotensive and hypertensive blood flow data obtained from either the whole kidney or the single-nephron show that indeed both the myogenic and tubuloglomerular feedback (TGF) mechanisms have time-varying characteristics. Furthermore, we utilized the Renyi entropy to measure the complexity of blood-flow dynamics in the time-frequency plane in an effort to discern differences between normotensive and hypertensive recordings. We found a clear difference in Renyi entropy between normotensive and hypertensive blood flow recordings at the whole kidney level for both forced (p 0.037) and spontaneous arterial pressure fluctuations (p 0.033), and at the single-nephron level (p 0.008). Especially at the single-nephron level, the mean Renyi entropy is significantly larger for hypertensive than normotensive rats, suggesting more complex dynamics in the hypertensive condition. To further evaluate whether or not the separation of dynamics between normotensive and hypertensive rats is found in the prescribed frequency ranges of the myogenic and TGF mechanisms, we employed multiresolution wavelet transform. Our analysis revealed that exclusively over scale ranges corresponding to the frequency intervals of the myogenic and TGF mechanisms, the widths of the blood flow wavelet coefficients fall into disjoint sets for normotensive and hypertensive rats. The separation of the scales at the myogenic and TGF frequency ranges is distinct and obtained with 100 accuracy. However, this observation remains valid only for the whole kidney blood pressure/flow data. The results suggest that understanding of the time-varying properties of the two mechanisms is required for a complete description of renal autoregulation", "label": ["time-varying properties", "sprague-dawley rats", "wistar rats", "long-evans rats", "whole kidney", "single-nephron", "renyi entropy", "spontaneous arterial pressure fluctuations", "hypertensive rats", "normotensive rats", "renal autoregulatory mechanisms", "broad-band forced arterial blood pressure fluctuations", "single-nephron renal blood flow", "spontaneous oscillations"], "stemmed_label": ["time-vari properti", "sprague-dawley rat", "wistar rat", "long-evan rat", "whole kidney", "single-nephron", "renyi entropi", "spontan arteri pressur fluctuat", "hypertens rat", "normotens rat", "renal autoregulatori mechan", "broad-band forc arteri blood pressur fluctuat", "single-nephron renal blood flow", "spontan oscil"]}
{"doc": "Using Internet search engines to estimate word frequency The present research investigated Internet search engines as a rapid, cost-effective alternative for estimating word frequencies. Frequency estimates for 382 words were obtained and compared across four methods: (1) Internet search engines, (2) the Kucera and Francis (1967) analysis of a traditional linguistic corpus, (3) the CELEX English linguistic database (Baayen et al., 1995), and (4) participant ratings of familiarity. The results showed that Internet search engines produced frequency estimates that were highly consistent with those reported by Kucera and Francis and those calculated from CELEX, highly consistent across search engines, and very reliable over a 6 month period of time. Additional results suggested that Internet search engines are an excellent option when traditional word frequency analyses do not contain the necessary data (e.g., estimates for forenames and slang). In contrast, participants' familiarity judgments did not correspond well with the more objective estimates of word frequency. Researchers are advised to use search engines with large databases (e.g., AltaVista) to ensure the greatest representativeness of the frequency estimates", "label": ["internet search engines", "word frequency estimation", "linguistic corpus", "celex english linguistic database", "participant familiarity ratings", "large databases"], "stemmed_label": ["internet search engin", "word frequenc estim", "linguist corpu", "celex english linguist databas", "particip familiar rate", "larg databas"]}
{"doc": "An algorithm to generate all spanning trees with flow Spanning tree enumeration in undirected graphs is an important issue and task in many problems encountered in computer network and circuit analysis. This paper discusses the spanning tree with flow for the case that there are flow requirements between each node pair. An algorithm based on minimal paths (MPs) is proposed to generate all spanning trees without flow. The proposed algorithm is a structured approach, which splits the system into structural MPs first, and also all steps in it are easy to follow", "label": ["undirected graphs", "spanning trees", "minimal paths", "computer network analysis", "circuit analysis"], "stemmed_label": ["undirect graph", "span tree", "minim path", "comput network analysi", "circuit analysi"]}
{"doc": "Vendor qualifications for IT staff and networking In some cases, vendor-run accreditation schemes can offer an objective measure of a job applicant's skills, but they do not always indicate the true extent of practical abilities", "label": ["vendor-run accreditation schemes", "job applicant", "it staff", "network administrators", "practical abilities"], "stemmed_label": ["vendor-run accredit scheme", "job applic", "it staff", "network administr", "practic abil"]}
{"doc": "On the emergence of rules in neural networks A simple associationist neural network learns to factor abstract rules (i.e., grammars) from sequences of arbitrary input symbols by inventing abstract representations that accommodate unseen symbol sets as well as unseen but similar grammars. The neural network is shown to have the ability to transfer grammatical knowledge to both new symbol vocabularies and new grammars. Analysis of the state-space shows that the network learns generalized abstract structures of the input and is not simply memorizing the input strings. These representations are context sensitive, hierarchical, and based on the state variable of the finite-state machines that the neural network has learned. Generalization to new symbol sets or grammars arises from the spatial nature of the internal representations used by the network, allowing new symbol sets to be encoded close to symbol sets that have already been learned in the hidden unit space of the network. The results are counter to the arguments that learning algorithms based on weight adaptation after each exemplar presentation (such as the long term potentiation found in the mammalian nervous system) cannot in principle extract symbolic knowledge from positive examples as prescribed by prevailing human linguistic theory and evolutionary psychology", "label": ["associationist neural network", "learns", "abstract rules", "neural network", "state-space", "symbolic knowledge", "cognitive neurosciences", "associationist learning"], "stemmed_label": ["associationist neural network", "learn", "abstract rule", "neural network", "state-spac", "symbol knowledg", "cognit neurosci", "associationist learn"]}
{"doc": "Stochastic systems with a random jump in phase trajectory: stability of their motions The probabilistic stability of the perturbed motion of a system with parameters under the action of a general Markov process is studied. The phase vector is assumed to experience random jumps when the structure the system suffers random jumps. Such a situation is encountered, for example, in the motion of a solid with random jumps in its mass. The mean-square stability of random-structure linear systems and stability. of nonlinear systems in the first approximation are studied. The applied approach is helpful in studying the asymptotic probabilistic stability and mean-square exponential stability of stochastic systems through the stability of the respective deterministic systems", "label": ["stochastic systems", "random jump", "phase trajectory", "general markov process", "asymptotic probabilistic stability", "mean-square exponential stability"], "stemmed_label": ["stochast system", "random jump", "phase trajectori", "gener markov process", "asymptot probabilist stabil", "mean-squar exponenti stabil"]}
{"doc": "Mobile computing \"Killer app\" competition Design competitions offer students an excellent way to gain hands-on experience in engineering and computer science courses. The University of Florida, in partnership with Motorola, has held two mobile computing design competitions. In Spring and Fall 2001, students in Abdelsalam Helal's Mobile Computing class designed killer apps for a Motorola smart phone", "label": ["mobile computing", "smart phone", "motorola", "design competitions"], "stemmed_label": ["mobil comput", "smart phone", "motorola", "design competit"]}
{"doc": "WEXTOR: a Web-based tool for generating and visualizing experimental designs and procedures WEXTOR is a Javascript-based experiment generator and teaching tool on the World Wide Web that can be used to design laboratory and Web experiments in a guided step-by-step process. It dynamically creates the customized Web pages and Javascripts needed for the experimental procedure and provides experimenters with a print-ready visual display of their experimental design. WEXTOR flexibly supports complete and incomplete factorial designs with between-subjects, within-subjects, and quasi-experimental factors, as well as mixed designs. The software implements client-side response time measurement and contains a content wizard for creating interactive materials, as well as dependent measures (graphical scales, multiple-choice items, etc.), on the experiment pages. However, it does not aim to replace a full-fledged HTML editor. Several methodological features specifically needed in Web experimental design have been implemented in the Web-based tool and are described in this paper. WEXTOR is platform independent. The created Web pages can be uploaded to any type of Web server in which data may be recorded in logfiles or via a database. The current version of WEXTOR is freely available for educational and noncommercial purposes. Its Web address is http://www.genpsylab.unizh.ch/wextor/index.html", "label": ["wextor", "web-based tool", "experimental design visualization", "javascript-based experiment generator", "teaching tool", "world wide web", "customized web pages", "print-ready visual display", "factorial designs", "client-side response time measurement", "content wizard", "html", "web server", "logfiles", "database", "free software"], "stemmed_label": ["wextor", "web-bas tool", "experiment design visual", "javascript-bas experi gener", "teach tool", "world wide web", "custom web page", "print-readi visual display", "factori design", "client-sid respons time measur", "content wizard", "html", "web server", "logfil", "databas", "free softwar"]}
{"doc": "Warranty reserves for nonstationary sales processes Estimation of warranty costs, in the event of product failure within the warranty period, is of importance to the manufacturer. Costs associated with replacement or repair of the product are usually drawn from a warranty reserve fund created by the manufacturer. Considering a stochastic sales process, first and second moments (and thereby the variance) are derived for the manufacturer's total discounted warranty cost of a single sale for single-component items under four different warranty policies from a manufacturer's point of view. These servicing strategies represent a renewable free-replacement, nonrenewable free-replacement, renewable pro-rata, and a nonrenewable minimal-repair warranty plans. The results are extended to determine the mean and variance of total discounted warranty costs for the total sales over the life cycle of the product. Furthermore, using a normal approximation, warranty reserves necessary for a certain protection level, so that reserves are not completely depleted, are found. Results and their managerial implications are studied through an extensive example", "label": ["nonstationary sales processes", "warranty reserves", "warranty costs estimation", "product failure", "product replacement", "product repair", "stochastic sales process", "first moments", "second moments", "variance", "total discounted warranty cost", "single-component items", "servicing strategies", "renewable free-replacement", "nonrenewable free-replacement", "renewable pro-rata", "nonrenewable minimal-repair warranty plans", "total discounted warranty costs", "product life cycle", "normal approximation", "managerial implications"], "stemmed_label": ["nonstationari sale process", "warranti reserv", "warranti cost estim", "product failur", "product replac", "product repair", "stochast sale process", "first moment", "second moment", "varianc", "total discount warranti cost", "single-compon item", "servic strategi", "renew free-replac", "nonrenew free-replac", "renew pro-rata", "nonrenew minimal-repair warranti plan", "total discount warranti cost", "product life cycl", "normal approxim", "manageri implic"]}
{"doc": "MATLAB code for plotting ambiguity functions A MATLAB code capable of plotting ambiguity functions of many different radar signals is presented. The program makes use of MATLAB's sparse matrix operations, and avoids loops. The program could be useful as a pedagogical tool in radar courses teaching pulse compression", "label": ["matlab code", "ambiguity functions plotting", "radar signals", "sparse matrix operations", "pedagogical tool", "radar courses", "pulse compression", "matched-filter response", "doppler-shifted signal version"], "stemmed_label": ["matlab code", "ambigu function plot", "radar signal", "spars matrix oper", "pedagog tool", "radar cours", "puls compress", "matched-filt respons", "doppler-shift signal version"]}
{"doc": "Defining electronic librarianship: a content analysis of job advertisements Advances in technology create dramatic changes within libraries. The complex issues surrounding this new electronic, end-user environment have major ramifications and require expert knowledge. Electronic services librarians and electronic resources librarians are two specialized titles that have recently emerged within the field of librarianship to fill this niche. Job advertisements listed in American Libraries from January 1989 to December 1998 were examined to identify responsibilities, qualifications, organizational and salary information relating to the newly emerging role of electronic librarian", "label": ["electronic librarianship", "content analysis", "job advertisements", "electronic end-user environment", "electronic resources librarians", "electronic services librarians", "american libraries", "responsibilities", "qualifications", "organizational information", "salary information"], "stemmed_label": ["electron librarianship", "content analysi", "job advertis", "electron end-us environ", "electron resourc librarian", "electron servic librarian", "american librari", "respons", "qualif", "organiz inform", "salari inform"]}
{"doc": "A context-aware decision engine for content adaptation Building a good content adaptation service for mobile devices poses many challenges. To meet these challenges, this quality-of-service-aware decision engine automatically negotiates for the appropriate adaptation decision for synthesizing an optimal content version", "label": ["content adaptation", "mobile devices", "quality-of-service-aware", "decision engine", "optimal content version", "adaptation decision"], "stemmed_label": ["content adapt", "mobil devic", "quality-of-service-awar", "decis engin", "optim content version", "adapt decis"]}
{"doc": "Control in active systems based on criteria and motivation For active systems where the principal varies the agents' goal functions by adding to them appropriately weighted goal functions of other agents or a balanced system of inter-agent transfers, the paper formulated and solved the problems of control based on criteria and motivation. Linear active systems were considered by way of example", "label": ["goal functions", "inter-agent transfers", "linear active systems", "criteria-based control", "motivation-based control"], "stemmed_label": ["goal function", "inter-ag transfer", "linear activ system", "criteria-bas control", "motivation-bas control"]}
{"doc": "The variance of firm growth rates: the 'scaling' puzzle Recent evidence suggests that a power-law relationship exists between a firm's size and the variance of its growth rate. The flatness of the relation is regarded as puzzling, in that it suggests that large firms are not much more stable than small firms. It has been suggested that the powerlaw nature of the relationship reflects the presence of some form of correlation of growth rates across the firm's constituent businesses. Here, it is shown that a model of independent businesses which allows for the fact that these businesses vary in size, as modelled by a simple 'partitions of integers' model, provides a good representation of what is observed empirically", "label": ["firm growth rates", "scaling puzzle", "power-law", "flatness", "correlation", "constituent businesses", "partitions of integers model", "size distribution", "corporate growth"], "stemmed_label": ["firm growth rate", "scale puzzl", "power-law", "flat", "correl", "constitu busi", "partit of integ model", "size distribut", "corpor growth"]}
{"doc": "World's biggest battery helps to stabilise Alaska In this paper, the author describes a battery energy storage system which is under construction to provide voltage compensation in support of Alaska's 138 kV Northern Intertie", "label": ["power system stabilisation", "battery energy storage system", "voltage compensation", "usa", "interconnected power systems", "138 kv", "77 mw"], "stemmed_label": ["power system stabilis", "batteri energi storag system", "voltag compens", "usa", "interconnect power system", "138 kv", "77 mw"]}
{"doc": "Virus hunting We all appreciate the need for, and hopefully we have all deployed, anti-virus software. The good news is that AV software has come a long way fast. Four or so years ago it was true to write that AV software could not detect Trojan Horses and similar intrusion attempts. Now it can and does. McAfee's VirusScan, for example, goes one further; it detects viruses, worms and Trojan Horses and deploys itself as a firewall to filter data packets, control access to Internet resources, activate rule sets for specific applications, in general to protect against hackers. But like so much software, we use it with little thought as to how it came to do its job. Behind the scenes there is an army of top notch programmers trying to stay ahead of the baddies who, at the last count, had produced some 60,000 viruses", "label": ["anti-virus software", "programmers", "worms", "trojan horses"], "stemmed_label": ["anti-viru softwar", "programm", "worm", "trojan hors"]}
{"doc": "Duality revisited: construction of fractional frequency distributions based on two dual Lotka laws Fractional frequency distributions of, for example, authors with a certain (fractional) number of papers are very irregular, and therefore not easy to model or to explain. The article gives a first attempt to this by as suming two simple Lotka laws (with exponent 2): one for the number of authors with n papers (total count here) and one for the number of papers with n authors, n in N. Based on an earlier made convolution model of Egghe, interpreted and reworked now for discrete scores, we are able to produce theoretical fractional frequency distributions with only one parameter, which are in very close agreement with the practical ones as found in a large dataset produced earlier by Rao (1995). The article also shows that (irregular) fractional frequency distributions are a consequence of Lotka's law, and are not examples of breakdowns of this famous historical law", "label": ["dual lotka laws", "convolution model", "discrete scores", "irregular fractional frequency distributions"], "stemmed_label": ["dual lotka law", "convolut model", "discret score", "irregular fraction frequenc distribut"]}
{"doc": "System embedding. Polynomial equations The class of solutions of the polynomial equations including their generalizations in the form of the Bezout matrix identities was constructed analytically using the technology of constructive system embedding. The structure of a solution depends on the number of steps of the Euclidean algorithm and is obtained explicitly by appropriate substitutions. Illustrative and descriptive examples are presented", "label": ["determinate systems", "polynomial equations", "bezout matrix identities", "constructive system embedding", "euclidean algorithm"], "stemmed_label": ["determin system", "polynomi equat", "bezout matrix ident", "construct system embed", "euclidean algorithm"]}
{"doc": "Computer processing of data on mental impairments during the acute period of concussion The article presents results of computer processing of experimental information obtained from patients during the acute period of concussion. A number of computational procedures are described", "label": ["computer processing", "mental impairments", "acute period of concussion", "computational procedures"], "stemmed_label": ["comput process", "mental impair", "acut period of concuss", "comput procedur"]}
{"doc": "Optimization of planning an advertising campaign of goods and services A generalization of the mathematical model and operations research problems formulated on its basis, which were presented by Belenky (2001) in the framework of an approach to planning an advertising campaign of goods and services, is considered, and corresponding nonlinear programming problems with linear constraints are formulated", "label": ["optimization", "advertising campaign planning", "operations research", "or", "nonlinear programming"], "stemmed_label": ["optim", "advertis campaign plan", "oper research", "or", "nonlinear program"]}
{"doc": "Learning weights for the quasi-weighted means We study the determination of weights for quasi-weighted means (also called quasi-linear means) when a set of examples is given. We consider first a simple case, the learning of weights for weighted means, and then we extend the approach to the more general case of a quasi-weighted mean. We consider the case of a known arbitrary generator f. The paper finishes considering the use of parametric functions that are suitable when the values to aggregate are measure values or ratio", "label": ["quasi-weighted means", "quasi-linear means", "learning", "parametric functions", "measure values", "ratio values"], "stemmed_label": ["quasi-weight mean", "quasi-linear mean", "learn", "parametr function", "measur valu", "ratio valu"]}
{"doc": "Scale-invariant segmentation of dynamic contrast-enhanced perfusion MR images with inherent scale selection Selection of the best set of scales is problematic when developing signal-driven approaches for pixel-based image segmentation. Often, different possibly conflicting criteria need to be fulfilled in order to obtain the best trade-off between uncertainty (variance) and location accuracy. The optimal set of scales depends on several factors: the noise level present in the image material, the prior distribution of the different types of segments, the class-conditional distributions associated with each type of segment as well as the actual size of the (connected) segments. We analyse, theoretically and through experiments, the possibility of using the overall and class-conditional error rates as criteria for selecting the optimal sampling of the linear and morphological scale spaces. It is shown that the overall error rate is optimized by taking the prior class distribution in the image material into account. However, a uniform (ignorant) prior distribution ensures constant class-conditional error rates. Consequently, we advocate for a uniform prior class distribution when an uncommitted, scale-invariant segmentation approach is desired. Experiments with a neural net classifier developed for segmentation of dynamic magnetic resonance (MR) images, acquired with a paramagnetic tracer, support the theoretical results. Furthermore, the experiments show that the addition of spatial features to the classifier, extracted from the linear or morphological scale spaces, improves the segmentation result compared to a signal-driven approach based solely on the dynamic MR signal. The segmentation results obtained from the two types of features are compared using two novel quality measures that characterize spatial properties of labelled images", "label": ["scale-invariant segmentation", "dynamic contrast-enhanced perfusion mr images", "inherent scale selection", "pixel-based image segmentation", "noise level", "class-conditional error rates", "optimal sampling", "experiments", "neural net classifier", "dynamic magnetic resonance images", "paramagnetic tracer", "quality measures", "labelled images", "class-conditional distributions"], "stemmed_label": ["scale-invari segment", "dynam contrast-enhanc perfus mr imag", "inher scale select", "pixel-bas imag segment", "nois level", "class-condit error rate", "optim sampl", "experi", "neural net classifi", "dynam magnet reson imag", "paramagnet tracer", "qualiti measur", "label imag", "class-condit distribut"]}
{"doc": "On the Beth properties of some intuitionistic modal logics Let L be one of the intuitionistic modal logics. As in the classical modal case, we define two different forms of the Beth property for L, which are denoted by B1 and B2; in this paper we study the relation among B1, B2 and the interpolation properties C1 and C2. It turns out that C1 implies B1, but contrary to the boolean case, is not equivalent to B1. It is shown that B2 and C2 are independent, and moreover it comes out that, in contrast to classical case, there exists an extension of the intuitionistic modal logic of S/sub 4/-type, that has not the property B2. Finally we give two algebraic properties, that characterize respectively B1 and B2", "label": ["beth properties", "interpolation properties", "intuitionistic modal logics"], "stemmed_label": ["beth properti", "interpol properti", "intuitionist modal logic"]}
{"doc": "Information architecture in JASIST: just where did we come from? The emergence of Information Architecture within the information systems world has been simultaneously drawn out yet rapid. Those with an eye on history are quick to point to Wurman's 1976 use of the term \"architecture of information,\" but it has only been in the last 2 years that IA has become the source of sufficient interest for people to label themselves professionally as Information Architects. The impetus for this recent emergence of IA can be traced to a historical summit, supported by ASIS&T in May 2000 at Boston. It was here that several hundred of us gathered to thrash out the questions of just what IA was and what this new field might become. At the time of the summit, invited to present a short talk on my return journey from the annual ACM SIGCHI conference, I entered the summit expecting little and convinced that IA was nothing new. I left 2 days later refreshed, not just by the enthusiasm of the attendees for this term but by IA's potential to unify the disparate perspectives and orientations of professionals from a range of disciplines. It was at this summit that the idea for the special issue took root. I proposed the idea to Don Kraft, hoping he would find someone else to run with it. AS luck would have it, I ended up taking charge of it myself, with initial support from David Blair. From the suggestion to the finished product-has been the best part of 2 years, and in that time more than 50 volunteers reviewed over 20 submissions", "label": ["information architecture", "information systems", "metadata fields", "controlled vocabularies", "web sites", "cd-rom", "qualified information architect"], "stemmed_label": ["inform architectur", "inform system", "metadata field", "control vocabulari", "web site", "cd-rom", "qualifi inform architect"]}
{"doc": "Gifts to a science academic librarian Gifts, by their altruistic nature, perfectly fit into the environment of universities and academic libraries. As a university's community and general public continue to donate materials, libraries accept donations willingly, both in-kind and monetary. Eight steps of gift processing are listed in the paper. Positive and negative aspects of gift acceptance are discussed. Gifts bring value for academic libraries. Gifts can be considered additional routes to contribute to library collections without direct purchases, options to add money to the library budget, and the cement of social relationships. But, unfortunately, large donations are time-consuming, labor-intensive and costly to process. Great amounts of staff time and processing space are two main negative aspects that cause concern and put the value of gift acceptance under consideration by librarians. Some strategies in handling gifts are recommended. To be effective, academic science librarians need to approach gifts as an investment. Librarians are not to be forced by moral and public notions and should be able to make professional decisions in evaluating proposed collections", "label": ["science academic librarian", "academic libraries", "donations", "gift processing", "library collections", "budget", "staff time", "professional decisions", "research libraries", "acquisitions", "gift books"], "stemmed_label": ["scienc academ librarian", "academ librari", "donat", "gift process", "librari collect", "budget", "staff time", "profession decis", "research librari", "acquisit", "gift book"]}
{"doc": "Reconfigurable context-sensitive middleware for pervasive computing Context-sensitive applications need data from sensors, devices, and user actions, and might need ad hoc communication support to dynamically discover new devices and engage in spontaneous information exchange. Reconfigurable Context-Sensitive Middleware facilitates the development and runtime operations of context-sensitive pervasive computing software", "label": ["pervasive computing", "reconfigurable context-sensitive middleware", "context-sensitive pervasive computing", "middleware", "context-sensitive applications"], "stemmed_label": ["pervas comput", "reconfigur context-sensit middlewar", "context-sensit pervas comput", "middlewar", "context-sensit applic"]}
{"doc": "Who Wants To Be A Millionaire(R): The classroom edition This paper introduces a version of the internationally popular television game show Who Wants To Be A Millionaire(R) that has been created for use in the classroom using Microsoft PowerPoint(R). A suggested framework for its classroom use is presented, instructions on operating and editing the classroom version of Who Wants To Be A Millionaire(R) are provided, and sample feedback from students who have played the classroom version of Who Wants To Be A Millionaire(R) is offered", "label": ["classroom", "who wants to be a millionaire(r)", "classroom version", "undergraduate business students", "student contestants"], "stemmed_label": ["classroom", "who want to be a millionaire(r)", "classroom version", "undergradu busi student", "student contest"]}
{"doc": "Allan variance and fractal Brownian motion Noise filtering is the subject of a voluminous literature in radio engineering. The methods of filtering require knowledge of the frequency response, which is usually unknown. D.W. Allan (see Proc. IEEE, vol.54, no.2, p.221-30, 1966; IEEE Trans. Instr. Measur., vol.IM-36, p.646-54, 1987) proposed a simple method of determining the interval between equally accurate observations which does without this information. In this method, the variances of the increments of noise and signal are equal, so that, in observations with a greater step, the variations caused by noise are smaller than those caused by the signal. This method is the standard accepted by the USA metrology community. The present paper is devoted to a statistical analysis of the Allan method and acquisition of additional information", "label": ["allan variance", "fractal brownian motion", "noise filtering", "radio engineering", "frequency response", "usa metrology community", "statistical analysis", "white noise"], "stemmed_label": ["allan varianc", "fractal brownian motion", "nois filter", "radio engin", "frequenc respons", "usa metrolog commun", "statist analysi", "white nois"]}
{"doc": "Information interaction: providing a framework for information architecture Information interaction is the process that people use in interacting with the content of an information system. Information architecture is a blueprint and navigational aid to the content of information-rich systems. As such information architecture performs an important supporting role in information interactivity. This article elaborates on a model of information interactivity that crosses the \"no-man's land\" between user and computer articulating a model that includes user, content and system, illustrating the context for information architecture", "label": ["information interaction", "navigational aid", "information-rich systems", "information interactivity"], "stemmed_label": ["inform interact", "navig aid", "information-rich system", "inform interact"]}
{"doc": "A simple graphic approach for observer decomposition Based upon the proposition that the roles of inputs and outputs in a physical system and those in the corresponding output-injection observer do not really have to be consistent, a systematic procedure is developed in this work to properly divide a set of sparse system models and measurement models into a number of independent subsets with the help of a visual aid. Several smaller sub-observers can then be constructed accordingly to replace the original one. The size of each sub-observer may be further reduced by strategically selecting one or more appended states. These techniques are shown to be quite effective in relieving on-line computation load of the output-injection observers and also in identifying detectable sub-systems", "label": ["graphic approach", "observer decomposition", "output-injection observer", "sparse system models", "measurement models", "independent subsets", "sub-observers", "online computation load", "detectable subsystems"], "stemmed_label": ["graphic approach", "observ decomposit", "output-inject observ", "spars system model", "measur model", "independ subset", "sub-observ", "onlin comput load", "detect subsystem"]}
{"doc": "How should team captains order golfers on the final day of the Ryder Cup matches? I used game theory to examine how team captains should select their slates for the final day of the Ryder Cup matches. Under the assumption that golfers have different abilities and are not influenced by pressure or momentum, I found that drawing names from a hat will do no worse than any other strategy", "label": ["golf", "golfer ordering", "ryder cup final day", "game theory", "slate"], "stemmed_label": ["golf", "golfer order", "ryder cup final day", "game theori", "slate"]}
{"doc": "Discrete output feedback sliding mode control of second order systems - a moving switching line approach The sliding mode control systems (SMCS) for which the switching variable is designed independent of the initial conditions are known to be sensitive to parameter variations and extraneous disturbances during the reaching phase. For second order systems this drawback is eliminated by using the moving switching line technique where the switching line is initially designed to pass the initial conditions and is subsequently moved towards a predetermined switching line. In this paper, we make use of the above idea of moving switching line together with the reaching law approach to design a discrete output feedback sliding mode control. The main contributions of this work are such that we do not require to use system states as it makes use of only the output samples for designing the controller. and by using the moving switching line a low sensitivity system is obtained through shortening the reaching phase. Simulation results show that the fast output sampling feedback guarantees sliding motion similar to that obtained using state feedback", "label": ["sliding mode control", "switching variable", "parameter variations", "moving switching line", "discrete output feedback", "fast output sampling feedback", "state feedback"], "stemmed_label": ["slide mode control", "switch variabl", "paramet variat", "move switch line", "discret output feedback", "fast output sampl feedback", "state feedback"]}
{"doc": "Regularization of linear regression problems The study considers robust estimation of linear regression parameters by the regularization method, the pseudoinverse method, and the Bayesian method allowing for correlations and errors in the data. Regularizing algorithms are constructed and their relationship with pseudoinversion, the Bayesian approach, and BLUE is investigated", "label": ["linear regression problems regularization", "robust estimation", "linear regression parameters", "pseudoinverse method", "bayesian method", "pseudoinversion", "bayesian approach", "blue"], "stemmed_label": ["linear regress problem regular", "robust estim", "linear regress paramet", "pseudoinvers method", "bayesian method", "pseudoinvers", "bayesian approach", "blue"]}
{"doc": "\"Hidden convexity\" of finite-dimensional stationary linear discrete-time systems under conical constraints New properties of finite-dimensional linear discrete-time systems under conical control constraints that are similar to the \"hidden convexity\" of continuous-time systems are studied", "label": ["hidden convexity", "finite-dimensional stationary linear discrete-time systems", "conical constraints", "control constraint"], "stemmed_label": ["hidden convex", "finite-dimension stationari linear discrete-tim system", "conic constraint", "control constraint"]}
{"doc": "Antipersistent Markov behavior in foreign exchange markets A quantitative check of efficiency in US dollar/Deutsche mark exchange rates is developed using high-frequency (tick by tick) data. The antipersistent Markov behavior of log-price fluctuations of given size implies, in principle, the possibility of a statistical forecast. We introduce and measure the available information of the quote sequence, and we show how it can be profitable following a particular trading rule", "label": ["antipersistent markov behavior", "foreign exchange markets", "efficiency", "us dollar", "deutsche mark", "exchange rates", "high-frequency data", "log-price fluctuations", "statistical forecast", "quote sequence", "trading rule", "shannon entropy", "forecasting"], "stemmed_label": ["antipersist markov behavior", "foreign exchang market", "effici", "us dollar", "deutsch mark", "exchang rate", "high-frequ data", "log-pric fluctuat", "statist forecast", "quot sequenc", "trade rule", "shannon entropi", "forecast"]}
{"doc": "Knowledge management-capturing the skills of key performers in the power industry The growing pressure to reduce the cost of electrical power in recent years has resulted in an enormous \"brain-drain\" within the power industry. A novel approach has been developed by Eskom to capture these skills before they are lost and to incorporate these into a computer-based programme called \"knowledge management\"", "label": ["power industry", "key performers", "knowledge management", "skills capture", "brain-drain", "eskom", "computer-based programme", "south africa", "personnel management"], "stemmed_label": ["power industri", "key perform", "knowledg manag", "skill captur", "brain-drain", "eskom", "computer-bas programm", "south africa", "personnel manag"]}
{"doc": "Hours of operation and service in academic libraries: toward a national standard In an effort toward establishing a standard for academic library hours, the article surveys and compares hours of operation and service for ARL libraries and IPEDS survey respondents. The article ranks the ARL (Association for Research Libraries) libraries according to hours of operation and reference hours and then briefly discusses such issues as libraries offering twenty-four access and factors affecting service hour decisions", "label": ["academic library hours", "operation/service hours", "arl libraries", "ipeds survey respondents", "integrated post secondary education data system", "association for research libraries"], "stemmed_label": ["academ librari hour", "operation/servic hour", "arl librari", "ipe survey respond", "integr post secondari educ data system", "associ for research librari"]}
{"doc": "A new graphical user interface for fast construction of computation phantoms and MCNP calculations: application to calibration of in vivo measurement systems Reports on a new utility for development of computational phantoms for Monte Carlo calculations and data analysis for in vivo measurements of radionuclides deposited in tissues. The individual properties of each worker can be acquired for a rather precise geometric representation of his (her) anatomy, which is particularly important for low energy gamma ray emitting sources such as thorium, uranium, plutonium and other actinides. The software enables automatic creation of an MCNP input data file based on scanning data. The utility includes segmentation of images obtained with either computed tomography or magnetic resonance imaging by distinguishing tissues according to their signal (brightness) and specification of the source and detector. In addition, a coupling of individual voxels within the tissue is used to reduce the memory demand and to increase the calculational speed. The utility was tested for low energy emitters in plastic and biological tissues as well as for computed tomography and magnetic resonance imaging scanning information", "label": ["computational phantoms", "monte carlo calculations", "in vivo measurements", "radionuclides", "tissues", "worker", "precise geometric representation", "mcnp input data file", "scanning data", "computed tomography", "brightness", "graphical user interface", "computation phantoms", "calibration", "in vivo measurement systems", "th", "u", "pu", "signal", "detector", "individual voxels", "memory demand", "calculational speed", "plastic", "biological tissues", "magnetic resonance imaging scanning information", "anatomy", "low energy gamma ray emitting sources", "actinides", "software", "automatic creation"], "stemmed_label": ["comput phantom", "mont carlo calcul", "in vivo measur", "radionuclid", "tissu", "worker", "precis geometr represent", "mcnp input data file", "scan data", "comput tomographi", "bright", "graphic user interfac", "comput phantom", "calibr", "in vivo measur system", "th", "u", "pu", "signal", "detector", "individu voxel", "memori demand", "calcul speed", "plastic", "biolog tissu", "magnet reson imag scan inform", "anatomi", "low energi gamma ray emit sourc", "actinid", "softwar", "automat creation"]}
{"doc": "The plot thins: thin-client computer systems and academic libraries The few libraries that have tried thin client architectures have noted a number of compelling reasons to do so. For starters, thin client devices are far less expensive than most PCs. More importantly, thin client computing devices are believed to be far less expensive to manage and support than traditional PCs", "label": ["academic libraries", "thin-client computer systems"], "stemmed_label": ["academ librari", "thin-client comput system"]}
{"doc": "Mount Sinai Hospital uses integer programming to allocate operating room time An integer-programming model and a post-solution heuristic allocates operating room time to the five surgical divisions at Toronto's Mount Sinai Hospital. The hospital has used this approach for several years and credits it with both administrative savings and the ability to produce quickly an equitable master surgical schedule", "label": ["mount sinai hospital", "integer programming", "operating room time allocation", "toronto", "ontario", "canada", "post-solution heuristic"], "stemmed_label": ["mount sinai hospit", "integ program", "oper room time alloc", "toronto", "ontario", "canada", "post-solut heurist"]}
{"doc": "A new approach to the decomposition of Boolean functions by the method of q-partitions.II. Repeated decomposition For pt.I. see Upr. Sist. Mash., no. 6, p. 29-42 (1999). A new approach to the decomposition of Boolean,functions that depend on n variables and are represented in various forms is considered. The approach is based on the method of q-partitioning of minterms and on the introduced concept of a decomposition clone. The theorem on simple disjunctive decomposition of full and partial functions is formulated. The approach proposed is illustrated by examples", "label": ["boolean functions decomposition", "minterms", "decomposition clone", "disjunctive decomposition", "partial functions", "logic synthesis", "q-partitions"], "stemmed_label": ["boolean function decomposit", "minterm", "decomposit clone", "disjunct decomposit", "partial function", "logic synthesi", "q-partit"]}
{"doc": "Prediction of ultraviolet spectral absorbance using quantitative structure-property relationships High performance liquid chromatography (HPLC) with ultraviolet (UV) spectrophotometric detection is a common method for analyzing reaction products in organic chemistry. This procedure would benefit from a computational model for predicting the relative response of organic molecules. Models are now reported for the prediction of the integrated UV absorbance for a diverse set of organic compounds using a quantitative structure-property relationship (QSPR) approach. A seven-descriptor linear correlation with a squared correlation coefficient (R/sup 2/) of 0.815 is reported for a data set of 521.compounds. Using the sum of ZINDO oscillator strengths in the integration range as an additional descriptor allowed reduction in the number of descriptors producing a robust model for 460 compounds with five descriptors and a squared correlation coefficient 0.857. The descriptors used in the models are discussed with respect to the physical nature of the UV absorption process", "label": ["ultraviolet spectral absorbance prediction", "quantitative structure-property relationship", "high performance liquid chromatography", "ultraviolet spectrophotometric detection", "reaction products", "organic chemistry", "computational model", "relative response", "seven-descriptor linear correlation", "squared correlation coefficient", "zindo oscillator strengths", "combinatorial chemistry", "generic quantitation", "configuration interaction calculation", "codessa program", "mos-f package"], "stemmed_label": ["ultraviolet spectral absorb predict", "quantit structure-properti relationship", "high perform liquid chromatographi", "ultraviolet spectrophotometr detect", "reaction product", "organ chemistri", "comput model", "rel respons", "seven-descriptor linear correl", "squar correl coeffici", "zindo oscil strength", "combinatori chemistri", "gener quantit", "configur interact calcul", "codessa program", "mos-f packag"]}
{"doc": "New lower bounds of the size of error-correcting codes for the Z-channel Optimization problems on graphs are formulated to obtain new lower bounds of the size of error-correcting codes for the Z-channel", "label": ["lower bounds", "error-correcting codes", "z-channel", "optimization problems", "graphs"], "stemmed_label": ["lower bound", "error-correct code", "z-channel", "optim problem", "graph"]}
{"doc": "Press shop. Industrial IT solutions for the press shop Globalization of the world's markets is challenging the traditional limits of manufacturing efficiency. The competitive advantage belongs to those who understand the new requirements and opportunities, and who commit to integrated solutions that span the value chain all the way from demand to production. ABB's automation and IT expertise and the process know-how gained from its long involvement with the automotive industry, have been brought together in new, state-of-the-art software solutions for press shops. Integrated into Industrial IT architecture, they allow the full potential of the shops to be realized, with advantages at every step in the supply chain", "label": ["press shops", "industrial it solutions", "market globalisation", "manufacturing efficiency", "automation", "state-of-the-art", "software solutions", "car manufacturing business", "supply chain"], "stemmed_label": ["press shop", "industri it solut", "market globalis", "manufactur effici", "autom", "state-of-the-art", "softwar solut", "car manufactur busi", "suppli chain"]}
{"doc": "Ideal sliding mode in the problems of convex optimization The characteristics of the sliding mode that appears with using continuous convex-programming algorithms based on the exact penalty functions were discussed. For the case under study, the ideal sliding mode was shown to occur in the absence of infinite number of switchings", "label": ["ideal sliding mode", "convex optimization", "continuous convex-programming algorithms", "exact penalty functions"], "stemmed_label": ["ideal slide mode", "convex optim", "continu convex-program algorithm", "exact penalti function"]}
{"doc": "Computational finite-element schemes for optimal control of an elliptic system with conjugation conditions New optimal control problems are considered for distributed systems described by elliptic equations with conjugate conditions and a quadratic minimized function. Highly accurate computational discretization schemes are constructed for the case where a feasible control set u/sub delta / coincides with the full Hilbert space u of controls", "label": ["optimal control problems", "distributed systems", "elliptic equations", "conjugate conditions", "quadratic minimized function", "computational discretization schemes"], "stemmed_label": ["optim control problem", "distribut system", "ellipt equat", "conjug condit", "quadrat minim function", "comput discret scheme"]}
{"doc": "The social impact of Internet gambling Technology has always played a role in the development of gambling practices and continues to provide new market opportunities. One of the fastest growing areas is that of Internet gambling. The effect of such technologies should not be accepted uncritically, particularly as there may be areas of potential concern based on what is known about problem gambling offline. This article has three aims. First, it overviews some of the main social concerns about the rise of Internet gambling. Second, it looks at the limited research that has been carried out in this area. Third, it examines whether Internet gambling is doubly addictive, given research that suggests that the Internet can be addictive itself. It is concluded that technological developments in Internet gambling will increase the potential for problem gambling globally, but that many of the ideas and speculations outlined in this article need to be addressed further by large-scale empirical studies", "label": ["social impact", "internet gambling", "market opportunities", "technological developments", "addiction", "electronic cash", "psychology"], "stemmed_label": ["social impact", "internet gambl", "market opportun", "technolog develop", "addict", "electron cash", "psycholog"]}
{"doc": "Feedforward maximum power point tracking of PV systems using fuzzy controller A feedforward maximum power (MP) point tracking scheme is developed for the interleaved dual boost (IDB) converter fed photovoltaic (PV) system using fuzzy controller. The tracking algorithm changes the duty ratio of the converter such that the solar cell array (SCA) voltage equals the voltage corresponding to the MP point at that solar insolation. This is done by the feedforward loop, which generates an error signal by comparing the instantaneous array voltage and reference voltage. The reference voltage for the feedforward loop, corresponding to the MP point, is obtained by an off-line trained neural network. Experimental data is used for off-line training of the neural network, which employs back-propagation algorithm. The proposed fuzzy feedforward peak power tracking effectiveness is demonstrated through the simulation and experimental results, and compared with the conventional proportional plus integral (PI) controller based system. Finally, a comparative study of interleaved boost and conventional boost converter for the PV applications is given and their suitability is discussed", "label": ["feedforward maximum power point tracking", "pv systems", "fuzzy controller", "interleaved dual boost converter feed", "photovoltaic system", "tracking algorithm", "duty ratio", "solar cell array voltage", "solar insolation", "feedforward loop", "error signal", "instantaneous array voltage", "reference voltage", "off-line trained neural network", "back-propagation algorithm", "fuzzy feedforward peak power tracking effectiveness"], "stemmed_label": ["feedforward maximum power point track", "pv system", "fuzzi control", "interleav dual boost convert feed", "photovolta system", "track algorithm", "duti ratio", "solar cell array voltag", "solar insol", "feedforward loop", "error signal", "instantan array voltag", "refer voltag", "off-lin train neural network", "back-propag algorithm", "fuzzi feedforward peak power track effect"]}
{"doc": "The Bagsik Oscillator without complex numbers We argue that the analysis of the so-called Bagsik Oscillator, recently published by Piotrowski and Sladkowski (2001), is erroneous due to: (1) the incorrect banking data used and (2) the application of statistical mechanism apparatus to processes that are totally deterministic", "label": ["bagsik oscillator", "noncomplex numbers", "incorrect banking data", "statistical mechanism apparatus", "game theory", "deterministic processes"], "stemmed_label": ["bagsik oscil", "noncomplex number", "incorrect bank data", "statist mechan apparatu", "game theori", "determinist process"]}
{"doc": "Matched-filter template generation via spatial filtering: application to fetal biomagnetic recordings We have developed a two-step procedure for signal processing of fetal biomagnetic recordings that removes cardiac interference and noise. First, a modified matched filter (MF) is applied to remove maternal cardiac interference; then, a simple signal space projection (SSP) is applied to remove noise. The key difference between our MF and a conventional one is that the interference template and the template scaling are derived from a signal that has been spatially filtered to isolate the interference, rather than from the raw signal. Unlike conventional MFs, ours is able to separate maternal and fetal cardiac complexes, even when they have similar morphology and overlap strongly. When followed by a SSP that preserves only the signal subspace, the noise is reduced to a low level", "label": ["maternal cardiac interference removal", "simple signal space projection", "noise removal", "signal subspace preservation", "fetal magnetocardiography", "spatial filtering", "interference template", "raw signal", "template scaling", "modified matched filter", "maternal cardiac interference"], "stemmed_label": ["matern cardiac interfer remov", "simpl signal space project", "nois remov", "signal subspac preserv", "fetal magnetocardiographi", "spatial filter", "interfer templat", "raw signal", "templat scale", "modifi match filter", "matern cardiac interfer"]}
{"doc": "Central hub for design assets: Adobe GoLive 6.0 Adobe GoLive is a strong contender for Web authoring and publishing. Version 6.0 features a flexible GUI environment combined with a comprehensive workgroup and collaboration server, plus tight integration with leading design tools", "label": ["adobe golive 6.0", "flash", "real", "java", "application servers", "web authoring", "gui", "workgroup server", "collaboration server", "livemotion 2.0", "animation and scripting tool", "macromedia swf format", "workgroup environment", "web publishing environment", "design-centric dynamic content"], "stemmed_label": ["adob goliv 6.0", "flash", "real", "java", "applic server", "web author", "gui", "workgroup server", "collabor server", "livemot 2.0", "anim and script tool", "macromedia swf format", "workgroup environ", "web publish environ", "design-centr dynam content"]}
{"doc": "Electrical facility construction work for information network structuring by the use of sewage conduits To confront the advent of the advanced information society, there has been a pressing demand for the adjustment of the communications infrastructure and the structuring of the information network by utilizing the sewage conduits. The City of Tokyo is promoting a project by the name of the sewer optical fiber teleway (SOFT) network plan. According to this plan, the total distance of the optical fiber network laid in the sewer conduits is scheduled to reach about 470 km by the end of March 2000. At the final stage, this distance will reach 800 km as a whole. We completed the construction work for the information control facilities scattered in 11 places inclusive of the Treatment Site S, with the intention to adjust and extend the information transmission network laid through the above-mentioned optical fiber network, to be used exclusively by the Bureau of Sewerage. This construction work is described in the paper", "label": ["electrical facility construction work", "information network structuring", "sewage conduits", "communications infrastructure", "tokyo", "sewer optical fiber teleway network plan", "information control facilities", "treatment site s", "information transmission network", "bureau of sewerage", "asynchronous transmission mode switches", "atm switches"], "stemmed_label": ["electr facil construct work", "inform network structur", "sewag conduit", "commun infrastructur", "tokyo", "sewer optic fiber teleway network plan", "inform control facil", "treatment site s", "inform transmiss network", "bureau of sewerag", "asynchron transmiss mode switch", "atm switch"]}
{"doc": "E-government The author provides an introduction to the main issues surrounding E-government modernisation and electronic delivery of all public services by 2005. The author makes it clear that E-government is about transformation, not computers and hints at the special legal issues which may arise", "label": ["e-government", "modernisation", "electronic delivery", "public services", "legal issues"], "stemmed_label": ["e-govern", "modernis", "electron deliveri", "public servic", "legal issu"]}
{"doc": "Firewall card shields data The SlotShield 3000 firewall on a PCI card saves power and space, but might not offer enough security for large networks", "label": ["slotshield 3000 firewall", "pci card", "security", "large networks"], "stemmed_label": ["slotshield 3000 firewal", "pci card", "secur", "larg network"]}
{"doc": "Prospecting virtual collections Virtual collections are a distinct sub-species of digital collections and digital archives. Archivists and curators as archivists and curators do not construct virtual collections; rather they enable virtual collections through the application of descriptive and other standards. Virtual collections are constructed by end users", "label": ["virtual collections", "digital collections", "digital archives", "archivists", "curators", "descriptive standards", "end users", "digitization"], "stemmed_label": ["virtual collect", "digit collect", "digit archiv", "archivist", "curat", "descript standard", "end user", "digit"]}
{"doc": "I-WAP: an intelligent WAP site management system The popularity regarding wireless communications is such that more and more WAP sites have been developed with wireless markup language (WML). Meanwhile, to translate hypertext markup language (HTML) pages into proper WML ones becomes imperative since it is difficult for WAP users to read most contents designed for PC users via their mobile phone screens. However, for those sites that have been maintained with hypertext markup language (HTML), considerable time and manpower costs will be incurred to rebuild them with WML. In this paper, we propose an intelligent WAP site management system to cope with these problems. With the help of the intelligent management system, the original contents of HTML Web sites can be automatically translated to proper WAP content in an efficient way. As a consequence, the costs associated with maintaining WAP sites could be significantly reduced. The management system also allows the system manager to define the relevance of numerals and keywords for removing unimportant or meaningless contents. The original contents will be reduced and reorganized to fit the size of mobile phone screens, thus reducing the communication cost and enhancing readability. Numerical results gained through various experiments have evinced the effective performance of the WAP management system", "label": ["intelligent wap site management system", "i-wap", "wireless communication", "wireless markup language", "hypertext markup language", "html pages", "mobile phone", "communication cost", "readability", "wireless mobile internet"], "stemmed_label": ["intellig wap site manag system", "i-wap", "wireless commun", "wireless markup languag", "hypertext markup languag", "html page", "mobil phone", "commun cost", "readabl", "wireless mobil internet"]}
{"doc": "A self-adjusting quality of service control scheme We propose and analyze a self-adjusting Quality of Service (QoS) control scheme with the goal of optimizing the system reward as a result of servicing different priority clients with varying workload, QoS and reward/penalty requirements. Our scheme is based on resource partitioning and designated \"degrade QoS areas\" such that system resources are partitioned into priority areas each of which is reserved specifically to serve only clients in a corresponding class with no QoS degradation, plus one \"degraded QoS area\" into which all clients can be admitted with QoS adjustment being applied only to the lowest priority clients. We show that the best partition is dictated by the workload and the reward/penalty characteristics of clients in difference priority classes. The analysis results can be used by a QoS manager to optimize the system total reward dynamically in response to changing workloads at run time. We demonstrate the validity of our scheme by means of simulation and comparing the proposed QoS self-adjusting scheme with those that do not use resource partitioning or designated degraded QoS areas", "label": ["self-adjusting quality of service control scheme", "priority clients", "resource partitioning", "simulation", "multimedia systems", "performance evaluation", "resource reservation"], "stemmed_label": ["self-adjust qualiti of servic control scheme", "prioriti client", "resourc partit", "simul", "multimedia system", "perform evalu", "resourc reserv"]}
{"doc": "Pervasive computing goes to work: interfacing to the enterprise The paperless office is an idea whose time has come, and come, and come again. To see how pervasive computing applications might bring some substance to this dream, the author spoke recently with key managers and technologists at McKesson Corporation (San Francisco), a healthcare supplier, service, and technology company with US$50 billion in sales last year, and also at AvantGo (Hayward, Calif.), a provider of mobile infrastructure software and services. For the past several years, McKesson has used mobility middleware developed by AvantGo to deploy major supply chain applications with thousands of pervasive clients and multiple servers that replace existing paper-based tracking systems. According to McKesson's managers, their system greatly reduced errors and associated costs caused by redelivery or loss of valuable products, giving McKesson a solid return on its investment", "label": ["paperless office", "pervasive clients", "multiple servers", "mobile workers", "enterprise resource planning", "data warehousing"], "stemmed_label": ["paperless offic", "pervas client", "multipl server", "mobil worker", "enterpris resourc plan", "data wareh"]}
{"doc": "Linear models of circuits based on the multivalued components Linearization and planarization of the circuit models is pivotal to the submicron technologies. On the other hand, the characteristics of the VLSI circuits can be sometimes improved by using the multivalued components. It was shown that any l-level circuit based on the multivalued components is representable as an algebraic model based on l linear arithmetic polynomials mapped correspondingly into l decision diagrams that are linear and planar by nature. Complexity of representing a circuit as the linear decision diagram was estimated as O(G) with G for the number of multivalued components in the circuit. The results of testing the LinearDesignMV algorithm on circuits of more than 8000 LGSynth 93 multivalued components were presented", "label": ["linear circuit model", "linearization", "planarization", "submicron technologies", "vlsi circuits", "linear arithmetic polynomials", "linear planar decision diagrams", "circuit representation complexity", "lineardesignmv algorithm", "lgsynth 93 multivalued components"], "stemmed_label": ["linear circuit model", "linear", "planar", "submicron technolog", "vlsi circuit", "linear arithmet polynomi", "linear planar decis diagram", "circuit represent complex", "lineardesignmv algorithm", "lgsynth 93 multivalu compon"]}
{"doc": "The use of the SPSA method in ECG analysis The classification, monitoring, and compression of electrocardiogram (ECG) signals recorded of a single patient over a relatively long period of time is considered. The particular application we have in mind is high-resolution ECG analysis, such as late potential analysis, morphology changes in QRS during arrythmias, T-wave alternants, or the study of drug effects on ventricular activation. We propose to apply a modification of a classical method of cluster analysis or vector quantization. The novelty of our approach is that we use a new distortion measure to quantify the distance of two ECG cycles, and the class-distortion measure is defined using a min-max criterion. The new class-distortion-measure is much more sensitive to outliers than the usual distortion measures using average-distance. The price of this practical advantage is that computational complexity is significantly increased. The resulting nonsmooth optimization problem is solved by an adapted version of the simultaneous perturbation stochastic approximation (SPSA) method of J. Spall (IEEE Trans. Automat. Contr., vol. 37, p. 332-41, Mar. 1992). The main idea is to generate a smooth approximation by a randomization procedure. The viability of the method is demonstrated on both simulated and real data. An experimental comparison with the widely used correlation method is given on real data", "label": ["class-distortion-measure", "nonsmooth optimization problem", "simultaneous perturbation stochastic approximation method", "cluster analysis", "randomization procedure", "correlation method", "electrodiagnostics", "ecg signals compression", "distortion measure", "ecg cycles"], "stemmed_label": ["class-distortion-measur", "nonsmooth optim problem", "simultan perturb stochast approxim method", "cluster analysi", "random procedur", "correl method", "electrodiagnost", "ecg signal compress", "distort measur", "ecg cycl"]}
{"doc": "Generating code at run time with Reflection.Emit The .NET framework SDK includes several tools that convert source code into executable code-the C# and VB.NET compilers get most of the attention, but there are others. The Regex class (in the System.Text.RegularExpressions namespace) has the ability to compile favorite regular expressions into a .NET assembly. In fact, the NET Common Language Runtime (CLR) contains a whole namespace full of classes to help us build assemblies, define types, and emit their implementations, all at run time. These classes, which comprise the System.Reflection.Emit namespace, are known collectively as Reflection. Emit", "label": [".net framework sdk", "runtime code generation", "regex class", ".net common language runtime", "assemblies", "types", "system.reflection.emit namespace", "reflection.emit"], "stemmed_label": [".net framework sdk", "runtim code gener", "regex class", ".net common languag runtim", "assembl", "type", "system.reflection.emit namespac", "reflection.emit"]}
{"doc": "Capturing niche markets with copper For \"last-mile access\" in niche applications, twisted copper pair may be the cable of best option to gain access and deliver desired services. The article discusses how operators can use network edge devices to serve new customers. Niche market segments represent a significant opportunity for cable TV delivery of television and high-speed Internet signals. But the existing telecommunications infrastructure in those developments frequently presents unique challenges for the service provider to overcome", "label": ["last-mile access", "twisted copper pair", "network edge devices", "copper cables", "niche markets"], "stemmed_label": ["last-mil access", "twist copper pair", "network edg devic", "copper cabl", "nich market"]}
{"doc": "A nonlinear time-optimal control problem Sufficient conditions for the existence of an optimal control in a time-optimal control problem with fixed ends for a smooth nonlinear control system are formulated. The properties of this system for characterizing the optimal control switching points are studied", "label": ["nonlinear time-optimal control problem", "sufficient existence conditions", "smooth nonlinear control system", "optimal control switching points"], "stemmed_label": ["nonlinear time-optim control problem", "suffici exist condit", "smooth nonlinear control system", "optim control switch point"]}
{"doc": "Broadcasts keep staff in picture intranets Mark Hawkins, chief operating officer at UK-based streaming media specialist Twofourtv, explains how firms can benefit by linking their corporate intranets to broadcasting technology", "label": ["corporate intranets", "twofourtv", "streaming media", "broadcasting technology"], "stemmed_label": ["corpor intranet", "twofourtv", "stream media", "broadcast technolog"]}
{"doc": "Modelling user acceptance of building management systems This study examines user acceptance of building management systems (BMS) using a questionnaire survey. These systems are crucial for optimising building performance and yet it has been widely reported that users are not making full use of their systems' facilities. Established models of technology acceptance have been employed in this research, and the positive influence of user perceptions of ease of use and compatibility has been demonstrated. Previous research has indicated differing levels of importance of perceived ease of use relative to other factors. Here, perceived ease of use is shown generally to be more important, though the balance between this and compatibility is moderated by the user perceptions of voluntariness", "label": ["user acceptance modelling", "building management systems", "technology acceptance model", "innovation characteristics", "information systems", "questionnaire survey", "user perceptions", "ease of use", "compatibility", "voluntariness"], "stemmed_label": ["user accept model", "build manag system", "technolog accept model", "innov characterist", "inform system", "questionnair survey", "user percept", "eas of use", "compat", "voluntari"]}
{"doc": "The maximum possible EVPI In this paper we calculate the maximum expected value of perfect information (EVPI) for any probability distribution for the states of the world. This maximum EVPI is an upper bound for the EVPI with given probabilities and thus an upper bound for any partial information about the states of the world", "label": ["decision analysis", "expected value of perfect information", "operations research", "management science", "probability distribution", "optimisation"], "stemmed_label": ["decis analysi", "expect valu of perfect inform", "oper research", "manag scienc", "probabl distribut", "optimis"]}
{"doc": "Model selection in electromagnetic source analysis with an application to VEFs In electromagnetic source analysis, it is necessary to determine how many sources are required to describe the electroencephalogram or magnetoencephalogram adequately. Model selection procedures (MSPs) or goodness of fit procedures give an estimate of the required number of sources. Existing and new MSPs are evaluated in different source and noise settings: two sources which are close or distant and noise which is uncorrelated or correlated. The commonly used MSP residual variance is seen to be ineffective, that is it often selects too many sources. Alternatives like the adjusted Hotelling's test, Bayes information criterion and the Wald test on source amplitudes are seen to be effective. The adjusted Hotelling's test is recommended if a conservative approach is taken and MSPs such as Bayes information criterion or the Wald test on source amplitudes are recommended if a more liberal approach is desirable. The MSPs are applied to empirical data (visual evoked fields)", "label": ["model selection", "electromagnetic source analysis", "noise settings", "residual variance", "wald test", "adjusted hotelling's test", "empirical data", "vefs", "meg source analysis", "eeg source analysis", "goodness-of-fit", "source localization", "visual evoked fields"], "stemmed_label": ["model select", "electromagnet sourc analysi", "nois set", "residu varianc", "wald test", "adjust hotelling' test", "empir data", "vef", "meg sourc analysi", "eeg sourc analysi", "goodness-of-fit", "sourc local", "visual evok field"]}
{"doc": "Fuzzy polynomial neural networks: hybrid architectures of fuzzy modeling We introduce a concept of fuzzy polynomial neural networks (FPNNs), a hybrid modeling architecture combining polynomial neural networks (PNNs) and fuzzy neural networks (FNNs). The development of the FPNNs dwells on the technologies of computational intelligence (CI), namely fuzzy sets, neural networks, and genetic algorithms. The structure of the FPNN results from a synergistic usage of FNN and PNN. FNNs contribute to the formation of the premise part of the rule-based structure of the FPNN. The consequence part of the FPNN is designed using PNNs. The structure of the PNN is not fixed in advance as it usually takes place in the case of conventional neural networks, but becomes organized dynamically to meet the required approximation error. We exploit a group method of data handling (GMDH) to produce this dynamic topology of the network. The performance of the FPNN is quantified through experimentation that exploits standard data already used in fuzzy modeling. The obtained experimental results reveal that the proposed networks exhibit high accuracy and generalization capabilities in comparison to other similar fuzzy models", "label": ["fuzzy polynomial neural networks", "hybrid architectures", "fuzzy modeling", "highly nonlinear rule-based models", "computational intelligence", "fuzzy sets", "genetic algorithms", "group method of data handling", "gmdh", "dynamic topology", "fuzzy inference method", "learning", "standard backpropagation", "membership functions", "learning rates", "momentum coefficients", "genetic optimization"], "stemmed_label": ["fuzzi polynomi neural network", "hybrid architectur", "fuzzi model", "highli nonlinear rule-bas model", "comput intellig", "fuzzi set", "genet algorithm", "group method of data handl", "gmdh", "dynam topolog", "fuzzi infer method", "learn", "standard backpropag", "membership function", "learn rate", "momentum coeffici", "genet optim"]}
{"doc": "Industrial/sup IT/ for performance buildings ABB has taken a close look at how buildings are used and has come up with a radical solution for the technical infrastructure that places the end-user's processes at the center and integrates all the building's systems around their needs. The new solution is based on the realization that tasks like setting up an office meeting, registering a hotel guest or moving a patient in a hospital, can all benefit from the same Industrial IT concepts employed by ABB to optimize manufacturing, for example in the automotive industry", "label": ["industrial/sup it/", "abb", "building management system", "technical infrastructure", "building systems integration", "industrial it concepts"], "stemmed_label": ["industrial/sup it/", "abb", "build manag system", "technic infrastructur", "build system integr", "industri it concept"]}
{"doc": "The congenial talking philosophers problem in computer networks Group mutual exclusion occurs naturally in situations where a resource can be shared by processes of the same group, but not by processes of different groups. For example, suppose data is stored in a CD-jukebox. Then, when a disc is loaded for access, users that need data on the disc can concurrently access the disc, while users that need data on a different disc have to wait until the current disc is unloaded. The design issues for group mutual exclusion have been modeled as the Congenial Talking Philosophers problem, and solutions for shared memory models have been proposed (Y.-J. Young, 2000; P. Keane and M. Moir, 1999). As in ordinary mutual exclusion and many other problems in distributed systems, however, techniques developed for shared memory do not necessarily apply to message passing (and vice versa). We investigate solutions for Congenial Talking Philosophers in computer networks where processes communicate by asynchronous message passing. We first present a solution that is a straightforward adaptation from G. Ricart and A.K. Agrawala's (1981) algorithm for ordinary mutual exclusion. Then we show that the simple modification suffers a severe performance degradation that could cause the system to behave as though only one process of a group can be in the critical section at a time. We then present a more efficient and highly concurrent distributed algorithm for the problem, the first such solution in computer networks", "label": ["congenial talking philosophers problem", "computer networks", "group mutual exclusion", "resource sharing", "shared-memory models", "distributed systems", "process communication", "asynchronous message passing", "critical section", "concurrent distributed algorithm"], "stemmed_label": ["congeni talk philosoph problem", "comput network", "group mutual exclus", "resourc share", "shared-memori model", "distribut system", "process commun", "asynchron messag pass", "critic section", "concurr distribut algorithm"]}
{"doc": "Solution of the safe problem on (0,1)-matrices A safe problem with mn locks is studied. It is reduced to a system of linear equations in the modulo 2 residue class. There are three possible variants defined by the numbers m and n evenness, with only one of them having a solution. In two other cases, correction of the initial state of the safe insuring a solution is proposed", "label": ["safe problem", "mn locks", "linear equations", "modulo 2 residue class", "(0;1)-matrices", "computer games", "linear diophantine equations"], "stemmed_label": ["safe problem", "mn lock", "linear equat", "modulo 2 residu class", "(0;1)-matric", "comput game", "linear diophantin equat"]}
{"doc": "A comparison of computational color constancy Algorithms. II. Experiments with image data For pt.I see ibid., vol. 11, no.9, p.972-84 (2002). We test a number of the leading computational color constancy algorithms using a comprehensive set of images. These were of 33 different scenes under 11 different sources representative of common illumination conditions. The algorithms studied include two gray world methods, a version of the Retinex method, several variants of Forsyth's (1990) gamut-mapping method, Cardei et al.'s (2000) neural net method, and Finlayson et al.'s color by correlation method (Finlayson et al. 1997, 2001; Hubel and Finlayson 2000). We discuss a number of issues in applying color constancy ideas to image data, and study in depth the effect of different preprocessing strategies. We compare the performance of the algorithms on image data with their performance on synthesized data. All data used for this study are available online at http://www.cs.sfu.ca/~color/data, and implementations for most of the algorithms are also available (http://www.cs.sfu.ca/~color/code). Experiments with synthesized data (part one of this paper) suggested that the methods which emphasize the use of the input data statistics, specifically color by correlation and the neural net algorithm, are potentially the most effective at estimating the chromaticity of the scene illuminant. Unfortunately, we were unable to realize comparable performance on real images. Here exploiting pixel intensity proved to be more beneficial than exploiting the details of image chromaticity statistics, and the three-dimensional (3-D) gamut-mapping algorithms gave the best performance", "label": ["computational color constancy algorithms", "images", "image data", "illumination conditions", "gray world methods", "retinex method", "gamut-mapping method", "neural net method", "color by correlation method", "preprocessing strategies", "synthesized data", "input data statistics", "chromaticity", "scene illuminant", "pixel intensity"], "stemmed_label": ["comput color constanc algorithm", "imag", "imag data", "illumin condit", "gray world method", "retinex method", "gamut-map method", "neural net method", "color by correl method", "preprocess strategi", "synthes data", "input data statist", "chromat", "scene illumin", "pixel intens"]}
{"doc": "Reachability sets of a class of multistep control processes: their design An upper estimate and an iterative \"restriction\" algorithm for the reachability set for determining the optimal control for a class of multistep control processes are designed", "label": ["reachability sets", "multistep control processes", "discrete systems", "upper estimate", "iterative restriction algorithm", "optimal control"], "stemmed_label": ["reachabl set", "multistep control process", "discret system", "upper estim", "iter restrict algorithm", "optim control"]}
{"doc": "Preintegration lateral inhibition enhances unsupervised learning A large and influential class of neural network architectures uses postintegration lateral inhibition as a mechanism for competition. We argue that these algorithms are computationally deficient in that they fail to generate, or learn, appropriate perceptual representations under certain circumstances. An alternative neural network architecture is presented here in which nodes compete for the right to receive inputs rather than for the right to generate outputs. This form of competition, implemented through preintegration lateral inhibition, does provide appropriate coding properties and can be used to learn such representations efficiently. Furthermore, this architecture is consistent with both neuroanatomical and neuropsychological data. We thus argue that preintegration lateral inhibition has computational advantages over conventional neural network architectures while remaining equally biologically plausible", "label": ["neural network architectures", "postintegration lateral inhibition", "competition", "preintegration lateral inhibition", "neural network", "unsupervised learning"], "stemmed_label": ["neural network architectur", "postintegr later inhibit", "competit", "preintegr later inhibit", "neural network", "unsupervis learn"]}
{"doc": "Acquisitions in the James Ford Bell Library This article presents basic acquisitions philosophy and approaches in a noted special collection, with commentary on \"just saying no\" and on how the electronic revolution has changed the acquisition of special collections materials", "label": ["james ford bell library", "library acquisitions philosophy", "out-of-print books", "university library", "special collections", "electronic revolution"], "stemmed_label": ["jame ford bell librari", "librari acquisit philosophi", "out-of-print book", "univers librari", "special collect", "electron revolut"]}
{"doc": "A better ballot box? Election officials are examining technologies to address a wide range of voting issues. The problems observed in the November 2000 US election accelerated existing trends to get rid of lever machines, punch-cards, and hand-counted paper ballots and replace them with mark-sense balloting, Internet, and automatic teller machine (ATM) kiosk style computer-based systems. An estimated US $2-$4 billion will be spent in the United States and Canada to update voting systems during the next decade. Voting online might enable citizens to vote even if they are unable to get to the polls. Yet making these methods work right turns out to be considerably more difficult than originally thought. New electronic voting systems pose risks as well as solutions. As it turns out, many of the voting products currently for sale provide less accountability, poorer reliability, and greater opportunity for widespread fraud than those already in use. This paper discusses the technology available and how to ensure accurate ballots", "label": ["ballot box", "mark-sense balloting", "automatic teller machine computer-based voting system", "atm kiosk style computer-based voting systems", "electronic voting", "online voting"], "stemmed_label": ["ballot box", "mark-sens ballot", "automat teller machin computer-bas vote system", "atm kiosk style computer-bas vote system", "electron vote", "onlin vote"]}
{"doc": "Information architecture for the Web: The IA matrix approach to designing children's portals The article presents a matrix that can serve as a tool for designing the information architecture of a Web portal in a logical and systematic manner. The information architect begins by inputting the portal's objective, target user, and target content. The matrix then determines the most appropriate information architecture attributes for the portal by filling in the Applied Information Architecture portion of the matrix. The article discusses how the matrix works using the example of a children's Web portal to provide access to museum information", "label": ["information architecture", "target user", "target content", "children's web portal", "museum information"], "stemmed_label": ["inform architectur", "target user", "target content", "children' web portal", "museum inform"]}
{"doc": "The role of CAUL (Council of Australian Libraries) in consortial purchasing The Council of Australian University Librarians, constituted in 1965 for the purposes of cooperative action and the sharing of information, assumed the role of consortial purchasing agent in 1996 on behalf of its members and associate organisations in Australia and New Zealand. This role continues to grow in tandem with the burgeoning of electronic publication and the acceptance of publishers of the advantages of dealing with consortia. The needs of the Australian university community overlap significantly with consortia in North America and Europe, but important differences are highlighted", "label": ["council of australian university librarians", "cooperative action", "information sharing", "consortial purchasing", "australia", "new zealand", "electronic publication", "north america", "europe"], "stemmed_label": ["council of australian univers librarian", "cooper action", "inform share", "consorti purchas", "australia", "new zealand", "electron public", "north america", "europ"]}
{"doc": "Information architecture without internal theory: an inductive design process This article suggests that Information Architecture (IA) design is primarily an inductive process. Although top-level goals, user attributes and available content are periodically considered, the process involves bottom-up design activities. IA is inductive partly because it lacks internal theory, and partly because it is an activity that supports emergent phenomena (user experiences) from basic design components. The nature of IA design is well described by Constructive Induction (CI), a design process that involves locating the best representational framework for the design problem, identifying a solution within that framework and translating it back to the design problem at hand. The future of IA, if it remains inductive or develops a body of theory (or both), is considered", "label": ["information architecture design", "inductive design process", "bottom-up design activities", "internal theory", "emergent phenomena", "user experiences", "constructive induction"], "stemmed_label": ["inform architectur design", "induct design process", "bottom-up design activ", "intern theori", "emerg phenomena", "user experi", "construct induct"]}
{"doc": "Medicine in the 21 st century: global problems, global solutions The objectives are to discuss application areas of information, technology in medicine and health care on the occasion of the opening of the Private Universitat fur Medizinische Informatik and Technik Tirol/University for Health Informatics and Technology Tyrol (LIMIT) at Innsbruck, Tyrol, Austria. Important application areas of information technology in medicine and health are appropriate individual access to medical knowledge, new engineering developments such as new radiant imaging methods and the implantable pacemaker/defibrillator devices, mathematical modeling for understanding the workings of the human body, the computer-based patient record, as well as new knowledge in molecular biology, human genetics, and biotechnology. Challenges and responsibilities for medical informatics research include medical data privacy and intellectual property rights inherent in the content of the information systems", "label": ["health care", "medicine", "information technology", "individual medical knowledge access", "engineering developments", "radiant imaging methods", "implantable pacemaker devices", "implantable defibrillator devices", "mathematical modeling", "human body", "computer-based patient record", "molecular biology", "human genetics", "biotechnology", "medical informatics research", "medical data privacy", "intellectual property rights", "information systems"], "stemmed_label": ["health care", "medicin", "inform technolog", "individu medic knowledg access", "engin develop", "radiant imag method", "implant pacemak devic", "implant defibril devic", "mathemat model", "human bodi", "computer-bas patient record", "molecular biolog", "human genet", "biotechnolog", "medic informat research", "medic data privaci", "intellectu properti right", "inform system"]}
{"doc": "Development of a health guidance support system for lifestyle improvement The objective is to provide automated advice for lifestyle adjustment based on an assessment of the results of a questionnaire and medical examination or health checkup data. A system was developed that gathers data based on questions regarding weight gain, exercise, smoking, sleep, eating habits, salt intake, animal fat intake, snacks, alcohol, and oral hygiene, body mass index, resting blood pressure, fasting blood sugar, total cholesterol, triglycerides, uric acid and liver function tests. Based on the relationships between the lifestyle data and the health checkup data, a health assessment sheet was generated for persons being allocated to a multiple-risk factor syndrome group. Health assessment and useful advice for lifestyle improvement were automatically extracted with the system, toward the high risk group for life style related diseases. The system is operational. In comparison with conventional, limited advice methods, we developed a practical system that defined the necessity for lifestyle improvement more clearly, and made giving advice easier", "label": ["health guidance support system", "lifestyle improvement", "questionnaire", "medical examination", "health checkup data", "weight gain", "smoking", "exercise", "sleep", "eating habits", "salt intake", "animal fat intake", "snacks", "alcohol", "oral hygiene", "body mass index", "resting blood pressure", "fasting blood sugar", "total cholesterol", "triglycerides", "uric acid", "liver function tests"], "stemmed_label": ["health guidanc support system", "lifestyl improv", "questionnair", "medic examin", "health checkup data", "weight gain", "smoke", "exercis", "sleep", "eat habit", "salt intak", "anim fat intak", "snack", "alcohol", "oral hygien", "bodi mass index", "rest blood pressur", "fast blood sugar", "total cholesterol", "triglycerid", "uric acid", "liver function test"]}
{"doc": "Controller performance analysis with LQG benchmark obtained under closed loop conditions This paper proposes a new method for obtaining a linear quadratic Gaussian (LQG) benchmark in terms of the variances of process input and output from closed-loop data, for assessing the controller performance. LQG benchmark has been proposed in the literature to assess controller performance since the LQG tradeoff curve represents the limit of performance in terms of input and output variances. However, an explicit parametric model is required to calculate the LQG benchmark. In this work, we propose a data driven subspace approach to calculate the LQG benchmark under closed-loop conditions with certain external excitations. The optimal LQG-benchmark variances are obtained directly from the subspace matrices corresponding to the deterministic inputs and the stochastic inputs, which are identified using closed-loop data with setpoint excitation. These variances are used for assessing the controller performance. The method proposed in this paper is applicable to both univariate and multivariate systems. Profit analysis for the implementation of feedforward control to the existing feedback-only control system is also analyzed under the optimal LQG performance framework", "label": ["controller performance analysis", "lqg benchmark", "linear quadratic gaussian benchmark", "closed-loop data", "subspace matrices", "deterministic inputs", "stochastic inputs", "univariate systems", "multivariate systems", "profit analysis", "feedforward control", "state space model"], "stemmed_label": ["control perform analysi", "lqg benchmark", "linear quadrat gaussian benchmark", "closed-loop data", "subspac matric", "determinist input", "stochast input", "univari system", "multivari system", "profit analysi", "feedforward control", "state space model"]}
{"doc": "Relativistic constraints on the distinguishability of orthogonal quantum states The constraints imposed by special relativity on the distinguishability of quantum states are discussed. An explicit expression relating the probability of an error in distinguishing two orthogonal single-photon states to their structure, the time t at which a measurement starts, and the interval of time T elapsed from the start of the measurement until the time at which the outcome is obtained by an observer is given as an example", "label": ["relativistic constraints", "orthogonal quantum states", "special relativity", "orthogonal single-photon states", "time interval", "observer", "nonrelativistic quantum information theory", "quantum communication channels", "quantum-state distinguishability"], "stemmed_label": ["relativist constraint", "orthogon quantum state", "special rel", "orthogon single-photon state", "time interv", "observ", "nonrelativist quantum inform theori", "quantum commun channel", "quantum-st distinguish"]}
{"doc": "Conformal-mapping design tools for coaxial couplers with complex cross section Numerical conformal mapping is exploited as a simple, accurate, and efficient tool for the analysis and design of coaxial waveguides and couplers of complex cross section. An implementation based on the Schwarz-Christoffel Toolbox, a public-domain MATLAB package, is applied to slotted coaxial cables and to symmetrical coaxial couplers, with circular or polygonal inner conductors and external shields. The effect of metallic diaphragms of arbitrary thickness, partially separating the inner conductors, is also easily taken into account. The proposed technique is validated against the results of the finite-element method, showing excellent agreement at a fraction of the computational cost, and is also extended to the case of nonsymmetrical couplers, providing the designer with important additional degrees of freedom", "label": ["conformal mapping design tools", "coaxial couplers", "complex cross section", "coaxial waveguides", "schwarz-christoffel toolbox", "public-domain matlab package", "slotted coaxial cables", "symmetrical couplers", "circular inner conductors", "polygonal inner conductors", "external shields", "metallic diaphragms", "nonsymmetrical couplers", "numerical conformal transformations"], "stemmed_label": ["conform map design tool", "coaxial coupler", "complex cross section", "coaxial waveguid", "schwarz-christoffel toolbox", "public-domain matlab packag", "slot coaxial cabl", "symmetr coupler", "circular inner conductor", "polygon inner conductor", "extern shield", "metal diaphragm", "nonsymmetr coupler", "numer conform transform"]}
{"doc": "Writing the fulfillment RFP publishing For the uninitiated, writing a request for proposal can seem both mysterious and daunting. Here's a format that will make you look like a pro the first time out", "label": ["request for proposal", "fulfillment", "publisher"], "stemmed_label": ["request for propos", "fulfil", "publish"]}
{"doc": "State-of-the-art in orthopaedic surgical navigation with a focus on medical image modalities This paper presents a review of surgical navigation systems in orthopaedics and categorizes these systems according to the image modalities that are used for the visualization of surgical action. Medical images used to be an essential part of surgical education and documentation as well as diagnosis and operation planning over many years. With the recent introduction of navigation techniques in orthopaedic surgery, a new field of application has been opened. Today surgical navigation systems - also known as image-guided surgery systems - are available for various applications in orthopaedic surgery. They visualize the position and orientation of surgical instruments as graphical overlays onto a medical image of the operated anatomy on a computer monitor. Preoperative image data such as computed tomography scans or intra operatively generated images (for example, ultrasonic, endoscopic or fluoroscopic images) are suitable for this purpose. A new category of medical images termed 'surgeon-defined anatomy' has been developed that exclusively relies upon the usage of navigation technology. Points on the anatomy are digitized interactively by the surgeon and are used to build up an abstract geometrical model of the bony structures to be operated on. This technique may be used when no other image data is available or appropriate for a given application", "label": ["orthopaedic surgical navigation", "medical image modalities", "surgical action visualization", "medical image processing", "surgical education", "image-guided surgery systems", "surgical instruments", "graphical overlays", "computer monitor", "computed tomography scans", "intra operatively generated images", "surgeon-defined anatomy", "abstract geometrical model", "bony structures", "image registration"], "stemmed_label": ["orthopaed surgic navig", "medic imag modal", "surgic action visual", "medic imag process", "surgic educ", "image-guid surgeri system", "surgic instrument", "graphic overlay", "comput monitor", "comput tomographi scan", "intra oper gener imag", "surgeon-defin anatomi", "abstract geometr model", "boni structur", "imag registr"]}
{"doc": "Randomized two-process wait-free test-and-set We present the first explicit, and currently simplest, randomized algorithm for two-process wait-free test-and-set. It is implemented with two 4-valued single writer single reader atomic variables. A test-and-set takes at most 11 expected elementary steps, while a reset takes exactly 1 elementary step. Based on a finite-state analysis, the proofs of correctness and expected length are compressed into one table", "label": ["randomized two-process wait-free test-and-set", "randomized algorithm", "4-valued single writer single reader atomic variables", "expected elementary steps", "finite-state analysis", "correctness proofs", "symmetry breaking", "asynchronous distributed protocols", "fault-tolerance", "shared memory", "wait-free read/write registers"], "stemmed_label": ["random two-process wait-fre test-and-set", "random algorithm", "4-valu singl writer singl reader atom variabl", "expect elementari step", "finite-st analysi", "correct proof", "symmetri break", "asynchron distribut protocol", "fault-toler", "share memori", "wait-fre read/writ regist"]}
{"doc": "The California Digital Library and the eScholarship program The eScholarship program was launched in 2000 to foster faculty-led innovation in scholarly publishing. An initiative of the University of California (UC) and a program of the California Digital Library, the eScholarship program has stimulated significant interest in its short life. Its modest but visible accomplishments garner praise from many quarters, within and beyond the University of California. In perhaps the best indication of its timeliness and momentum, there are more proposals submitted to eScholarship today than the CDL can manage. This early success is due in part to the sheer power of an idea whose time has come, but also to the unique approach on which CDL was founded and the eScholarship initiative was first launched", "label": ["escholarship program", "faculty-led innovation", "california digital library", "scholarly publishing", "university of california"], "stemmed_label": ["escholarship program", "faculty-l innov", "california digit librari", "scholarli publish", "univers of california"]}
{"doc": "Evaluation of combined dispatching and routeing strategies for a flexible manufacturing system This paper deals with the evaluation of combined dispatching and routeing strategies on the performance of a flexible manufacturing system. Three routeing policies - no alternative routings, alternative routeing dynamics and alternative routeing plans - are considered with four dispatching rules with finite buffer capacity. In addition, the effect of changing part mix ratios is also discussed. The performance measures considered are makespan, average machine utilization, average flow time and average delay at local input buffers. Simulation results indicate that the alternative routings dynamic policy gives the best results in three performance measures except for average delay at local input buffers. Further, the effect of changing part mix ratios is not significant", "label": ["alternative routings", "flexible manufacturing system", "fms", "dispatching rules", "finite buffer capacity", "part mix ratios", "average flow time"], "stemmed_label": ["altern rout", "flexibl manufactur system", "fm", "dispatch rule", "finit buffer capac", "part mix ratio", "averag flow time"]}
{"doc": "Using molecular equivalence numbers to visually explore structural features that distinguish chemical libraries A molecular equivalence number (meqnum) classifies a molecule with respect to a class of structural features or topological shapes such as its cyclic system or its set of functional groups. Meqnums can be used to organize molecular structures into nonoverlapping, yet highly relatable classes. We illustrate the construction of some different types of meqnums and present via examples some methods of comparing diverse chemical libraries based on meqnums. In the examples we compare a library which is a random sample from the MDL Drug Data Report (MDDR) with a library which is a random sample from the Available Chemical Directory (ACD). In our analyses, we discover some interesting features of the topological shape of a molecule and its set of functional groups that are strongly linked with compounds occurring in the MDDR but not in the ACD. We also illustrate the utility of molecular equivalence indices in delineating the structural domain over which an SAR conclusion is valid", "label": ["molecular equivalence number", "molecule classification", "structural features", "topological shapes", "cyclic system", "functional groups", "nonoverlapping relatable classes", "chemical libraries", "mdl drug data report", "available chemical directory", "molecular equivalence indices"], "stemmed_label": ["molecular equival number", "molecul classif", "structur featur", "topolog shape", "cyclic system", "function group", "nonoverlap relat class", "chemic librari", "mdl drug data report", "avail chemic directori", "molecular equival indic"]}
{"doc": "MTD-PLS: a PLS-based variant of the MTD method. II. Mapping ligand-receptor interactions. Enzymatic acetic acid esters hydrolysis The PLS variant of the MTD method (T.I. Oprea et al., SAR QSAR Environ. Res. 2001, 12, 75-92) was applied to a series of 25 acetylcholinesterase hydrolysis substrates. Statistically significant MTD-PLS models (q/sup 2/ between 0.7 and 0.8) are in agreement with previous MTD models, with the advantage that local contributions are understood beyond the occupancy/nonoccupancy interpretation in MTD. A \"chemically intuitive\" approach further forces MTD-PLS coefficients to assume only negative (or zero) values for fragmental volume descriptors and positive (or zero) values for fragmental hydrophobicity descriptors. This further separates the various kinds of local interactions at each vertex of the MTD hypermolecule, making this method suitable for medicinal chemistry synthesis planning", "label": ["minimum topological difference method", "mtd-pls models", "pls-based variant", "ligand-receptor interactions mapping", "enzymatic acetic acid esters hydrolysis", "acetylcholinesterase hydrolysis substrates", "chemically intuitive approach", "fragmental volume descriptors", "fragmental hydrophobicity descriptors", "hypermolecule", "medicinal chemistry synthesis planning", "steric misfit", "additive approach", "intermolecular force categories", "regression coefficients", "ligand binding affinity", "hydrogen bonding", "polarizabilities", "statistical model stability"], "stemmed_label": ["minimum topolog differ method", "mtd-pl model", "pls-base variant", "ligand-receptor interact map", "enzymat acet acid ester hydrolysi", "acetylcholinesteras hydrolysi substrat", "chemic intuit approach", "fragment volum descriptor", "fragment hydrophob descriptor", "hypermolecul", "medicin chemistri synthesi plan", "steric misfit", "addit approach", "intermolecular forc categori", "regress coeffici", "ligand bind affin", "hydrogen bond", "polariz", "statist model stabil"]}
{"doc": "Industry insiders loading up on cheap company stock A surge of telecom executives and directors purchasing their own companies, stock in the last two months points toward a renewed optimism in the beleaguered sector, say some observers, who view the rash of insider buying as a vote of confidence from management. Airgate PCS, Charter Communications, Cox Communications, Crown Castle International, Nextel Communications and Nortel Networks all have seen infusions of insider investment this summer, echoing trends in both the telecom industry and the national economy", "label": ["telecom industry", "insider investment"], "stemmed_label": ["telecom industri", "insid invest"]}
{"doc": "Universal dynamic synchronous self-stabilization We prove the existence of a \"universal\" synchronous self-stabilizing protocol, that is, a protocol that allows a distributed system to stabilize to a desired nonreactive behaviour (as long as a protocol stabilizing to that behaviour exists). Previous proposals required drastic increases in asymmetry and knowledge to work, whereas our protocol does not use any additional knowledge, and does not require more symmetry-breaking conditions than available; thus, it is also stabilizing with respect to dynamic changes in the topology. We prove an optimal quiescence time n + D for a synchronous network of n processors and diameter D; the protocol can be made finite state with a negligible loss in quiescence time. Moreover, an optimal D + 1 protocol is given for the case of unique identifiers. As a consequence, we provide an effective proof technique that allows one to show whether self-stabilization to a certain behaviour is possible under a wide range of models", "label": ["universal dynamic synchronous self-stabilization", "synchronous self-stabilizing protocol", "distributed system", "nonreactive behaviour", "topology", "dynamic changes", "optimal quiescence time", "synchronous network", "finite state", "quiescence time", "optimal protocol", "unique identifiers", "proof technique", "self-stabilization", "anonymous networks", "graph fibrations"], "stemmed_label": ["univers dynam synchron self-stabil", "synchron self-stabil protocol", "distribut system", "nonreact behaviour", "topolog", "dynam chang", "optim quiescenc time", "synchron network", "finit state", "quiescenc time", "optim protocol", "uniqu identifi", "proof techniqu", "self-stabil", "anonym network", "graph fibrat"]}
{"doc": "Real-time tissue characterization on the basis of in vivo Raman spectra The application of in vivo Raman spectroscopy for clinical diagnosis demands dedicated software that can perform the necessary signal processing and subsequent (multivariate) data analysis, enabling clinically relevant parameters to be extracted and made available in real time. Here we describe the design and implementation of a software package that allows for real-time signal processing and data analysis of Raman spectra. The design is based on automatic data exchange between Grams, a spectroscopic data acquisition and analysis program, and Matlab, a program designed for array-based calculations. The data analysis software has a modular design providing great flexibility in developing custom data analysis routines for different applications. The implementation is illustrated by a computationally demanding application for the classification of skin spectra using principal component analysis and linear discriminant analysis", "label": ["real-time tissue characterization", "clinically relevant parameters extraction", "array-based calculations", "computationally demanding application", "modular design", "data analysis software", "clinical diagnosis", "dedicated software", "multivariate data analysis", "automatic data exchange", "grams", "matlab", "linear discriminant analysis", "skin spectra classification"], "stemmed_label": ["real-tim tissu character", "clinic relev paramet extract", "array-bas calcul", "comput demand applic", "modular design", "data analysi softwar", "clinic diagnosi", "dedic softwar", "multivari data analysi", "automat data exchang", "gram", "matlab", "linear discrimin analysi", "skin spectra classif"]}
{"doc": ".NET obfuscation and intellectual property The author considers obfuscation options for protecting .NET code. Many programs won't need obfuscation because the loss caused by reverse engineering will be nonexistent. Numerous obfuscators are already available for the .NET platform, ranging from a basic renaming obfuscator to a fully functional obfuscator that handles mixed IL/native code assemblies created in any managed language, including Microsoft's C++ with Managed Extensions. An obfuscator simply makes your application harder to reverse engineer. It does not prevent reverse engineering. However, the cost of obfuscation is insignificant when compared to the cost of a typical software development project. If you feel like an obfuscator provides you any benefit at all, it's probably worth the price", "label": [".net obfuscation", "intellectual property", "reverse engineering"], "stemmed_label": [".net obfusc", "intellectu properti", "revers engin"]}
{"doc": "The UK's National Electronic Site Licensing Initiative (NESLI) In 1998 the UK created the National Electronic Site Licensing Initiative (NESLI) to increase and improve access to electronic journals and to negotiate license agreements on behalf of academic libraries. The use of a model license agreement and the success of site licensing is discussed. Highlights from an interim evaluation by the Joint Information Systems Committee (JISC) are noted and key issues and questions arising from the evaluation are identified", "label": ["national electronic site licensing initiative", "nesli", "electronic journals", "license agreements", "academic libraries", "joint information systems committee", "usage statistics", "jisc", "icolc"], "stemmed_label": ["nation electron site licens initi", "nesli", "electron journal", "licens agreement", "academ librari", "joint inform system committe", "usag statist", "jisc", "icolc"]}
{"doc": "An object-oriented version of SIMLIB (a simple simulation package) This paper introduces an object-oriented version of SIMLIB (an easy-to-understand discrete-event simulation package). The object-oriented version is preferable to the original procedural language versions of SIMLIB in that it is easier to understand and teach simulation from an object point of view. A single-server queue simulation is demonstrated using the object-oriented SIMLIB", "label": ["object-oriented version", "simlib", "discrete-event simulation", "teach simulation"], "stemmed_label": ["object-ori version", "simlib", "discrete-ev simul", "teach simul"]}
{"doc": "Fuzzy modeling based on generalized conjunction operations An approach to fuzzy modeling based on the tuning of parametric conjunction operations is proposed. First, some methods for the construction of parametric generalized conjunction operations simpler than the known parametric classes of conjunctions are considered and discussed. Second, several examples of function approximation by fuzzy models, based on the tuning of the parameters of the new conjunction operations, are given and their approximation performances are compared with the approaches based on a tuning of membership functions and other approaches proposed in the literature. It is seen that the tuning of the conjunction operations can be used for obtaining fuzzy models with a sufficiently good performance when the tuning of membership functions is not possible or not desirable", "label": ["fuzzy modeling", "generalized conjunction operations", "function approximation", "tuning", "approximation performances", "membership functions", "t-norm", "fuzzy inference systems"], "stemmed_label": ["fuzzi model", "gener conjunct oper", "function approxim", "tune", "approxim perform", "membership function", "t-norm", "fuzzi infer system"]}
{"doc": "A 0.8-V 128-kb four-way set-associative two-level CMOS cache memory using two-stage wordline/bitline-oriented tag-compare (WLOTC/BLOTC) scheme This paper reports a 0.8-V 128-kb four-way set-associative two-level CMOS cache memory using a novel two-stage wordline/bitline-oriented tag-compare (WLOTC/BLOTC) and sense wordline/bitline (SWL/SBL) tag-sense amplifiers with an eight-transistor (8-T) tag cell in Level 2 (L2) and a 10-T shrunk logic swing (SLS) memory cell. with the ground/floating (G/F) data sense amplifier in Level 1 (L1) for high-speed operation for low-voltage low-power VLSI system applications. Owing to the reduced loading at the SWL in the new 11-T tag cell using the WLOTC scheme, the 10-T SLS memory cell with G/F sense amplifier in L1, and the split comparison of the index signal in the 8-T tag cells with SWL/SBL tag sense amplifiers in L2, this 0.8-V cache memory implemented in a 1.8-V 0.18- mu m CMOS technology has a measured L1/L2 hit time of 11.6/20.5 ns at the average dissipation of 0.77 mW at 50 MHz", "label": ["four-way set-associative memory", "two-level cmos cache memory", "cache memory architecture", "wordline/bitline-oriented tag-compare", "sense wordline/bitline amplifiers", "tag-sense amplifiers", "eight-transistor tag cell", "ten-transistor memory cell", "shrunk logic swing memory cell", "ground/floating data sense amplifier", "high-speed operation", "low-voltage vlsi system applications", "low-power vlsi system applications", "0.8 v", "128 kbit", "50 mhz", "0.77 mw", "1.8 v", "0.18 micron", "11.6 ns", "20.5 ns"], "stemmed_label": ["four-way set-associ memori", "two-level cmo cach memori", "cach memori architectur", "wordline/bitline-ori tag-compar", "sens wordline/bitlin amplifi", "tag-sens amplifi", "eight-transistor tag cell", "ten-transistor memori cell", "shrunk logic swing memori cell", "ground/float data sens amplifi", "high-spe oper", "low-voltag vlsi system applic", "low-pow vlsi system applic", "0.8 v", "128 kbit", "50 mhz", "0.77 mw", "1.8 v", "0.18 micron", "11.6 ns", "20.5 ns"]}
{"doc": "Elimination of zero-order diffraction in digital holography A simple method to suppress the zero-order diffraction in the reconstructed image of digital holography is presented. In this method, the Laplacian of a detected hologram is used instead of the hologram itself for numerical reconstruction by computing the discrete Fresnel integral. This method can significantly improve the image quality and give better resolution and higher accuracy of the reconstructed image. The main advantages of this method are its simplicity in experimental requirements and convenience in data processing", "label": ["zero-order diffraction suppression", "reconstructed image", "laplacian", "detected hologram", "numerical image reconstruction", "discrete fresnel integral", "image quality", "image resolution", "accuracy", "data processing", "image. processing", "digital holography"], "stemmed_label": ["zero-ord diffract suppress", "reconstruct imag", "laplacian", "detect hologram", "numer imag reconstruct", "discret fresnel integr", "imag qualiti", "imag resolut", "accuraci", "data process", "image. process", "digit holographi"]}
{"doc": "Underground poetry, collecting poetry, and the librarian A powerful encounter with underground poetry and its important role in poetry, literature, and culture is discussed. The acquisitions difficulties encountered in the unique publishing world of underground poetry are introduced. Strategies for acquiring underground poetry for library collections are proposed, including total immersion and local focus, with accompanying action", "label": ["librarian", "underground poetry", "publishing", "library collections", "out-of-print books", "special collections", "literature", "culture"], "stemmed_label": ["librarian", "underground poetri", "publish", "librari collect", "out-of-print book", "special collect", "literatur", "cultur"]}
{"doc": "Building a better game through dynamic programming: a Flip analysis Flip is a solitaire board game produced by craft woodworkers. We analyze Flip and suggest modifications to the rules to make the game more marketable. In addition to being an interesting application of dynamic programming, this case shows the use of operations research in managerial decision making", "label": ["dynamic programming", "flip analysis", "operations research", "managerial decision making", "solitaire board game", "craft woodworkers"], "stemmed_label": ["dynam program", "flip analysi", "oper research", "manageri decis make", "solitair board game", "craft woodwork"]}
{"doc": "A pretopological approach for structural analysis The aim of this paper is to present a methodological approach for problems encountered in structural analysis. This approach is based upon the pretopological concepts of pseudoclosure and minimal closed subsets. The advantage of this approach is that it provides a framework which is general enough to model and formulate different types of connections that exist between the elements of a population. In addition, it has enabled us to develop a new structural analysis algorithm. An explanation of the definitions and properties of the pretopological concepts applied in this work is first shown and illustrated in sample settings. The structural analysis algorithm is then described and the results obtained in an economic study of the impact of geographic proximity on scientific collaborations are presented", "label": ["pretopological approach", "structural analysis", "minimal closed subsets", "pseudoclosure", "connections", "economic study", "geographic proximity", "scientific collaborations"], "stemmed_label": ["pretopolog approach", "structur analysi", "minim close subset", "pseudoclosur", "connect", "econom studi", "geograph proxim", "scientif collabor"]}
{"doc": "Using NetCloak to develop server-side Web-based experiments without writing CGI programs Server-side experiments use the Web server, rather than the participant's browser, to handle tasks such as random assignment, eliminating inconsistencies with Java and other client-side applications. Heretofore, experimenters wishing to create server-side experiments have had to write programs to create common gateway interface (CGI) scripts in programming languages such as Perl and C++. NetCloak uses simple, HTML-like commands to create CGIs. We used NetCloak to implement an experiment on probability estimation. Measurements of time on task and participants' IP addresses assisted quality control. Without prior training, in less than 1 month, we were able to use NetCloak to design and create a Web-based experiment and to help graduate students create three Web-based experiments of their own", "label": ["netcloak", "server-side web-based experiments", "cgi programs", "web server", "random assignment", "java", "client-side applications", "common gateway interface scripts", "perl", "c++ language", "html", "probability estimation", "ip addresses", "quality control", "graduate students", "internet", "behavioral data", "psychology"], "stemmed_label": ["netcloak", "server-sid web-bas experi", "cgi program", "web server", "random assign", "java", "client-sid applic", "common gateway interfac script", "perl", "c++ languag", "html", "probabl estim", "ip address", "qualiti control", "graduat student", "internet", "behavior data", "psycholog"]}
{"doc": "Finally! some sensible European legislation on software The European Commission has formally tabled a draft Directive on the Protection by Patents of Computer-Implemented Inventions. The aim of this very important Directive is to harmonise national patent laws relating to inventions using software. It follows an extensive consultation launched by the Commission in October 2000. The impetus behind the Directive was the recognition at EU level of a total lack of unity between the European Patent Office and European national courts in deciding what was or was not deemed patentable when it came to the subject of computer programs", "label": ["european commission", "directive on the protection by patents of computer-implemented inventions", "national patent laws", "law harmonisation", "eu", "european patent office", "national courts", "computer programs"], "stemmed_label": ["european commiss", "direct on the protect by patent of computer-impl invent", "nation patent law", "law harmonis", "eu", "european patent offic", "nation court", "comput program"]}
{"doc": "Control centers are here to stay Despite changes with different structures, market rules, and uncertainties, a control center must always be in place to maintain the security, reliability, and quality of electric service. This article focuses on the energy management system (EMS) control center, identifying the major functions that have become standard components of every application software package. The two most important control center functions, security control and load-following control, guarantee the continuity of electric service, which after all, is the end-product of the utility business. New technology trends in the design of control center infrastructures are emerging in the liberalized environment of the energy market. An example of a control center infrastructure is described. The article ends with a concern for the security of the control center itself", "label": ["ems control centers", "energy management system", "standard components", "application software package", "security control", "load-following control", "electric service continuity", "control center infrastructures", "liberalized environment", "energy market"], "stemmed_label": ["em control center", "energi manag system", "standard compon", "applic softwar packag", "secur control", "load-follow control", "electr servic continu", "control center infrastructur", "liber environ", "energi market"]}
{"doc": "On emotion and bounded rationality: reply to Hanoch The author refers to the comment made by Hanoch (see ibid. vol.49 (2000)) on his model of bounded rationality and the role of the Yerkes-Dodson law and emotional arousal in it. The author points out that Hanoch's comment, however, conspicuously fails to challenge - much less contradict - the central hypothesis of his paper. In addition, several of Hanoch's criticisms are based on a wrong characterization of the positions", "label": ["emotion", "bounded rationality", "yerkes-dodson law", "decision-making", "psychology"], "stemmed_label": ["emot", "bound ration", "yerkes-dodson law", "decision-mak", "psycholog"]}
{"doc": "Fitting mixed-effects models for repeated ordinal outcomes with the NLMIXED procedure This paper presents an analysis of repeated ordinal outcomes arising from two psychological studies. The first case is a repeated measures analysis of variance; the second is a mixed-effects regression. in a longitudinal design. In both, the subject-specific variation is modeled by including random effects in the linear predictor (inside a link function) of a generalized linear model. The NLMIXED procedure in SAS is used to fit the mixed-effects models for the categorical response data. The presentation emphasizes the parallel between the model. specifications and the SAS statements. The purpose of this paper is to facilitate the use of mixed-effects models in the analysis of repeated ordinal outcomes", "label": ["repeated ordinal outcomes", "psychological studies", "repeated measures analysis of variance", "mixed-effects regression", "longitudinal design", "subject-specific variation modeling", "random effects", "linear predictor", "generalized linear model", "nlmixed procedure", "mixed-effects model fitting", "categorical response data", "model specifications"], "stemmed_label": ["repeat ordin outcom", "psycholog studi", "repeat measur analysi of varianc", "mixed-effect regress", "longitudin design", "subject-specif variat model", "random effect", "linear predictor", "gener linear model", "nlmix procedur", "mixed-effect model fit", "categor respons data", "model specif"]}
{"doc": "Customer in-reach and library strategic systems: the case of ILLiad Libraries have walls. Recognizing this fact, the Interlibrary Loan Department at Virginia Tech is creating systems and services that enable our customers to reach past our walls at anytime from anywhere. Customer in-reach enables Virginia Tech faculty, students, and staff anywhere in the world to obtain information and services heretofore available only to our on-campus customers. ILLiad, Virginia Tech's interlibrary borrowing system, is the library strategic system that attains this goal. The principles that guided development of ILLiad are widely applicable", "label": ["library strategic systems", "interlibrary loan department", "virginia tech", "customer in-reach", "illiad", "interlibrary borrowing system"], "stemmed_label": ["librari strateg system", "interlibrari loan depart", "virginia tech", "custom in-reach", "illiad", "interlibrari borrow system"]}
{"doc": "Implications of document-level literacy skills for Web site design The proliferation of World Wide Web (Web) sites and the low cost of publishing information on the Web have placed a tremendous amount of information at the fingertips of millions of people. Although most of this information is at least intended to be accurate, there is much that is rumor, innuendo, urban legend, and outright falsehood. This raises problems especially for students (of all ages) trying to do research or learn about some topic. Finding accurate, credible information requires document level literacy skills, such as integration, sourcing, corroboration, and search. This paper discusses these skills and offers a list of simple ways that designers of educational Web sites can help their visitors utilize these skills", "label": ["document-level literacy skills", "rumor", "innuendo", "urban legend", "falsehood", "students", "accurate credible information", "integration", "sourcing", "corroboration", "search", "educational web site design"], "stemmed_label": ["document-level literaci skill", "rumor", "innuendo", "urban legend", "falsehood", "student", "accur credibl inform", "integr", "sourc", "corrobor", "search", "educ web site design"]}
{"doc": "When a better interface and easy navigation aren't enough: examining the information architecture in a law enforcement agency An information architecture that allows users to easily navigate through a system and quickly recover from mistakes is often defined as a highly usable system. But usability in systems design goes beyond a good interface and efficient navigation. In this article we describe two database systems in a law enforcement agency. One system is a legacy, text-based system with cumbersome navigation (RMS); the newer system is a graphical user interface with simplified navigation (CopNet). It is hypothesized that law enforcement users will evaluate CopNet higher than RMS, but experts of the older system will evaluate it higher than others will. We conducted two user studies. One study examined what users thought of RMS and CopNet, and compared RMS experts' evaluations with nonexperts. We found that all users evaluated CopNet as more effective, easier to use, and easier to navigate than RMS, and this was especially noticeable for users who were not experts with the older system. The second, follow-up study examined use behavior after CopNet was deployed some time later. The findings revealed that evaluations of CopNet were not associated with its use. If the newer system had a better interface and was easier to navigate than the older, legacy system, why were law enforcement personnel reluctant to switch? We discuss reasons why switching to a new system is difficult, especially for those who are most adept at using the older system. Implications for system design and usability are also discussed", "label": ["information architecture", "law enforcement agency", "legacy text-based system", "rms", "graphical user interface", "simplified navigation", "copnet", "law enforcement users"], "stemmed_label": ["inform architectur", "law enforc agenc", "legaci text-bas system", "rm", "graphic user interfac", "simplifi navig", "copnet", "law enforc user"]}
{"doc": "Separation and tracking of multiple broadband sources with one electromagnetic vector sensor A structure for adaptively separating, enhancing and tracking uncorrelated sources with an electromagnetic vector sensor (EMVS) is presented. The structure consists of a set of parallel spatial processors, one for each individual source. Two stages of processing are involved in each spatial processor. The first preprocessing stage rejects all other sources except the one of interest, while the second stage is an adaptive one for maximizing the signal-to-noise ratio (SNR) and tracking the desired source. The preprocessings are designed using the latest source parameter estimates obtained from the source trackers, and a redesign is activated periodically or whenever any source has been detected by the source trackers to have made significant movement. Compared with conventional adaptive beamforming, the algorithm has the advantage that no a priori information on any desired signal location is needed, the sources are separated at maximum SNR, and their locations are available. The structure is also well suited for parallel implementation. Numerical examples are included to illustrate the capability and performance of the algorithm", "label": ["multiple broadband sources separation", "multiple broadband sources tracking", "uncorrelated sources", "electromagnetic vector sensor", "single em vector sensor", "parallel spatial processors", "preprocessing stage", "adaptive second stage", "signal-to-noise ratio", "snr maximization", "maximum snr", "signal source location", "parallel implementation", "adaptive source enhancement"], "stemmed_label": ["multipl broadband sourc separ", "multipl broadband sourc track", "uncorrel sourc", "electromagnet vector sensor", "singl em vector sensor", "parallel spatial processor", "preprocess stage", "adapt second stage", "signal-to-nois ratio", "snr maxim", "maximum snr", "signal sourc locat", "parallel implement", "adapt sourc enhanc"]}
{"doc": "Internet-based psychological experimenting: five dos and five don'ts Internet-based psychological experimenting is presented as a method that needs careful consideration of a number of issues-from potential data corruption to revealing confidential information about participants. Ten issues are grouped into five areas of actions to be taken when developing an Internet experiment (dos) and five errors to be avoided (don'ts). Dos include: (a) utilizing dropout as a dependent variable, (b) the use of dropout to detect motivational confounding, (c) placement of questions for personal information, (d) using a collection of techniques, and (e) using Internet-based tools. Don'ts are about: (a) unprotected directories, (b) public access to confidential data, (c) revealing the experiment's structure, (d) ignoring the Internet's technical variance, and (e) improper use of form elements", "label": ["internet-based psychological experimenting", "data corruption", "data confidentiality", "dropout", "motivational confounding", "personal information", "unprotected directories", "web experiment", "online research techniques", "psychology"], "stemmed_label": ["internet-bas psycholog experi", "data corrupt", "data confidenti", "dropout", "motiv confound", "person inform", "unprotect directori", "web experi", "onlin research techniqu", "psycholog"]}
{"doc": "Post-haste. 100th robotic containerization system installed in US mail sorting center Spot welding, machine tending, material handling, picking, packing, painting, palletizing, assembly...the list of tasks being performed by ABB robots keeps on growing. Adding to this portfolio is a new robot containerization system (RCS) that ABB developed specifically for the United States Postal Service (USPS). The RCS has brought new levels of speed, accuracy, efficiency and productivity to the process of sorting and containerizing mail and packages. Recently, the 100th ABB RCS was installed at the USPS processing and distribution center in Columbus, Ohio", "label": ["mail sorting center", "robotic containerization system", "usa", "abb robots", "united states postal service", "mail sorting", "packages sorting"], "stemmed_label": ["mail sort center", "robot container system", "usa", "abb robot", "unit state postal servic", "mail sort", "packag sort"]}
{"doc": "General solution of a density functionally gradient piezoelectric cantilever and its applications We have used the plane strain theory of transversely isotropic bodies to study a piezoelectric cantilever. In order to find the general solution of a density functionally gradient piezoelectric cantilever, we have used the inverse method (i.e. the Airy stress function method). We have obtained the stress and induction functions in the form of polynomials as well as the general solution of the beam. Based on this general solution, we have deduced the solutions of the cantilever under different loading conditions. Furthermore, as applications of this general solution in engineering, we have studied the tip deflection and blocking force of a piezoelectric cantilever actuator. Finally, we have addressed a method to determine the density distribution profile for a given piezoelectric material", "label": ["plane strain theory", "transversely isotropic bodies", "inverse method", "airy stress function", "polynomials", "loading conditions", "piezoelectric cantilever actuator", "density distribution profile", "piezoelectric material"], "stemmed_label": ["plane strain theori", "transvers isotrop bodi", "invers method", "airi stress function", "polynomi", "load condit", "piezoelectr cantilev actuat", "densiti distribut profil", "piezoelectr materi"]}
{"doc": "Identification of evolving fuzzy rule-based models An approach to identification of evolving fuzzy rule-based (eR) models is proposed. eR models implement a method for the noniterative update of both the rule-base structure and parameters by incremental unsupervised learning. The rule-base evolves by adding more informative rules than those that previously formed the model. In addition, existing rules can be replaced with new rules based on ranking using the informative potential of the data. In this way, the rule-base structure is inherited and updated when new informative data become available, rather than being completely retrained. The adaptive nature of these evolving rule-based models, in combination with the highly transparent and compact form of fuzzy rules, makes them a promising candidate for modeling and control of complex processes, competitive to neural networks. The approach has been tested on a benchmark problem and on an air-conditioning component modeling application using data from an installation serving a real building. The results illustrate the viability and efficiency of the approach", "label": ["evolving fuzzy rule-based models", "identification", "noniterative update", "rule-base structure", "incremental unsupervised learning", "ranking", "informative potential", "fuzzy rules", "complex processes", "air-conditioning component modeling", "adaptive nonlinear control", "fault detection", "fault diagnostics", "performance analysis", "forecasting", "knowledge extraction", "robotics", "behavior modeling"], "stemmed_label": ["evolv fuzzi rule-bas model", "identif", "nonit updat", "rule-bas structur", "increment unsupervis learn", "rank", "inform potenti", "fuzzi rule", "complex process", "air-condit compon model", "adapt nonlinear control", "fault detect", "fault diagnost", "perform analysi", "forecast", "knowledg extract", "robot", "behavior model"]}
{"doc": "Lower bounds on the information rate of secret sharing schemes with homogeneous access structure We present some new lower bounds on the optimal information rate and on the optimal average information rate of secret sharing schemes with homogeneous access structure. These bounds are found by using some covering constructions and a new parameter, the k-degree of a participant, that is introduced in this paper. Our bounds improve the previous ones in almost all cases", "label": ["lower bounds", "optimal information rate", "optimal average information rate", "k-degree", "cryptography", "information rate", "secret sharing schemes", "homogeneous access structure"], "stemmed_label": ["lower bound", "optim inform rate", "optim averag inform rate", "k-degre", "cryptographi", "inform rate", "secret share scheme", "homogen access structur"]}
{"doc": "Search for efficient solutions of multi-criterion problems by target-level method The target-level method is considered for solving continuous multi-criterion maximization problems. In the first step, the decision-maker specifies a target-level point (the desired criterion values); then in the set of vector evaluations we seek points that are closest to the target point in the Chebyshev metric. The vector evaluations obtained in this way are in general weakly efficient. To identify the efficient evaluations, the second step maximizes the sum of the criteria on the set generated in step 1. We prove the relationship between the evaluations and decisions obtained by the proposed procedure, on the one hand, and the efficient (weakly efficient) evaluations and decisions, on the other hand. If the Edgeworth-Pareto hull of the set of vector evaluations is convex, the set of efficient vector evaluations can be approximated by the proposed method", "label": ["multi-criterion problems", "target-level method", "continuous multi-criterion maximization problems", "target-level point", "chebyshev metric", "edgeworth-pareto hull"], "stemmed_label": ["multi-criterion problem", "target-level method", "continu multi-criterion maxim problem", "target-level point", "chebyshev metric", "edgeworth-pareto hull"]}
{"doc": "Simple minds health care IT A few things done properly, and soon, is the short-term strategy for the UK NHS IT programme. Can it deliver this time?", "label": ["uk nhs it programme", "health care", "strategy"], "stemmed_label": ["uk nh it programm", "health care", "strategi"]}
{"doc": "Learning spatial relations using an inductive logic programming system The ability to learn spatial relations is a prerequisite for performing many relevant tasks such as those associated with motion, orientation, navigation, etc. This paper reports on using an Inductive Logic Programming (ILP) system for learning function-free Horn-clause descriptions of spatial knowledge. Its main contribution, however, is to show that an existing relation between two reference systems-the speaker-relative and the absolute-can be automatically learned by an ILP system, given the proper background knowledge and positive examples", "label": ["spatial relations learning", "inductive logic programming system", "spatial relations", "function-free horn-clause descriptions"], "stemmed_label": ["spatial relat learn", "induct logic program system", "spatial relat", "function-fre horn-claus descript"]}
{"doc": "Strong active solution in non-cooperative games For the non-cooperative games and the problems of accepting or rejecting a proposal, a new notion of equilibrium was proposed, its place among the known basic equilibria was established, and its application to the static and dynamic game problems was demonstrated", "label": ["strong active solution", "noncooperative games", "static game problems", "dynamic game problems"], "stemmed_label": ["strong activ solut", "noncoop game", "static game problem", "dynam game problem"]}
{"doc": "Model intestinal microflora in computer simulation: a simulation and modeling package for host-microflora interactions The ecology of the human intestinal microflora and its interaction with the host are poorly understood. Though more and more data are being acquired, in part using modern molecular methods, development of a quantitative theory has not kept pace with this increase in observing power. This is in part due to the complexity of the system and to the lack of simulation environments in which to test what the ecological effect of a hypothetical mechanism of interaction would be, before resorting to laboratory experiments. The MIMICS project attempts to address this through the development of a cellular automaton for simulation of the intestinal microflora. In this paper, the design and evaluation of this simulator is discussed", "label": ["human intestines", "intestinal microflora", "molecular methods", "observing power", "system complexity", "microbial ecology", "parallel computing", "host-microflora interactions", "quantitative theory", "mimics project", "complex microbial ecosystem"], "stemmed_label": ["human intestin", "intestin microflora", "molecular method", "observ power", "system complex", "microbi ecolog", "parallel comput", "host-microflora interact", "quantit theori", "mimic project", "complex microbi ecosystem"]}
{"doc": "Option pricing from path integral for non-Gaussian fluctuations. Natural martingale and application to truncated Levy distributions Within a path integral formalism for non-Gaussian price fluctuations, we set up a simple stochastic calculus and derive a natural martingale for option pricing from the wealth balance of options, stocks, and bonds. The resulting formula is evaluated for truncated Levy distributions", "label": ["option pricing", "path integrals", "stochastic calculus", "stocks", "bonds", "nongaussian fluctuations", "natural martingale", "truncated levy distributions"], "stemmed_label": ["option price", "path integr", "stochast calculu", "stock", "bond", "nongaussian fluctuat", "natur martingal", "truncat levi distribut"]}
{"doc": "Chemical information based scaling of molecular descriptors: a universal chemical scale for library design and analysis Scaling is a difficult issue for any analysis of chemical properties or molecular topology when disparate descriptors are involved. To compare properties across different data sets, a common scale must be defined. Using several publicly available databases (ACD, CMC, MDDR, and NCI) as a basis, we propose to define chemically meaningful scales for a number of molecular properties and topology descriptors. These chemically derived scaling functions have several advantages. First, it is possible to define chemically relevant scales, greatly simplifying similarity and diversity analyses across data sets. Second, this approach provides a convenient method for setting descriptor boundaries that define chemically reasonable topology spaces. For example, descriptors can be scaled so that compounds with little potential for biological activity, bioavailability, or other drug-like characteristics are easily identified as outliers. We have compiled scaling values for 314 molecular descriptors. In addition the 10th and 90th percentile values for each descriptor have been calculated for use in outlier filtering", "label": ["universal chemical scale", "library design", "library analysis", "chemical information based scaling", "molecular descriptors", "molecular topology", "chemical properties", "databases", "diversity analyses", "similarity analyses", "data sets", "descriptor boundaries", "drug-like characteristics", "biological activity", "bioavailability", "outliers"], "stemmed_label": ["univers chemic scale", "librari design", "librari analysi", "chemic inform base scale", "molecular descriptor", "molecular topolog", "chemic properti", "databas", "divers analys", "similar analys", "data set", "descriptor boundari", "drug-lik characterist", "biolog activ", "bioavail", "outlier"]}
{"doc": "Integrating building management system and facilities management on the Internet Recently, it is of great interest to adopt the Internet/intranet to develop building management systems (BMS) and facilities management systems (FMS). This paper addresses two technical issues: the Web-based access (including database integration) and the integration of BMS and FMS. These should be addressed for accessing BMS remotely via the Internet, integrating control networks using the Internet protocols and infrastructures, and using Internet/intranet for building facilities management. An experimental Internet-enabled system that integrates building and facilities management systems has been developed and tested. This system integrated open control networks with the Internet and is developed utilizing the embedded Web server, the PC Web server and the Distributed Component Object Model (DCOM) software development technology on the platform of an open control network. Three strategies for interconnecting BMS local networks via Internet/intranet are presented and analyzed", "label": ["intranet", "building management systems", "bms", "facilities management systems", "fms", "web-based access", "database integration", "internet protocols", "embedded web server", "pc web server", "distributed component object model", "dcom", "software development technology", "open control network", "local network interconnection"], "stemmed_label": ["intranet", "build manag system", "bm", "facil manag system", "fm", "web-bas access", "databas integr", "internet protocol", "embed web server", "pc web server", "distribut compon object model", "dcom", "softwar develop technolog", "open control network", "local network interconnect"]}
{"doc": "Visual-word identification thresholds for the 260 fragmented words of the Snodgrass and Vanderwart pictures in Spanish Word difficulty varies from language to language; therefore, normative data of verbal stimuli cannot be imported directly from another language. We present mean identification thresholds for the 260 screen-fragmented words corresponding to the total set of Snodgrass and Vanderwart (1980) pictures. Individual words were fragmented in eight levels using Turbo Pascal, and the resulting program was implemented on a PC microcomputer. The words were presented individually to a group of 40 Spanish observers, using a controlled time procedure. An unspecific learning effect was found showing that performance improved due to practice with the task. Finally, of the 11 psycholinguistic variables that previous researchers have shown to affect word identification, only imagery accounted for a significant amount of variance in the threshold values", "label": ["visual-word identification thresholds", "fragmented words", "snodgrass and vanderwart pictures", "spanish", "word difficulty", "verbal stimuli", "mean identification thresholds", "screen-fragmented words", "turbo pascal", "pc microcomputer", "controlled time procedure", "unspecific learning effect", "psycholinguistic variables", "word identification"], "stemmed_label": ["visual-word identif threshold", "fragment word", "snodgrass and vanderwart pictur", "spanish", "word difficulti", "verbal stimuli", "mean identif threshold", "screen-frag word", "turbo pascal", "pc microcomput", "control time procedur", "unspecif learn effect", "psycholinguist variabl", "word identif"]}
{"doc": "From revenue management concepts to software systems In 1999, after developing and installing over 170 revenue management (RM) systems for more than 70 airlines, PROS Revenue Management, Inc. had the opportunity to develop RM systems for three companies in nonairline industries. PROS research and design department designed the opportunity analysis study (OAS), a mix of OR/MS, consulting, and software development practices to determine the applicability of RM in new business situations. PROS executed OASs with the three companies. In all three cases, the OAS supported the value of RM and led to contracts for implementation of RM systems", "label": ["revenue management concepts", "software systems", "rm systems", "pros revenue management", "inc", "opportunity analysis study", "oas", "or/ms", "consulting practices", "software development practices"], "stemmed_label": ["revenu manag concept", "softwar system", "rm system", "pro revenu manag", "inc", "opportun analysi studi", "oa", "or/m", "consult practic", "softwar develop practic"]}
{"doc": "Open courseware and shared knowledge in higher education Most college and university campuses in the United States and much of the developed world today maintain one, two, or several learning management systems (LMSs), which are courseware products that provide students and faculty with Web-based tools to manage course-related applications. Since the mid-1990s, two predominant models of Web courseware management systems have emerged: commercial and noncommercial. Some of the commercial products available today were created in academia as noncommercial but have since become commercially encumbered. Other products remain noncommercial but are struggling to survive in a world of fierce commercial competition. This article argues for an ethics of pedagogy in higher education that would be based on the guiding assumptions of the non-proprietary, peer-to-peer, open-source software movement", "label": ["open courseware", "shared knowledge", "higher education", "learning management systems", "college", "university", "web courseware management systems", "commercial products", "ethics", "internet", "open-source software"], "stemmed_label": ["open coursewar", "share knowledg", "higher educ", "learn manag system", "colleg", "univers", "web coursewar manag system", "commerci product", "ethic", "internet", "open-sourc softwar"]}
{"doc": "Application of multiprocessor systems for computation of jets The article describes the implementation of methods for numerical solution of gas-dynamic problems on a wide class of multiprocessor systems, conventionally characterized as \"cluster\" systems. A standard data-transfer interface - the so-called message passing interface - is used for parallelization of application algorithms among processors. Simulation of jets escaping into a low-pressure region is chosen as a computational example", "label": ["multiprocessor systems", "computation of jets", "gas-dynamic problems", "cluster systems", "data-transfer interface", "message passing interface", "low-pressure region"], "stemmed_label": ["multiprocessor system", "comput of jet", "gas-dynam problem", "cluster system", "data-transf interfac", "messag pass interfac", "low-pressur region"]}
{"doc": "Adaptive neural/fuzzy control for interpolated nonlinear systems Adaptive control for nonlinear time-varying systems is of both theoretical and practical importance. We propose an adaptive control methodology for a class of nonlinear systems with a time-varying structure. This class of systems is composed of interpolations of nonlinear subsystems which are input-output feedback linearizable. Both indirect and direct adaptive control methods are developed, where the spatially localized models (in the form of Takagi-Sugeno fuzzy systems or radial basis function neural networks) are used as online approximators to learn the unknown dynamics of the system. Without assumptions on rate of change of system dynamics, the proposed adaptive control methods guarantee that all internal signals of the system are bounded and the tracking error is asymptotically stable. The performance of the adaptive controller is demonstrated using a jet engine control problem", "label": ["adaptive neural/fuzzy control", "interpolated nonlinear systems", "time-varying systems", "input-output feedback linearizable systems", "indirect control", "direct control", "spatially localized models", "takagi-sugeno fuzzy systems", "radial basis function neural networks", "online approximators", "unknown dynamics", "tracking error", "jet engine control", "stability analysis"], "stemmed_label": ["adapt neural/fuzzi control", "interpol nonlinear system", "time-vari system", "input-output feedback lineariz system", "indirect control", "direct control", "spatial local model", "takagi-sugeno fuzzi system", "radial basi function neural network", "onlin approxim", "unknown dynam", "track error", "jet engin control", "stabil analysi"]}
{"doc": "Windows XP fast user switching The Windows NT family of operating systems has always supported the concept of multiple user accounts, but they've taken the concept a step further with Windows XP's Fast User Switching feature. Fast User Switching is a new feature of Windows XP that allows multiple users to log on to the same machine and quickly switch between the logged on accounts. Fast User Switching is implemented using some of the built-in capabilities of Terminal Services. Terminal Server has been around for a while but is much more feature rich and integrated in Windows XP. A machine with the terminal services (Remote Desktop) client can log on to and run applications on a remote machine running the terminal server", "label": ["windows xp fast user switching", "multiple user logon access", "operating systems", "multiple user accounts", "terminal services", "terminal server", "remote desktop"], "stemmed_label": ["window xp fast user switch", "multipl user logon access", "oper system", "multipl user account", "termin servic", "termin server", "remot desktop"]}
{"doc": "Virtual engineering office: a state-of-the-art platform for engineering collaboration A sales force in Latin America, the design department in Europe, and production in Asia? Arrangements of this kind are the new business reality for today's global manufacturing companies. But how are such global operations to be effectively coordinated? ABB's answer was to develop and implement a new platform for high-performance, real-time collaboration. Globally distributed engineering teams can now work together, regardless of time, location or the CAD system they use, making ABB easier to do business with, for customers as well as suppliers", "label": ["virtual engineering office", "state-of-the-art", "engineering collaboration platform", "business", "global manufacturing companies", "abb", "cad system", "globally distributed engineering teams"], "stemmed_label": ["virtual engin offic", "state-of-the-art", "engin collabor platform", "busi", "global manufactur compani", "abb", "cad system", "global distribut engin team"]}
{"doc": "An application of fuzzy linear regression to the information technology in Turkey Fuzzy set theory deals with the vagueness of human thought. A major contribution of fuzzy set theory is its capability of representing vague knowledge. Fuzzy set theory is very practical when sufficient and reliable data isn't available. Information technology (IT) is the acquisition, processing, storage and dissemination of information in all its forms (auditory, pictorial, textual and numerical) through a combination of computers, telecommunication, networks and electronic devices. IT includes matters concerned with the furtherance of computer science and technology, design, development, installation and implementation of information systems and applications. In the paper, assuming that there are n independent variables and the regression function is linear, the possible levels of information technology (the sale levels of computer equipment) in Turkey are forecasted by using fuzzy linear regression. The independent variables assumed are the import level and the export level of computer equipment", "label": ["fuzzy linear regression", "information technology", "turkey", "vague knowledge representation", "it", "computers", "telecommunication", "electronic devices", "computer science", "computer technology", "information systems", "regression function", "computer equipment export level"], "stemmed_label": ["fuzzi linear regress", "inform technolog", "turkey", "vagu knowledg represent", "it", "comput", "telecommun", "electron devic", "comput scienc", "comput technolog", "inform system", "regress function", "comput equip export level"]}
{"doc": "Teaching psychology as a laboratory science in the age of the Internet For over 30 years, psychologists have relied on computers to teach experimental psychology. With the advent of experiment generators, students can create well-designed experiments and can test sophisticated hypotheses from the start of their undergraduate training. Characteristics of new Net-based experiment generators are discussed and compared with traditional stand-alone generators. A call is made to formally evaluate the instructional effectiveness of the wide range of experiment generators now available. Specifically, software should be evaluated in terms of known learning outcomes, using appropriate control groups. The many inherent differences between any two software programs should be made clear. The teacher's instructional method should be fully described and held constant between comparisons. Finally, the often complex interaction between the teacher's instructional method and the pedagogical details of the software must be considered", "label": ["experimental psychology teaching", "laboratory science", "internet", "computers", "well-designed experiments", "hypothesis testing", "undergraduate training", "net-based experiment generators", "stand-alone generators", "instructional effectiveness", "software", "known learning outcomes", "control groups", "teacher instructional method", "pedagogical details"], "stemmed_label": ["experiment psycholog teach", "laboratori scienc", "internet", "comput", "well-design experi", "hypothesi test", "undergradu train", "net-bas experi gener", "stand-alon gener", "instruct effect", "softwar", "known learn outcom", "control group", "teacher instruct method", "pedagog detail"]}
{"doc": "Evicting orang utans from the office electronic storage of legal files Having espoused the principle of the paperless office some time ago, we decided to apply it to our stored files. First we consulted the Law Society rules governing storage of files on electronic media. The next step was for us to draw up a protocol for scanning the files. The benefits of the exercise have been significant. The area previously used for storage has been freed for other use. Files are now available online, instantaneously. When we have needed to send out files to the client or following a change of solicitor, we have been able to do so almost immediately, by E-mail, retaining a copy for our future reference. The files are protected from loss or deterioration, back-up copies having been taken which are stored off site. The complete stored file archive can be put in your pocket (in CD-ROM format) or on a laptop, facilitating remote working", "label": ["paperless office", "legal files", "electronic storage", "law society rules", "file scanning", "cd-rom", "file archive"], "stemmed_label": ["paperless offic", "legal file", "electron storag", "law societi rule", "file scan", "cd-rom", "file archiv"]}
{"doc": "eMarketing: restaurant Web sites that click A number of global companies have adopted electronic commerce as a means of reducing transaction related expenditures, connecting with current and potential customers, and enhancing revenues and profitability. If a restaurant is to have an Internet presence, what aspects of the business should be highlighted? Food service companies that have successfully ventured onto the web have employed assorted web-based technologies to create a powerful marketing tool of unparalleled strength. Historically, it has been difficult to create a set of criteria against which to evaluate website effectiveness. As practitioners consider additional resources for website development, the effectiveness of e-marketing investment becomes increasingly important. Care must be exercised to ensure that the quality of the site adheres to high standards and incorporates evolving technology, as appropriate. Developing a coherent website strategy, including an effective website design, are proving critical to an effective web presence", "label": ["e-marketing", "restaurant web sites", "electronic commerce", "internet presence", "food service companies", "revenues", "profitability"], "stemmed_label": ["e-market", "restaur web site", "electron commerc", "internet presenc", "food servic compani", "revenu", "profit"]}
{"doc": "Books on demand: just-in-time acquisitions The Purdue University Libraries Interlibrary Loan unit proposed a pilot project to purchase patrons' loan requests from Amazon. com, lend them to the patrons, and then add the titles to the collection. Staff analyzed previous monograph loans, developed ordering criteria, implemented the proposal as a pilot project for six months, and evaluated the resulting patron comments, statistics, and staff perceptions. As a result of enthusiastic patron comments and a review of the project statistics, the program was extended", "label": ["purdue university libraries interlibrary loan unit", "monograph loans", "ordering criteria", "staff perceptions", "patron comments", "publication on demand"], "stemmed_label": ["purdu univers librari interlibrari loan unit", "monograph loan", "order criteria", "staff percept", "patron comment", "public on demand"]}
{"doc": "On the use of neural network ensembles in QSAR and QSPR Despite their growing popularity among neural network practitioners, ensemble methods have not been widely adopted in structure-activity and structure-property correlation. Neural networks are inherently unstable, in that small changes in the training set and/or training parameters can lead to large changes in their generalization performance. Recent research has shown that by capitalizing on the diversity of the individual models, ensemble techniques can minimize uncertainty and produce more stable and accurate predictors. In this work, we present a critical assessment of the most common ensemble technique known as bootstrap aggregation, or bagging, as applied to QSAR and QSPR. Although aggregation does offer definitive advantages, we demonstrate that bagging may not be the best possible choice and that simpler techniques such as retraining with the full sample can often produce superior results. These findings are rationalized using Krogh and Vedelsby's (1995) decomposition of the generalization error into a term that measures the average generalization performance of the individual networks and a term that measures the diversity among them. For networks that are designed to resist over-fitting, the benefits of aggregation are clear but not overwhelming", "label": ["neural network ensembles", "qsar", "qspr", "training set", "training parameters", "generalization performance", "uncertainty", "bootstrap aggregation", "bagging", "retraining", "generalization error decomposition", "structure-activity correlation", "structure-property correlation"], "stemmed_label": ["neural network ensembl", "qsar", "qspr", "train set", "train paramet", "gener perform", "uncertainti", "bootstrap aggreg", "bag", "retrain", "gener error decomposit", "structure-act correl", "structure-properti correl"]}
{"doc": "A study on meaning processing of dialogue with an example of development of travel consultation system This paper describes an approach to processing meaning instead of processing information in computing. Human intellectual activity is supported by linguistic activities in the brain. Therefore, processing the meaning of language instead of processing information should allow us to realize human intelligence on a computer. As an example of the proposed framework for processing meaning, we build a travel consultation dialogue system which can understand utterance by a user and retrieve information through dialogue. Through a simulation example of the system, we show that both information processing and language processing are integrated", "label": ["meaning processing", "human intellectual activity", "linguistic activities", "travel consultation dialogue system", "user utterance understanding", "information retrieval", "information processing", "language processing"], "stemmed_label": ["mean process", "human intellectu activ", "linguist activ", "travel consult dialogu system", "user utter understand", "inform retriev", "inform process", "languag process"]}
{"doc": "Application of nonlinear time series analysis techniques to high-frequency currency exchange data In this work we have applied nonlinear time series analysis to high-frequency currency exchange data. The time series studied are the exchange rates between the US Dollar and 18 other foreign currencies from within and without the Euro zone. Our goal was to determine if their dynamical behaviours were in some way correlated. The nonexistence of stationarity called for the application of recurrence quantification analysis as a tool for this analysis, and is based on the definition of several parameters that allow for the quantification of recurrence plots. The method was checked using the European Monetary System currency exchanges. The results show, as expected, the high correlation between the currencies that are part of the Euro, but also a strong correlation between the Japanese Yen, the Canadian Dollar and the British Pound. Singularities of the series are also demonstrated taking into account historical events, in 1996, in the Euro zone", "label": ["nonlinear time series", "high-frequency currency exchange data", "exchange rates", "us dollar", "foreign currencies", "euro zone", "stationarity", "recurrence quantification analysis", "recurrence plots", "european monetary system", "japanese yen", "canadian dollar", "british pound", "historical events", "econophysics", "nonlinear dynamics"], "stemmed_label": ["nonlinear time seri", "high-frequ currenc exchang data", "exchang rate", "us dollar", "foreign currenc", "euro zone", "stationar", "recurr quantif analysi", "recurr plot", "european monetari system", "japanes yen", "canadian dollar", "british pound", "histor event", "econophys", "nonlinear dynam"]}
{"doc": "New tuning method for PID controller In this paper, a tuning method for proportional-integral-derivative (PID) controller and the performance assessment formulas for this method are proposed. This tuning method is based on a genetic algorithm based PID controller design method. For deriving the tuning formula, the genetic algorithm based design method is applied to design PID controllers for a variety of processes. The relationship between the controller parameters and the parameters that characterize the process dynamics are determined and the tuning formula is then derived. Using simulation studies, the rules for assessing the performance of a PID controller tuned by the proposed method are also given. This makes it possible to incorporate the capability to determine if the PID controller is well tuned or not into an autotuner. An autotuner based on this new tuning method and the corresponding performance assessment rules is also established. Simulations and real-time experimental results are given to demonstrate the effectiveness and usefulness of these formulas", "label": ["tuning method", "pid controller", "proportional-integral-derivative controller", "genetic algorithm", "controller design method", "process dynamics", "autotuner"], "stemmed_label": ["tune method", "pid control", "proportional-integral-deriv control", "genet algorithm", "control design method", "process dynam", "autotun"]}
{"doc": "Computer mediated communication and university international students The design for the preliminary study presented was based on the experiences of the international students and faculty members of a small southwest university being surveyed and interviewed. The data collection procedure blends qualitative and quantitative data. A strong consensus was found that supports the study's premise that there is an association between the use of computer mediated communication (CMC) and teaching and learning performance of international students. Both groups believe CMC to be an effective teaching and learning tool by: increasing the frequency and quality of communication between students and instructors; improving language skills through increased writing and communication opportunities; allowing students and instructors to stay current and to compete effectively; providing alternative teaching and learning methods to increase students' confidence in their ability to communicate effectively with peers and instructors; and improving the instructors' pedagogical focus and questioning techniques", "label": ["computer mediated communication", "university international students", "faculty members", "small southwest university", "data collection procedure", "quantitative data", "qualitative data", "cmc", "teaching", "learning performance", "language skills", "communication opportunities", "instructors", "student confidence", "peers", "pedagogical focus", "questioning techniques"], "stemmed_label": ["comput mediat commun", "univers intern student", "faculti member", "small southwest univers", "data collect procedur", "quantit data", "qualit data", "cmc", "teach", "learn perform", "languag skill", "commun opportun", "instructor", "student confid", "peer", "pedagog focu", "question techniqu"]}
{"doc": "Measuring keyboard response delays by comparing keyboard and joystick inputs The response characteristics of PC keyboards have to be identified when they are used as response devices in psychological experiments. In the past, the proposed method has been to check the characteristics independently by means of external measurement equipment. However, with the availability of different PC models and the rapid pace of model change, there is an urgent need for the development of convenient and accurate methods of checking. The method proposed consists of raising the precision of the PC's clock to the microsecond level and using a joystick connected to the MIDI terminal of a sound board to give the PC an independent timing function. Statistical processing of the data provided by this method makes it possible to estimate accurately the keyboard scanning interval time and the average keyboard delay time. The results showed that measured keyboard delay times varied from 11 to 73 msec, depending on the keyboard model, with most values being less than 30 msec", "label": ["keyboard response delay measurement", "joystick inputs", "keyboard inputs", "pc keyboards", "psychological experiments", "model change", "checking", "pc clock precision", "midi terminal", "sound board", "independent timing function", "statistical data processing", "keyboard scanning interval time", "average keyboard delay time"], "stemmed_label": ["keyboard respons delay measur", "joystick input", "keyboard input", "pc keyboard", "psycholog experi", "model chang", "check", "pc clock precis", "midi termin", "sound board", "independ time function", "statist data process", "keyboard scan interv time", "averag keyboard delay time"]}
{"doc": "Affine invariants of convex polygons In this correspondence, we prove that the affine invariants, for image registration and object recognition, proposed recently by Yang and Cohen (see ibid., vol.8, no.7, p.934-46, July 1999) are algebraically dependent. We show how to select an independent and complete set of the invariants. The use of this new set leads to a significant reduction of the computing complexity without decreasing the discrimination power", "label": ["affine invariants", "convex polygons", "algebraically dependent. invariants", "complexity reduction", "image registration", "object recognition", "convex quadruplet", "feature vector"], "stemmed_label": ["affin invari", "convex polygon", "algebra dependent. invari", "complex reduct", "imag registr", "object recognit", "convex quadruplet", "featur vector"]}
{"doc": "Improvements and critique on Sugeno's and Yasukawa's qualitative modeling Investigates Sugeno's and Yasukawa's (1993) qualitative fuzzy modeling approach. We propose some easily implementable solutions for the unclear details of the original paper, such as trapezoid approximation of membership functions, rule creation from sample data points, and selection of important variables. We further suggest an improved parameter identification algorithm to be applied instead of the original one. These details are crucial concerning the method's performance as it is shown in a comparative analysis and helps to improve the accuracy of the built-up model. Finally, we propose a possible further rule base reduction which can be applied successfully in certain cases. This improvement reduces the time requirement of the method by up to 16 in our experiments", "label": ["qualitative modeling", "fuzzy modeling", "trapezoid approximation", "membership functions", "rule creation", "parameter identification algorithm", "rule base reduction", "sugeno-yasukawa method"], "stemmed_label": ["qualit model", "fuzzi model", "trapezoid approxim", "membership function", "rule creation", "paramet identif algorithm", "rule base reduct", "sugeno-yasukawa method"]}
{"doc": "Genetic algorithm guided selection: variable selection and subset selection A novel genetic algorithm guided selection method, GAS, has been described. The method utilizes a simple encoding scheme which can represent both compounds and variables used to construct a QSAR/QSPR model. A genetic algorithm is then utilized to simultaneously optimize the encoded variables that include both descriptors and compound subsets. The GAS method generates multiple models each applying to a subset of the compounds. Typically the subsets represent clusters with different chemotypes. Also a procedure based on molecular similarity is presented to determine which model should be applied to a given test set compound. The variable selection method implemented in GAS has been tested and compared using the Selwood data set (n = 31 compounds; nu = 53 descriptors). The results showed that the method is comparable to other published methods. The subset selection method implemented in GAS has been first tested using an artificial data set (n = 100 points; nu = 1 descriptor) to examine its ability to subset data points and second applied to analyze the XLOGP data set (n = 1831 compounds; nu = 126 descriptors). The method is able to correctly identify artificial data points belonging to various subsets. The analysis of the XLOGP data set shows that the subset selection method can be useful in improving a QSAR/QSPR model when the variable selection method fails", "label": ["genetic algorithm guided selection method", "encoding scheme", "compounds", "variables", "variable selection", "subset selection", "qsar/qspr model", "optimization", "descriptors", "compound subsets", "multiple models", "clusters", "chemotypes", "molecular similarity", "selwood data set", "xlogp data set", "artificial data points"], "stemmed_label": ["genet algorithm guid select method", "encod scheme", "compound", "variabl", "variabl select", "subset select", "qsar/qspr model", "optim", "descriptor", "compound subset", "multipl model", "cluster", "chemotyp", "molecular similar", "selwood data set", "xlogp data set", "artifici data point"]}
{"doc": "An intelligent fuzzy decision system for a flexible manufacturing system with multi-decision points This paper describes an intelligent fuzzy decision support system for real-time scheduling and dispatching of parts in a flexible manufacturing system (FMS), with alternative routing possibilities for all parts. A fuzzy logic approach is developed to improve the system performance by considering multiple performance measures and at multiple decision points. The characteristics of the system status, instead of parts, are fed back to assign priority to the parts waiting to be processed. A simulation model is developed and it is shown that the proposed intelligent fuzzy decision support system keeps all performance measures at a good level. The proposed intelligent system is a promising tool for dealing with scheduling FMSs, in contrast to traditional rules", "label": ["flexible manufacturing system", "fms", "fuzzy logic", "multiple decision points", "intelligent decision support system", "real-time system", "scheduling", "simulation"], "stemmed_label": ["flexibl manufactur system", "fm", "fuzzi logic", "multipl decis point", "intellig decis support system", "real-tim system", "schedul", "simul"]}
{"doc": "Generalized predictive control for non-uniformly sampled systems In this paper, we study digital control systems with non-uniform updating and sampling patterns, which include multirate sampled-data systems as special cases. We derive lifted models in the state-space domain. The main obstacle for generalized predictive control (GPC) design using the lifted models is the so-called causality constraint. Taking into account this design constraint, we propose a new GPC algorithm, which results in optimal causal control laws for the non-uniformly sampled systems. The solution applies immediately to multirate sampled-data systems where rates are integer multiples of some base period", "label": ["generalized predictive control design", "nonuniformly sampled systems", "digital control systems", "nonuniform updating patterns", "nonuniform sampling patterns", "multirate sampled-data systems", "state-space models", "gpc", "causality constraint", "optimal causal control laws", "integer multiples"], "stemmed_label": ["gener predict control design", "nonuniformli sampl system", "digit control system", "nonuniform updat pattern", "nonuniform sampl pattern", "multir sampled-data system", "state-spac model", "gpc", "causal constraint", "optim causal control law", "integ multipl"]}
{"doc": "Optimization of the characteristics of computational processes in scalable resources The scalableness of resources is taken to mean the possibility of the prior change in the obtained dynamic characteristics of computational processes for a certain basic set of processors and the communication medium in an effort to optimize the dynamics of software applications. A method is put forward for the generation of optimal strategies-a set of the versions of the fulfillment of programs on the basis of a vector criterion. The method is urgent for the effective use of resources of computational clusters and metacomputational media and also for dynamic control of processes in real time on the basis of the static scaling", "label": ["computational processes", "scalable resources", "dynamic characteristics", "communication medium", "software applications", "optimal strategies", "vector criterion", "computational clusters", "metacomputational media", "dynamic control", "static scaling"], "stemmed_label": ["comput process", "scalabl resourc", "dynam characterist", "commun medium", "softwar applic", "optim strategi", "vector criterion", "comput cluster", "metacomput media", "dynam control", "static scale"]}
{"doc": "Recommendations for implementing Internet inquiry projects The purpose of the study presented was to provide recommendations to teachers who are interested in implementing Internet inquiry projects. Four classes of ninth- and tenth-grade honors students (N = 100) participated in an Internet inquiry project in which they were presented with an ecology question that required them to make a decision based on information that they gathered, analyzed, and synthesized from the Internet and their textbook. Students then composed papers with a rationale for their decision. Students in one group had access to pre-selected relevant Web sites, access to the entire Internet, and were provided with less online support. Students in the other group had access to only pre-selected relevant Web sites, but were provided with more online support. Two of the most important recommendations were: 1) to provide students with more online support; and 2) to provide students with pre-selected relevant Web sites and allow them to search the Internet for information", "label": ["internet inquiry projects", "teachers", "honors students", "ecology question", "pre-selected relevant web sites", "online support"], "stemmed_label": ["internet inquiri project", "teacher", "honor student", "ecolog question", "pre-select relev web site", "onlin support"]}
{"doc": "Building digital collections at the OAC: current strategies with a view to future uses Providing a context for the exploration of user defined virtual collections, the article describes the history and recent development of the Online Archive of California (OAC). Stating that usability and user needs are primary factors in digital resource development, issues explored include collaborations to build digital collections, reliance upon professional standards for description and encoding, system architecture, interface design, the need for user tools, and the role of archivists as interpreters in the digital environment", "label": ["digital collections", "oac", "future uses", "user defined virtual collections", "history", "online archive of california", "user needs", "digital resource", "professional standards", "system architecture", "interface design", "user tools", "digital environment", "encoded archival description", "archival descriptive standards", "metadata standards", "best practices", "user studies"], "stemmed_label": ["digit collect", "oac", "futur use", "user defin virtual collect", "histori", "onlin archiv of california", "user need", "digit resourc", "profession standard", "system architectur", "interfac design", "user tool", "digit environ", "encod archiv descript", "archiv descript standard", "metadata standard", "best practic", "user studi"]}
{"doc": "Choice from a three-element set: some lessons of the 2000 presidential campaign in the United States We consider the behavior of four choice rules - plurality voting, approval voting, Borda count, and self-consistent choice - when applied to choose the best option from a three-element set. It is assumed that the two main options are preferred by a large majority of the voters, while the third option gets a very small number of votes and influences the election outcome only when the two main options receive a close number of votes. When used to rate the main options, Borda count and self-consistent choice contain terms that allow both for the \"strength of preferences\" of the voters and the rating of the main candidates by voters who vote for the third option. In this way, it becomes possible to determine more reliably the winner when plurality voting or approval voting produce close results", "label": ["three-element set", "2000 presidential campaign", "plurality voting", "approval voting", "borda count", "self-consistent choice"], "stemmed_label": ["three-el set", "2000 presidenti campaign", "plural vote", "approv vote", "borda count", "self-consist choic"]}
{"doc": "Analyzing the benefits of 300 mm conveyor-based AMHS While the need for automation in 300 mm fabs is not debated, the form and performance of such automation is still in question. Software simulation that compares conveyor-based continuous flow transport technology to conventional car-based wafer-lot delivery has detailed delivery time and throughput advantages to the former", "label": ["software simulation", "car-based wafer-lot delivery", "conveyor-based continuous flow transport technology", "automated material handling system", "semiconductor fab", "throughput", "wafer processing", "delivery time", "300 mm"], "stemmed_label": ["softwar simul", "car-bas wafer-lot deliveri", "conveyor-bas continu flow transport technolog", "autom materi handl system", "semiconductor fab", "throughput", "wafer process", "deliveri time", "300 mm"]}
{"doc": "Standard protocol for exchange of health-checkup data based on SGML: the Health-checkup Data Markup Language (HDML) The objectives are to develop a health/medical data interchange model for efficient electronic exchange of data among health-checkup facilities. A Health-checkup Data Markup Language (HDML) was developed on the basis of the Standard Generalized Markup Language (SGML), and a feasibility study carried out, involving data exchange between two health checkup facilities. The structure of HDML is described. The transfer of numerical lab data, summary findings and health status assessment was successful. HDML is an improvement to laboratory data exchange. Further work has to address the exchange of qualitative and textual data", "label": ["health checkup data exchange", "sgml", "health-checkup data markup language", "data interchange model", "numerical lab data", "summary findings", "health status assessment"], "stemmed_label": ["health checkup data exchang", "sgml", "health-checkup data markup languag", "data interchang model", "numer lab data", "summari find", "health statu assess"]}
{"doc": "The archival imagination of David Bearman, revisited Many archivists regard the archival imagination evidenced in the writings of David Bearman as avant-garde. Archivist L. Henry (1998) has sharply criticized Bearman for being irreverent toward the archival theory and practice outlined by classical American archivist T. R. Schellenberg. Although Bearman is sometimes credited (and sometimes berated) for establishing \"a new paradigm\" centered on the archival management of electronic records, his methods and strategies are intended to encompass all forms of record keeping. The article provides general observations on Bearman's archival imagination, lists some of its components, and addresses elements of Henry's critique. Although the long lasting impact of Bearman's imagination upon the archival profession might be questioned, it nonetheless deserves continued consideration by archivists and inclusion as a component of graduate archival education", "label": ["archival imagination", "david bearman", "archival theory", "classical american archivist", "schellenberg", "archival management", "electronic records", "record keeping", "archival profession", "graduate archival education"], "stemmed_label": ["archiv imagin", "david bearman", "archiv theori", "classic american archivist", "schellenberg", "archiv manag", "electron record", "record keep", "archiv profess", "graduat archiv educ"]}
{"doc": "A spatial rainfall simulator for crop production modeling in Southern Africa This paper describes a methodology for simulating rainfall in dekads across a set of spatial units in areas where long-term meteorological records are available for a small number of sites only. The work forms part of a larger simulation model of the food system in a district of Zimbabwe, which includes a crop production component for yields of maize, small grains and groundnuts. Only a limited number of meteorological stations are available within or surrounding the district that have long time series of rainfall records. Preliminary analysis of rainfall data for these stations suggested that intra-seasonal temporal correlation was negligible, but that rainfall at any given station was correlated with rainfall at neighbouring stations. This spatial correlation structure can be modeled using a multivariate normal distribution consisting of 30 related variables, representing dekadly rainfall in each of the 30 wards. For each ward, log-transformed rainfall for each of the 36 dekads in the year was characterized by a mean and standard deviation, which were interpolated from surrounding meteorological stations. A covariance matrix derived from a distance measure was then used to represent the spatial correlation between wards. Sets of random numbers were then drawn from this distribution to simulate rainfall across the wards in any given dekad. Cross-validation of estimated rainfall parameters against observed parameters for the one meteorological station within the district suggests that the interpolation process works well. The methodology developed is useful in situations where long-term climatic records are scarce and where rainfall shows pronounced spatial correlation, but negligible temporal correlation", "label": ["simulating rainfall", "crop production modeling", "zimbabwe", "covariance matrix", "rainfall records", "rainfall data", "spatial correlation", "multivariate normal distribution", "parameter estimation", "southern africa"], "stemmed_label": ["simul rainfal", "crop product model", "zimbabw", "covari matrix", "rainfal record", "rainfal data", "spatial correl", "multivari normal distribut", "paramet estim", "southern africa"]}
{"doc": "The role of speech input in wearable computing Speech recognition seems like an attractive input mechanism for wearable computers, and as we saw in this magazine's first issue, several companies are promoting products that use limited speech interfaces for specific tasks. However, we must overcome several challenges to using speech recognition in more general contexts, and interface designers must be wary of applying the technology to situations where speech is inappropriate", "label": ["wearable computing", "speech input", "speech recognition", "wearable computer", "speech recognizers", "mobile speech recognition", "background noise", "speech interfaces"], "stemmed_label": ["wearabl comput", "speech input", "speech recognit", "wearabl comput", "speech recogn", "mobil speech recognit", "background nois", "speech interfac"]}
{"doc": "Emotion and self-control A biology-based model of choice is used to examine time-inconsistent preferences and the problem of self-control. Emotion is shown to be the biological substrate of choice, in that emotional systems assign value to 'goods' in the environment and also facilitate the learning of expectations regarding alternative options for acquiring those goods. A third major function of the emotional choice systems is motivation. Self-control is shown to be the result of a problem with the inhibition of the motive force of emotion, where this inhibition is necessary for higher level deliberation", "label": ["choice model", "inhibition", "learning", "time-inconsistent preferences", "self-control", "emotional choice systems", "emotion"], "stemmed_label": ["choic model", "inhibit", "learn", "time-inconsist prefer", "self-control", "emot choic system", "emot"]}
{"doc": "K-12 instruction and digital access to archival materials Providing K-12 schools with digital access to archival materials can strengthen both student learning and archival practice, although it cannot replace direct physical access to records. The article compares a variety of electronic and nonelectronic projects to promote teaching with primary source materials. The article also examines some of the different historiographical and pedagogical approaches used in archival Web sites geared for K-12 instruction, focusing on differences between the educational sites sponsored by the Library of Congress and the National Archives and Records Administration", "label": ["k-12 instruction", "digital access", "archival materials", "student learning", "archival practice", "electronic projects", "nonelectronic projects", "primary source materials", "direct physical access", "historiographical approaches", "pedagogical approaches", "archival web", "educational sites", "library of congress", "national archives and records administration"], "stemmed_label": ["k-12 instruct", "digit access", "archiv materi", "student learn", "archiv practic", "electron project", "nonelectron project", "primari sourc materi", "direct physic access", "historiograph approach", "pedagog approach", "archiv web", "educ site", "librari of congress", "nation archiv and record administr"]}
{"doc": "Self-organizing feature maps predicting sea levels In this paper, a new method for predicting sea levels employing self-organizing feature maps is introduced. For that purpose the maps are transformed from an unsupervised learning procedure to a supervised one. Two concepts, originally developed to solve the problems of convergence of other network types, are proposed to be applied to Kohonen networks: a functional relationship between the number of neurons and the number of learning examples and a criterion to break off learning. The latter one can be shown to be conform with the process of self-organization by using U-matrices for visualization of the learning procedure. The predictions made using these neural models are compared for accuracy with observations and with the prognoses prepared using six models: two hydrodynamic models, a statistical model, a nearest neighbor model, the persistence model, and the verbal forecasts that are broadcast and kept on record by the Sea Level Forecast Service of the Federal Maritime and Hydrography Agency (BSH) in Hamburg. Before training the maps, the meteorological and oceanographic situation has to be condensed as well as possible, and the weight and learning vectors have to be made as small as possible. The self-organizing feature maps predict sea levels better than all six models of comparison", "label": ["self-organizing feature maps", "sea level prediction", "supervised learning", "kohonen networks", "neurons", "u-matrices", "visualization", "hydrodynamic models", "statistical model", "nearest neighbor model", "persistence model", "verbal forecasts", "sea level forecast service", "federal maritime and hydrography agency", "oceanographic situation", "meteorological situation", "learning vectors"], "stemmed_label": ["self-organ featur map", "sea level predict", "supervis learn", "kohonen network", "neuron", "u-matric", "visual", "hydrodynam model", "statist model", "nearest neighbor model", "persist model", "verbal forecast", "sea level forecast servic", "feder maritim and hydrographi agenc", "oceanograph situat", "meteorolog situat", "learn vector"]}
{"doc": "Theoretical and experimental investigations on coherence of traffic noise transmission through an open window into a rectangular room in high-rise buildings A method for theoretically calculating the coherence between sound pressure inside a rectangular room in a high-rise building and that outside the open window of the room is proposed. The traffic noise transmitted into a room is generally dominated by low-frequency components, to which active noise control (ANC) technology may find an application. However, good coherence between reference and error signals is essential for an effective noise reduction and should be checked first. Based on traffic noise prediction methods, wave theory, and mode coupling theory, the results of this paper enabled one to determine the potentials and limitations of ANC used to reduce such a transmission. Experimental coherence results are shown for two similar, empty rectangular rooms located on the 17th and 30th floors of a 34 floor high-rise building. The calculated results with the proposed method are generally in good agreement with the experimental results and demonstrate the usefulness of the method for predicting the coherence", "label": ["traffic noise transmission", "open window", "rectangular room", "high-rise buildings", "sound pressure", "low-frequency components", "active noise control technology", "traffic noise prediction methods", "mode coupling theory", "wave theory"], "stemmed_label": ["traffic nois transmiss", "open window", "rectangular room", "high-ris build", "sound pressur", "low-frequ compon", "activ nois control technolog", "traffic nois predict method", "mode coupl theori", "wave theori"]}
{"doc": "A method of determining a sequence of the best solutions to the problems of optimization on finite sets and the problem of network reconstruction A method of determining a sequence of the best solutions to the problems of optimization on finite sets was proposed. Its complexity was estimated by a polynomial of the dimension of problem input, given number of sequence terms, and complexity of completing the design of the original extremal problem. The technique developed was applied to the typical problem of network reconstruction with the aim of increasing its throughput under restricted reconstruction costs", "label": ["best solutions", "optimization", "finite sets", "network reconstruction", "complexity"], "stemmed_label": ["best solut", "optim", "finit set", "network reconstruct", "complex"]}
{"doc": "Library services today and tomorrow: lessons from iLumina, a digital library for creating and sharing teaching resources This article is based on the emerging experience associated with a digital library of instructional resources, iLumina, in which the contributors of resources and the users of those resources are the same-an open community of instructors in science, mathematics, engineering, and technology. Moreover, it is not the resources, most of which will be distributed across the Internet, but metadata about the resources that is the focus of the central iLumina repository and its support services for resource contributors and users. The distributed iLumina library is a community-sharing library for repurposing and adding value to potentially useful, mostly non-commercial instructional resources that are typically more granular in nature than commercially developed course materials. The experience of developing iLumina is raising a range of issues that have nothing to do with the place and time characteristics of the instructional context in which iLumina instructional resources are created or used. The issues instead have their locus in the democratization of both the professional roles of librarians and the quality assurance mechanisms associated with traditional peer review", "label": ["ilumina", "digital library", "teaching resource sharing", "internet", "metadata", "information resources", "community-sharing library", "professional roles", "academic library", "librarians", "quality assurance", "peer review", "library automation", "standards", "interoperability", "reusable software", "distributed systems", "user issues"], "stemmed_label": ["ilumina", "digit librari", "teach resourc share", "internet", "metadata", "inform resourc", "community-shar librari", "profession role", "academ librari", "librarian", "qualiti assur", "peer review", "librari autom", "standard", "interoper", "reusabl softwar", "distribut system", "user issu"]}
{"doc": "Layer-based machining: recent development and support structure design There is growing interest in additive and subtractive shaping theories that are synthesized to integrate the layered manufacturing process and material removal process. Layer-based machining has emerged as a promising method for integrated additive and subtractive shaping theory. In the paper, major layer-based machining systems are reviewed and compared according to characteristics of stock layers, numerical control machining configurations, stacking operations, input format and raw materials. Support structure, a major issue in machining-based systems which has seldom been addressed in previous research, is investigated in the paper with considerations of four situations: floating overhang, cantilever, vaulted overhang and ceiling. Except for the floating overhang where a support structure should not be overlooked, the necessity for support structures for the other three situations is determined by stress and deflection analysis. This is demonstrated by the machining of a large castle model", "label": ["layer-based machining", "support structure design", "additive shaping theories", "subtractive shaping theories", "layered manufacturing process", "material removal process", "stock layers", "numerical control machining configurations", "stacking operations", "input format", "raw materials", "floating overhang", "cantilever", "vaulted overhang", "ceiling", "stress", "deflection analysis"], "stemmed_label": ["layer-bas machin", "support structur design", "addit shape theori", "subtract shape theori", "layer manufactur process", "materi remov process", "stock layer", "numer control machin configur", "stack oper", "input format", "raw materi", "float overhang", "cantilev", "vault overhang", "ceil", "stress", "deflect analysi"]}
{"doc": "An efficient parallel algorithm for the calculation of canonical MP2 energies We present the parallel version of a previous serial algorithm for the efficient calculation of canonical MP2 energies. It is based on the Saebo-Almlof direct-integral transformation, coupled with an efficient prescreening of the AO integrals. The parallel algorithm avoids synchronization delays by spawning a second set of slaves during the bin-sort prior to the second half-transformation. Results are presented for systems with up to 2000 basis functions. MP2 energies for molecules with 400-500 basis functions can be routinely calculated to microhartree accuracy on a small number of processors (6-8) in a matter of minutes with modern PC-based parallel computers", "label": ["parallel algorithm", "canonical mp2 energies", "saebo-almlof direct-integral transformation", "ao integrals", "synchronization delays", "second half-transformation", "basis functions", "mp2 energies", "microhartree accuracy", "pc-based parallel computers"], "stemmed_label": ["parallel algorithm", "canon mp2 energi", "saebo-almlof direct-integr transform", "ao integr", "synchron delay", "second half-transform", "basi function", "mp2 energi", "microhartre accuraci", "pc-base parallel comput"]}
{"doc": "Selecting rail grade crossing investments with a decision support system The Federal Railroad Administration (FRA) has developed a series of rail and rail-related analysis tools that assist FRA officials, Metropolitan Planning Organizations (MPOs), state Department of Transportation (DOT), and other constituents in evaluating the cost and benefits of potential infrastructure projects. To meet agency objectives, the FRA wants to add a high-speed rail grade crossing analysis tool to its package of rail and rail-related intermodal software products. This paper presents a conceptual decision support system (DSS) that can assist officials in achieving this goal. The paper first introduces the FRA's objectives and the role of cost benefit analysis in achieving these objectives. Next, there is a discussion of the models needed to assess the feasibility of proposed high-speed rail grade crossing investments and the presentation of a decision support system (DSS) that can deliver these models transparently to users. Then, the paper illustrates a system session and examines the potential benefits from system use", "label": ["rail grade crossing investment selection", "decision support system", "federal railroad administration", "metropolitan planning organizations", "department of transportation", "infrastructure projects", "high-speed rail grade crossing analysis tool", "rail-related intermodal software products", "rail intermodal software products", "cost benefit analysis"], "stemmed_label": ["rail grade cross invest select", "decis support system", "feder railroad administr", "metropolitan plan organ", "depart of transport", "infrastructur project", "high-spe rail grade cross analysi tool", "rail-rel intermod softwar product", "rail intermod softwar product", "cost benefit analysi"]}
{"doc": "Strategies for high throughput, templated zeolite synthesis The design and redesign of high throughput experiments for zeolite synthesis are addressed. A model that relates materials function to the chemical composition of the zeolite and the structure directing agent is introduced. Using this model, several Monte Carlo-like design protocols are evaluated. Multi-round protocols are bound to be effective, and strategies that use a priori information about the structure-directing libraries are found to be the best", "label": ["templated zeolite synthesis", "high throughput strategies", "materials function", "chemical composition", "structure directing agent", "monte carlo-like design protocols", "multi-round protocols", "a priori information", "catalytic activity", "catalytic selectivity", "organo-cation template molecules", "combinatorial methods", "random energy model", "figure of merit", "material discovery", "small molecule design", "voronoi diagram", "phase-dependent random gaussian variables", "metropolis-type method", "ligand libraries", "reflecting boundary conditions"], "stemmed_label": ["templat zeolit synthesi", "high throughput strategi", "materi function", "chemic composit", "structur direct agent", "mont carlo-lik design protocol", "multi-round protocol", "a priori inform", "catalyt activ", "catalyt select", "organo-c templat molecul", "combinatori method", "random energi model", "figur of merit", "materi discoveri", "small molecul design", "voronoi diagram", "phase-depend random gaussian variabl", "metropolis-typ method", "ligand librari", "reflect boundari condit"]}
{"doc": "Data management in location-dependent information services Location-dependent information services have great promise for mobile and pervasive computing environments. They can provide local and nonlocal news, weather, and traffic reports as well as directory services. Before they can be implemented on a large scale, however, several research issues must be addressed", "label": ["location-dependent information services", "wireless networks", "pervasive computing", "mobile computing", "news", "weather", "traffic reports", "data management", "directory services"], "stemmed_label": ["location-depend inform servic", "wireless network", "pervas comput", "mobil comput", "news", "weather", "traffic report", "data manag", "directori servic"]}
{"doc": "Information architecture for bilingual Web sites Creating an information architecture for a bilingual Web site presents particular challenges beyond those that exist for single and multilanguage sites. This article reports work in progress on the development of a content-based bilingual Web site to facilitate the sharing of resources and information between Speech and Language Therapists. The development of the information architecture is based on a combination of two aspects: an abstract structural analysis of existing bilingual Web designs focusing on the presentation of bilingual material, and a bilingual card-sorting activity conducted with potential users. Issues for bilingual developments are discussed, and some observations are made regarding the use of card-sorting activities", "label": ["information architecture", "content-based bilingual web site", "speech therapists", "language therapists", "bilingual card-sorting activity", "bilingual developments", "world wide web"], "stemmed_label": ["inform architectur", "content-bas bilingu web site", "speech therapist", "languag therapist", "bilingu card-sort activ", "bilingu develop", "world wide web"]}
{"doc": "Block truncation image bit plane coding Block truncation coding (BTC) is a successful image compression technique due to its simple and fast computational burden. The bit rate is fixed to 2.0 bits/pixel, whose performance is moderate in terms of compression ratio compared to other compression schemes such as discrete cosine transform (DCT), vector quantization (VQ), wavelet transform coding (WTC), etc. Two kinds of overheads are required for BTC coding: bit plane and quantization values, respectively. A new technique is presented to reduce the bit plane overhead. Conventional bit plane overhead is 1.0 bits/pixel; we decrease it to 0.734 bits/pixel while maintaining the same decoded quality as absolute moment BTC (AMBTC) does for the \"Lena\" image. Compared to other published bit plane coding strategies, the proposed method outperforms all of the existing methods", "label": ["image bit plane coding", "block truncation coding", "image compression technique", "bit rate", "performance", "compression ratio", "bit plane overhead", "decoded quality", "absolute moment btc", "ambtc", "quantization values", "lena image"], "stemmed_label": ["imag bit plane code", "block truncat code", "imag compress techniqu", "bit rate", "perform", "compress ratio", "bit plane overhead", "decod qualiti", "absolut moment btc", "ambtc", "quantiz valu", "lena imag"]}
{"doc": "A conceptual framework for evaluation of information technology investments The decision to acquire a new information technology poses a number of serious evaluation and selection problems to technology managers, because the new system must not only meet current information requirements of the organisation, but also the needs for future expansion. Tangible and intangible benefits factors, as well as risks factors, must be identified and evaluated. The paper provides a review of ten major evaluation categories and available models, which fall under each category, showing their advantages and disadvantages in handling the above difficulties. This paper describes strategic implications involved in the selection decision, and the inherent difficulties in: (1) choosing or developing a model, (2) obtaining realistic inputs for the model, and (3) making tradeoffs among the conflicting factors. It proposes a conceptual framework to help the decision maker in choosing the most appropriate methodology in the evaluation process. It also offers a new model, called GAHP, for the evaluation problem combining integer goal linear programming and analytic hierarchy process (AHP) in a single hybrid multiple objective multi-criteria model. A goal programming methodology, with zero-one integer variables and mixed integer constraints, is used to set goal target values against which information technology alternatives are evaluated and selected. AHP is used to structure the evaluation process providing pairwise comparison mechanisms to quantify subjective, nonmonetary, intangible benefits and risks factors, in deriving data for the model. A case illustration is provided showing how GAHP can be formulated and solved", "label": ["information technology investments", "technology managers", "information requirements", "risks factors", "evaluation categories", "selection decision", "tradeoffs", "decision maker", "analytic hierarchy process", "hybrid multiple objective multi-criteria model", "goal programming methodology", "zero-one integer variables", "mixed integer constraints", "goal target values", "information technology alternatives", "pairwise comparison mechanisms", "nonmonetary benefits", "intangible benefits", "group decision process"], "stemmed_label": ["inform technolog invest", "technolog manag", "inform requir", "risk factor", "evalu categori", "select decis", "tradeoff", "decis maker", "analyt hierarchi process", "hybrid multipl object multi-criteria model", "goal program methodolog", "zero-on integ variabl", "mix integ constraint", "goal target valu", "inform technolog altern", "pairwis comparison mechan", "nonmonetari benefit", "intang benefit", "group decis process"]}
{"doc": "Recording quantum properties of light in a long-lived atomic spin state: towards quantum memory We report an experiment on mapping a quantum state of light onto the ground state spin of an ensemble of Cs atoms with the lifetime of 2 ms. Recording of one of the two quadrature phase operators of light is demonstrated with vacuum and squeezed states of light. The sensitivity of the mapping procedure at the level of approximately 1 photon/sec per Hz is shown. The results pave the road towards complete (storing both quadrature phase observables) quantum memory for Gaussian states of light. The experiment also sheds new light on fundamental limits of sensitivity of the magneto-optical resonance method", "label": ["light quantum properties recording", "long-lived atomic spin state", "quantum memory", "ground state spin", "ensemble", "two quadrature phase operators", "vacuum states", "squeezed states", "mapping procedure", "magnetooptical resonance method", "2 ms", "cs"], "stemmed_label": ["light quantum properti record", "long-liv atom spin state", "quantum memori", "ground state spin", "ensembl", "two quadratur phase oper", "vacuum state", "squeez state", "map procedur", "magnetoopt reson method", "2 ms", "cs"]}
{"doc": "Absorption of long waves by nonresonant parametric microstructures Using simple acoustical and mechanical models, we consider the conceptual possibility of designing an active absorbing (nonreflecting) coating in the form of a thin layer with small-scale stratification and fast time modulation of parameters. Algorithms for space-time modulation of the controlled-layer structure are studied in detail for a one-dimensional boundary-value problem. These algorithms do not require wave-field measurements, which eliminates the self-excitation problem that is characteristic of active systems. The majority of the considered algorithms of parametric control transform the low-frequency incident wave to high-frequency waves of the technological band for which the waveguiding medium inside the layer is assumed to be opaque (absorbing). The efficient use conditions are found for all the algorithms. It is shown that the absorbing layer can be as thin as desired with respect to the minimum spatial scale of the incident wave and ensures efficient absorption in a wide frequency interval (starting from zero frequency) that is bounded from above only by a finite space-time resolution of the parameter-control operations. The structure of a three-dimensional parametric \"'black\" coating whose efficiency is independent of the angle of incidence of an incoming wave is developed on the basis of the studied one-dimensional problems. The general solution of the problem of diffraction of incident waves from such a coating is obtained. This solution is analyzed in detail for the case of a disk-shaped element", "label": ["acoustical models", "mechanical models", "active absorbing coating", "nonreflecting coating", "thin layer", "small-scale stratification", "fast time modulation", "space-time modulation", "controlled-layer structure", "one-dimensional boundary-value problem", "parametric control", "low-frequency incident wave", "high-frequency waves", "waveguiding medium", "absorbing layer", "angle of incidence", "one-dimensional problems", "diffraction", "disk-shaped element"], "stemmed_label": ["acoust model", "mechan model", "activ absorb coat", "nonreflect coat", "thin layer", "small-scal stratif", "fast time modul", "space-tim modul", "controlled-lay structur", "one-dimension boundary-valu problem", "parametr control", "low-frequ incid wave", "high-frequ wave", "waveguid medium", "absorb layer", "angl of incid", "one-dimension problem", "diffract", "disk-shap element"]}
{"doc": "More constructions for Boolean algebras We construct Boolean algebras with prescribed behaviour concerning depth for the free product of two Boolean algebras over a third, in ZFC using pcf; assuming squares we get results on ultraproducts. We also deal with the family of cardinalities and topological density of homomorphic images of Boolean algebras (you can translate it to topology-on the cardinalities of closed subspaces); and lastly we deal with inequalities between cardinal invariants, mainly d(B)/sup kappa / |B| implies ind(B) /sup kappa /V Depth(B) or=log(|B|)", "label": ["boolean algebras", "prescribed behaviour", "free product", "zfc", "ultraproducts", "homomorphic images", "cardinal invariants"], "stemmed_label": ["boolean algebra", "prescrib behaviour", "free product", "zfc", "ultraproduct", "homomorph imag", "cardin invari"]}
{"doc": "A generalized PERT/CPM implementation in a spreadsheet This paper describes the implementation of the traditional PERT/CPM algorithm for finding the critical path in a project network in a spreadsheet. The problem is of importance due to the recent shift of attention to using the spreadsheet environment as a vehicle for delivering management science/operations research (MS/OR) techniques to end-users", "label": ["generalized pert/cpm implementation", "spreadsheet", "critical path", "ms/or techniques"], "stemmed_label": ["gener pert/cpm implement", "spreadsheet", "critic path", "ms/or techniqu"]}
{"doc": "Uncertainty bounds and their use in the design of interval type-2 fuzzy logic systems We derive inner- and outer-bound sets for the type-reduced set of an interval type-2 fuzzy logic system (FLS), based on a new mathematical interpretation of the Karnik-Mendel iterative procedure for computing the type-reduced set. The bound sets can not only provide estimates about the uncertainty contained in the output of an interval type-2 FLS, but can also be used to design an interval type-2 FLS. We demonstrate, by means of a simulation experiment, that the resulting system can operate without type-reduction and can achieve similar performance to one that uses type-reduction. Therefore, our new design method, based on the bound sets, can relieve the computation burden of an interval type-2 FLS during its operation, which makes an interval type-2 FLS useful for real-time applications", "label": ["uncertainty bounds", "interval type-2 fuzzy logic systems", "inner-bound sets", "outer-bound sets", "type-reduced set", "karnik-mendel iterative procedure", "real-time applications", "time-series forecasting"], "stemmed_label": ["uncertainti bound", "interv type-2 fuzzi logic system", "inner-bound set", "outer-bound set", "type-reduc set", "karnik-mendel iter procedur", "real-tim applic", "time-seri forecast"]}
{"doc": "Color plane interpolation using alternating projections Most commercial digital cameras use color filter arrays to sample red, green, and blue colors according to a specific pattern. At the location of each pixel only one color sample is taken, and the values of the other colors must be interpolated using neighboring samples. This color plane interpolation is known as demosaicing; it is one of the important tasks in a digital camera pipeline. If demosaicing is not performed appropriately, images suffer from highly visible color artifacts. In this paper we present a new demosaicing technique that uses inter-channel correlation effectively in an alternating-projections scheme. We have compared this technique with six state-of-the-art demosaicing techniques, and it outperforms all of them, both visually and in terms of mean square error", "label": ["color plane interpolation", "alternating projections", "digital cameras", "demosaicing", "color filter arrays", "color artifacts", "inter-channel correlation"], "stemmed_label": ["color plane interpol", "altern project", "digit camera", "demosa", "color filter array", "color artifact", "inter-channel correl"]}
{"doc": "Motion estimation using modified dynamic programming A new method for computing precise estimates of the motion vector field of moving objects in a sequence of images is proposed. Correspondence vector-field computation is formulated as a matching optimization problem for multiple dynamic images. The proposed method is a heuristic modification of dynamic programming applied to the 2-D optimization problem. Motion-vector-field estimates using real movie images demonstrate good performance of the algorithm in terms of dynamic motion analysis", "label": ["modified dynamic programming", "motion estimation", "precise estimates", "motion vector field", "moving objects", "image sequence", "vector-field computation", "matching optimization problem", "multiple dynamic images", "heuristic modification", "dynamic programming", "2-d optimization problem", "motion vector field estimates", "real movie images", "algorithm", "dynamic motion analysis"], "stemmed_label": ["modifi dynam program", "motion estim", "precis estim", "motion vector field", "move object", "imag sequenc", "vector-field comput", "match optim problem", "multipl dynam imag", "heurist modif", "dynam program", "2-d optim problem", "motion vector field estim", "real movi imag", "algorithm", "dynam motion analysi"]}
{"doc": "MEMS applications in computer disk drive dual-stage servo systems We present a decoupled discrete time pole placement design method, which can be combined with a self-tuning scheme to compensate variations in the microactuator's (MA's) resonance mode. Section I of the paper describes the design and fabrication of a prototype microactuator with an integrated gimbal structure. Section II presents a decoupled track-following controller design and a self-tuning control scheme to compensate for the MA's resonance mode variations", "label": ["computer disk drive dual-stage servo systems", "mems", "microactuator", "servo control", "hard disk drives", "decoupled discrete time pole placement design method", "self-tuning scheme", "electrostatic design", "fabrication process", "track-following controller design"], "stemmed_label": ["comput disk drive dual-stag servo system", "mem", "microactu", "servo control", "hard disk drive", "decoupl discret time pole placement design method", "self-tun scheme", "electrostat design", "fabric process", "track-follow control design"]}
{"doc": "Multiple model adaptive estimation with filter spawning Multiple model adaptive estimation (MMAE) with filter spawning is used to detect and estimate partial actuator failures on the VISTA F-16. The truth model is a full six-degree-of-freedom simulation provided by Calspan and General Dynamics. The design models are chosen as 13-state linearized models, including first order actuator models. Actuator failures are incorporated into the truth model and design model assuming a \"failure to free stream.\" Filter spawning is used to include additional filters with partial actuator failure hypotheses into the MMAE bank. The spawned filters are based on varying degrees of partial failures (in terms of effectiveness) associated with the complete-actuaton-failure hypothesis with the highest conditional probability of correctness at the current time. Thus, a blended estimate of the failure effectiveness is found using the filters' estimates based upon a no-failure hypothesis, a complete actuator failure hypothesis, and the spawned filters' partial-failure hypotheses. This yields substantial precision in effectiveness estimation, compared with what is possible without spawning additional filters, making partial failure adaptation a viable methodology", "label": ["multiple model adaptive estimation", "filter spawning", "partial actuator failures", "vista f-16", "truth model", "six-degree-of-freedom simulation", "calspan", "in-flight simulator", "test aircraft", "flight control systems", "general dynamics", "linearized models", "mmae", "partial failures", "conditional probability", "no-failure hypothesis"], "stemmed_label": ["multipl model adapt estim", "filter spawn", "partial actuat failur", "vista f-16", "truth model", "six-degree-of-freedom simul", "calspan", "in-flight simul", "test aircraft", "flight control system", "gener dynam", "linear model", "mmae", "partial failur", "condit probabl", "no-failur hypothesi"]}
{"doc": "The development of a mobile manipulator imaging system for bridge crack inspection A mobile manipulator imaging system is developed for the automation of bridge crack inspection. During bridge safety inspections, an eyesight inspection is made for preliminary evaluation and screening before a more precise inspection. The inspection for cracks is an important part of the preliminary evaluation. Currently, the inspectors must stand on the platform of a bridge inspection vehicle or a temporarily erected scaffolding to examine the underside of a bridge. However, such a procedure is risky. To help automate the bridge crack inspection process, we installed two CCD cameras and a four-axis manipulator system on a mobile vehicle. The parallel cameras are used to detect cracks. The manipulator system is equipped with binocular charge coupled devices (CCD) for examining structures that may not be accessible to the eye. The system also reduces the danger of accidents to the human inspectors. The manipulator system consists of four arms. Balance weights are placed at the ends of arms 2 and 4, respectively, to maintain the center of gravity during operation. Mechanically, arms 2 and 4 can revolve smoothly. Experiments indicated that the system could be useful for bridge crack inspections", "label": ["mobile manipulator", "imaging system", "bridge crack inspection", "automation", "eyesight inspection", "ccd cameras", "four-axis manipulator", "parallel cameras", "binocular ccd", "charge coupled devices"], "stemmed_label": ["mobil manipul", "imag system", "bridg crack inspect", "autom", "eyesight inspect", "ccd camera", "four-axi manipul", "parallel camera", "binocular ccd", "charg coupl devic"]}
{"doc": "Spam solution? The author describes a solution to spam E-mails: disposable E-mail addresses (DEA). Mailshell's free trial Web-based E-mail service allows you, if you start getting spammed on that DEA, just to delete the DEA in Mailshell, and all E-mail thereafter sent to that address will automatically be junked (though you can later restore that address if you want). Mailshell allows any number of DEA", "label": ["spam e-mails", "disposable e-mail addresses", "mailshell", "web-based e-mail"], "stemmed_label": ["spam e-mail", "dispos e-mail address", "mailshel", "web-bas e-mail"]}
{"doc": "An efficient retrieval selection algorithm for video servers with random duplicated assignment storage technique Random duplicated assignment (RDA) is an approach in which video data is stored by assigning a number of copies of each data block to different, randomly chosen disks. It has been shown that this approach results in smaller response times and lower disk and RAM costs compared to the well-known disk stripping techniques. Based on this storage approach, one has to determine, for each given batch of data blocks, from which disk each of the data blocks is to be retrieved. This is to be done in such a way that the maximum load of the disks is minimized. The problem is called the retrieval selection problem (RSP). In this paper, we propose a new efficient algorithm for RSP. This algorithm is based on the breadth-first search approach and is able to guarantee optimal solutions for RSP in O(n/sup 2/+mn), where m and n correspond to the number of data blocks and the number of disks, respectively. We show that our proposed algorithm has a lower time complexity than an existing algorithm, called the MFS algorithm", "label": ["efficient retrieval selection algorithm", "video servers", "random duplicated assignment storage technique", "copies", "data block", "randomly chosen disks", "response times", "ram costs", "disk costs", "maximum load", "breadth-first search", "optimal solutions", "time complexity"], "stemmed_label": ["effici retriev select algorithm", "video server", "random duplic assign storag techniqu", "copi", "data block", "randomli chosen disk", "respons time", "ram cost", "disk cost", "maximum load", "breadth-first search", "optim solut", "time complex"]}
{"doc": "Building 3D anatomical scenes on the Web We propose a new service for building user-defined 3D anatomical structures on the Web. The Web server is connected to a database storing more than 1000 3D anatomical models reconstructed from the Visible Human. Users may combine existing models as well as planar oblique slices in order to create their own structured anatomical scenes. Furthermore, they may record sequences of scene construction and visualization actions. These actions enable the server to construct high-quality video animations, downloadable by the user. Professionals and students in anatomy, medicine and related disciplines are invited to use the server and create their own anatomical scenes", "label": ["3d anatomical scenes", "world wide web", "user-defined 3d anatomical structures", "web server", "database", "3d anatomical models", "visible human", "planar oblique slices", "structured anatomical scenes", "volume visualization", "surface reconstruction", "applet-based rendering engine", "java", "visualization", "scene construction", "high-quality video animation"], "stemmed_label": ["3d anatom scene", "world wide web", "user-defin 3d anatom structur", "web server", "databas", "3d anatom model", "visibl human", "planar obliqu slice", "structur anatom scene", "volum visual", "surfac reconstruct", "applet-bas render engin", "java", "visual", "scene construct", "high-qual video anim"]}
{"doc": "2002 in-house fulfillment systems report publishing CM's 13th annual survey of in-house fulfillment system suppliers brings you up to date on the current capabilities of the leading publication software packages", "label": ["survey", "in-house fulfillment system", "suppliers", "publication software packages"], "stemmed_label": ["survey", "in-hous fulfil system", "supplier", "public softwar packag"]}
{"doc": "Optimal online algorithm for scheduling on two identical machines with machine availability constraints This paper considers the online scheduling on two identical machines with machine availability constraints for minimizing makespan. We assume that machine M/sub j/ is unavailable during period from s/sub j/ to t/sub j/ (0 or= s/sub j/ t/sub j/), j = 1, 2, and the unavailable periods of two machines do not overlap. We show that the competitive ratio of list scheduling is 3. We further give an optimal algorithm with a competitive ratio 5/2", "label": ["optimal online algorithm", "makespan minimisation", "list scheduling", "identical machines scheduling", "machine availability constraints"], "stemmed_label": ["optim onlin algorithm", "makespan minimis", "list schedul", "ident machin schedul", "machin avail constraint"]}
{"doc": "The two populations' cellular automata model with predation based on the Penna model In Penna's (1995) single-species asexual bit-string model of biological ageing, the Verhulst factor has too strong a restraining effect on the development of the population. Danuta Makowiec gave an improved model based on the lattice, where the restraining factor of the four neighbours take the place of the Verhulst factor. Here, we discuss the two populations' Penna model with predation on the planar lattice of two dimensions. A cellular automata model containing movable wolves and sheep has been built. The results show that both the quantity of the wolves and the sheep fluctuate in accordance with the law that one quantity increases while the other one decreases", "label": ["cellular automata model", "population", "penna model", "single-species asexual bit-string model", "biological ageing", "verhulst factor", "restraining effect", "lattice", "wolves", "sheep", "fluctuation", "lotka-volterra model", "predation"], "stemmed_label": ["cellular automata model", "popul", "penna model", "single-speci asexu bit-str model", "biolog age", "verhulst factor", "restrain effect", "lattic", "wolv", "sheep", "fluctuat", "lotka-volterra model", "predat"]}
{"doc": "The acquisition of out-of-print music Non-specialist librarians are alerted to factors important in the successful acquisition of out-of-print music, both scholarly editions and performance editions. The appropriate technical music vocabulary, the music publishing industry, specialized publishers and vendors, and methods of acquisition of out-of-print printed music are introduced, and the need for familiarity with them is emphasized", "label": ["out-of-print music", "scholarly editions", "performance editions", "technical music vocabulary", "music publishing industry", "specialized publishers", "specialized vendors", "out-of-print printed music"], "stemmed_label": ["out-of-print music", "scholarli edit", "perform edit", "technic music vocabulari", "music publish industri", "special publish", "special vendor", "out-of-print print music"]}
{"doc": "Putting pen to screen on Tablet PCs With the release of the first Tablet PCs produced to Microsoft Corp.'s general specifications, handheld computers may be about to leap into the ring with today's laptops. They will be about the size of the smaller laptops, will be at least as powerful, and maybe their biggest selling point-will be able to handle handwritten text. The Tablet PCs will be amply configured, general-purpose machines with more than enough power to run the full-blown Windows XP operating system. In particular, they will allow handwritten text to be entered onto a digitizing tablet and recognized, a functionality that's called pen-based computing. The Tablet PC will far outpace the computing power of existing small devices such as PDAs (personal digital assistants), including those variants based on Microsoft's own Pocket PC operating system", "label": ["tablet pc", "microsoft", "handheld computers", "handwritten text", "windows xp operating system", "digitizing tablet", "pen-based computing"], "stemmed_label": ["tablet pc", "microsoft", "handheld comput", "handwritten text", "window xp oper system", "digit tablet", "pen-bas comput"]}
{"doc": "ISCSI poised to lower SAN costs IT managers building storage area networks or expanding their capacity may be able to save money by using iSCSI and IP systems rather than Fibre Channel technologies", "label": ["san costs", "storage area networks", "iscsi", "ip systems"], "stemmed_label": ["san cost", "storag area network", "iscsi", "ip system"]}
{"doc": "Design PID controllers for desired time-domain or frequency-domain response Practical requirements on the design of control systems, especially process control systems, are usually specified in terms of time-domain response, such as overshoot and rise time, or frequency-domain response, such as resonance peak and stability margin. Although numerous methods have been developed for the design of the proportional-integral-derivative (PID) controller, little work has been done in relation to the quantitative time-domain and frequency-domain responses. In this paper, we study the following problem: Given a nominal stable process with time delay, we design a suboptimal PID controller to achieve the required time-domain response or frequency-domain response for the nominal system or the uncertain system. An H/sub infinity / PID controller is developed based on optimal control theory and the parameters are derived analytically. Its properties are investigated and compared with that of two developed suboptimal controllers: an H/sub 2/ PID controller and a Maclaurin PID controller", "label": ["time-domain response", "frequency-domain response", "process control systems", "overshoot", "rise time", "resonance peak", "stability margin", "proportional-integral derivative controller", "nominal stable process", "suboptimal controller", "h/sub infinity / pid controller", "h/sub 2/ pid controller", "optimal control", "maclaurin pid controller"], "stemmed_label": ["time-domain respons", "frequency-domain respons", "process control system", "overshoot", "rise time", "reson peak", "stabil margin", "proportional-integr deriv control", "nomin stabl process", "suboptim control", "h/sub infin / pid control", "h/sub 2/ pid control", "optim control", "maclaurin pid control"]}
{"doc": "Licensing experiences in the Netherlands The licensing strategy of university libraries in the Netherlands is closely connected with university policies to develop document servers and to make research publications available on the Web. National agreements have been made with major publishers, such as Elsevier Science and Kluwer Academic, to provide access to a wide range of scientific information and to experiment with new ways of providing information and new business models", "label": ["licensing strategy", "university libraries", "netherlands", "university policies", "document servers", "research publications", "web", "elsevier science", "kluwer academic", "scientific information", "business models"], "stemmed_label": ["licens strategi", "univers librari", "netherland", "univers polici", "document server", "research public", "web", "elsevi scienc", "kluwer academ", "scientif inform", "busi model"]}
{"doc": "Full-screen ultrafast video modes over-clocked by simple VESA routines and registers reprogramming under MS-DOS Fast full-screen presentation of stimuli is necessary in psychological research. Although Spitczok von Brisinski (1994) introduced a method that achieved ultrafast display by reprogramming the registers, he could not produce an acceptable full-screen display. In this report, the author introduces a new method combining VESA routine calling with register reprogramming that can yield a display at 640 * 480 resolution, with a refresh rate of about 150 Hz", "label": ["full-screen ultrafast video modes", "fast full-screen stimuli presentation", "psychological research", "vesa routine calling", "ms-dos", "register reprogramming"], "stemmed_label": ["full-screen ultrafast video mode", "fast full-screen stimuli present", "psycholog research", "vesa routin call", "ms-do", "regist reprogram"]}
{"doc": "Real-time implementation of a new low-memory SPIHT image coding algorithm using DSP chip Among all algorithms based on wavelet transform and zerotree quantization, Said and Pearlman's (1996) set partitioning in hierarchical trees (SPIHT) algorithm is well-known for its simplicity and efficiency. This paper deals with the real-time implementation of SPIHT algorithm using DSP chip. In order to facilitate the implementation and improve the codec's performance, some relative issues are thoroughly discussed, such as the optimization of program structure to speed up the wavelet decomposition. SPIHT's high memory requirement is a major drawback for hardware implementation. In this paper, we modify the original SPIHT algorithm by presenting two new concepts-number of error bits and absolute zerotree. Consequently, the memory cost is significantly reduced. We also introduce a new method to control the coding process by number of error bits. Our experimental results show that the implementation meets common requirement of real-time video coding and is proven to be a practical and efficient DSP solution", "label": ["spiht algorithm", "real-time implementation", "wavelet transform", "zerotree quantization", "codec", "wavelet decomposition", "number of error bits", "absolute zerotree", "dsp chip", "set partitioning in hierarchical trees", "memory cost reduction", "video coding"], "stemmed_label": ["spiht algorithm", "real-tim implement", "wavelet transform", "zerotre quantiz", "codec", "wavelet decomposit", "number of error bit", "absolut zerotre", "dsp chip", "set partit in hierarch tree", "memori cost reduct", "video code"]}
{"doc": "Hybrid simulation of space plasmas: models with massless fluid representation of electrons. IV. Kelvin-Helmholtz instability For pt.III. see Prikl. Mat. Informatika, MAKS Press, no. 4, p. 5-56 (2000). This is a survey of the literature on hybrid simulation of the Kelvin-Helmholtz instability. We start with a brief review of the theory: the simplest model of the instability - a transition layer in the form of a tangential discontinuity; compressibility of the medium; finite size of the velocity shear region; pressure anisotropy. We then describe the electromagnetic hybrid model (ions as particles and electrons as a massless fluid) and the main numerical schemes. We review the studies on two-dimensional and three-dimensional hybrid simulation of the process of particle mixing across the magnetopause shear layer driven by the onset of a Kelvin-Helmholtz instability. The article concludes with a survey of literature on hybrid simulation of the Kelvin-Helmholtz instability in finite-size objects: jets moving across the magnetic field in the middle of the field reversal layer; interaction between a magnetized plasma flow and a cylindrical plasma source with zero own magnetic field", "label": ["hybrid simulation", "space plasmas", "massless fluid representation", "kelvin-helmholtz instability", "transition layer", "tangential discontinuity", "pressure anisotropy", "electromagnetic hybrid model", "three-dimensional hybrid simulation", "magnetopause shear layer", "field reversal layer", "magnetized plasma flow", "cylindrical plasma source"], "stemmed_label": ["hybrid simul", "space plasma", "massless fluid represent", "kelvin-helmholtz instabl", "transit layer", "tangenti discontinu", "pressur anisotropi", "electromagnet hybrid model", "three-dimension hybrid simul", "magnetopaus shear layer", "field revers layer", "magnet plasma flow", "cylindr plasma sourc"]}
{"doc": "Using virtual reality to teach disability awareness A desktop virtual reality (VR) program was designed and evaluated to teach children about the accessibility and attitudinal barriers encountered by their peers with mobility impairments. Within this software, children sitting in a virtual wheelchair experience obstacles such as stairs, narrow doors, objects too high to reach, and attitudinal barriers such as inappropriate comments. Using a collaborative research methodology, 15 youth with mobility impairments assisted in developing and beta-testing the software. The effectiveness of the program was then evaluated with 60 children in Grades 4-6 using a controlled pretest/posttest design. The results indicated that the program was effective for increasing children's knowledge of accessibility barriers. Attitudes, grade level, familiarity with individuals with a disability, and gender were also investigated", "label": ["virtual reality", "disability awareness teaching", "children", "accessibility", "virtual wheelchair", "collaborative research methodology", "mobility impairments", "software beta-testing", "collaborative software development", "computer aided instruction", "software effectiveness", "gender"], "stemmed_label": ["virtual realiti", "disabl awar teach", "children", "access", "virtual wheelchair", "collabor research methodolog", "mobil impair", "softwar beta-test", "collabor softwar develop", "comput aid instruct", "softwar effect", "gender"]}
{"doc": "Trust in online advice Many people are now influenced by the information and advice they find on the Internet, much of it of dubious quality. This article describes two studies concerned with those factors capable of influencing people's response to online advice. The first study is a qualitative account of a group of house-hunters attempting to find worthwhile information online. The second study describes a survey of more than 2,500 people who had actively sought advice over the Internet. A framework for understanding trust in online advice is proposed in which first impressions are distinguished from more detailed evaluations. Good Web design can influence the first process, but three key factors-source credibility, personalization, and predictability-are shown to predict whether people actually follow the advice given", "label": ["online advice trust", "internet", "survey", "online mortgage advice", "web design", "source credibility", "personalization", "predictability", "e-commerce", "house buying advice"], "stemmed_label": ["onlin advic trust", "internet", "survey", "onlin mortgag advic", "web design", "sourc credibl", "person", "predict", "e-commerc", "hous buy advic"]}
{"doc": "Baseball, optimization, and the World Wide Web The competition for baseball play-off spots-the fabled pennant race-is one of the most closely watched American sports traditions. While play-off race statistics, such as games back and magic number, are informative, they are overly conservative and do not account for the remaining schedule of games. Using optimization techniques, one can model schedule effects explicitly and determine precisely when a team has secured a play-off spot or has been eliminated from contention. The RIOT Baseball Play-off Races Web site developed at the University of California, Berkeley, provides automatic updates of new, optimization-based play-off race statistics each day of the major league baseball season. In developing the site, we found that we could determine the first-place elimination status of all teams in a division using a single linear-programming formulation, since a minimum win threshold for teams finishing in first place applies to all teams in a division. We identified a similar (but weaker) result for the problem of play-off elimination with wildcard teams", "label": ["baseball play-off spot competition", "optimization", "world wide web", "pennant race", "play-off race statistics", "games back", "magic number", "game schedule", "riot baseball play-off races web site", "linear programming", "lp", "minimum win threshold"], "stemmed_label": ["basebal play-off spot competit", "optim", "world wide web", "pennant race", "play-off race statist", "game back", "magic number", "game schedul", "riot basebal play-off race web site", "linear program", "lp", "minimum win threshold"]}
{"doc": "Pattern recognition strategies for molecular surfaces. II. Surface complementarity For pt.I see ibid., vol.23, p.1176-87 (2002). Fuzzy logic based algorithms for the quantitative treatment of complementarity of molecular surfaces are presented. Therein, the overlapping surface patches defined in part I of this series are used. The identification of complementary surface patches can be considered as a first step for the solution of molecular docking problems. Standard technologies can then be used for further optimization of the resulting complex structures. The algorithms are applied to 33 biomolecular complexes. After the optimization with a downhill simplex method, for all these complexes one structure was found, which is in very good agreement with the experimental results", "label": ["pattern recognition strategies", "surface complementarity", "fuzzy logic based algorithms", "quantitative treatment", "molecular surfaces", "overlapping surface", "biomolecular complexes", "optimization", "downhill simplex method"], "stemmed_label": ["pattern recognit strategi", "surfac complementar", "fuzzi logic base algorithm", "quantit treatment", "molecular surfac", "overlap surfac", "biomolecular complex", "optim", "downhil simplex method"]}
{"doc": "Analyzing the potential of a firm: an operations research approach An approach to analyzing the potential of a firm, which is understood as the firm's ability to provide goods or (and) services to be supplied to a marketplace under restrictions imposed by a business environment in which the firm functions, is proposed. The approach is based on using linear inequalities and, generally, mixed variables in modelling this ability for a broad spectrum of industrial, transportation, agricultural, and other types of firms and allows one to formulate problems of analyzing the potential of a firm as linear programming problems or mixed programming problems with linear constraints. This approach generalizes a previous one which was proposed for a more narrow class of models, and allows one to effectively employ a widely available software for solving practical problems of the considered kind, especially for firms described by large scale models of mathematical programming", "label": ["firm potential analysis", "operations research", "or", "linear inequalities", "industrial firms", "transportation firms", "agricultural firms", "linear programming", "mixed programming", "large-scale models", "mathematical programming"], "stemmed_label": ["firm potenti analysi", "oper research", "or", "linear inequ", "industri firm", "transport firm", "agricultur firm", "linear program", "mix program", "large-scal model", "mathemat program"]}
{"doc": "Shaping the future. BendWizard: a tool for off-line programming of robotic tending systems Setting up a robot to make metal cabinets or cases for desktop computers can be a complex operation. For instance, one expert might be required to carry out a feasibility study, and then another to actually program the robot. Understandably, the need for so much expertise, and the time that's required, generally limits the usefulness of automation to high-volume production. Workshops producing parts in batches smaller than 50 or so, or which rely heavily on semiskilled operators, are therefore often discouraged from investing in automation, and so miss out on its many advantages. What is needed is a software tool that operators without special knowledge of robotics, or with no more than rudimentary CAD skills, can use. One which allows easy offline programming and simulation of the work cell on a PC", "label": ["robotic tending systems", "bendwizard offline programming tool", "metal cabinets", "desktop computer cases", "feasibility study", "high-volume production", "workshops", "cad skills", "work cell simulation"], "stemmed_label": ["robot tend system", "bendwizard offlin program tool", "metal cabinet", "desktop comput case", "feasibl studi", "high-volum product", "workshop", "cad skill", "work cell simul"]}
{"doc": "The efficacy of electronic telecommunications in fostering interpersonal relationships The effectiveness of electronic telecommunications as a supplementary aid to instruction and as a communication link between students, and between students and instructors in fostering interpersonal relationships was explored in this study. More specifically, the impacts of e-mail, one of the most accessible, convenient, and easy to use computer-mediated communications, on student attitudes toward the instructor, group-mates, and other classmates were investigated. A posttest-only experimental design was adopted. In total, 68 prospective teachers enrolling in a \"Computers in Education\" course participated in the study for a whole semester. Results from the study provided substantial evidence supporting e-mail's beneficial effects on student attitudes toward the instructor and other classmates", "label": ["interpersonal relationships", "telecommunications", "student communication link", "e-mail", "computer-mediated communications", "student attitudes", "computers in education course", "educational technology"], "stemmed_label": ["interperson relationship", "telecommun", "student commun link", "e-mail", "computer-medi commun", "student attitud", "comput in educ cours", "educ technolog"]}
{"doc": "Novel active noise-reducing headset using earshell vibration control Active noise-reducing (ANR) headsets are available commercially in applications varying from aviation communication to consumer audio. Current ANR systems use passive attenuation at high frequencies and loudspeaker-based active noise control at low frequencies to achieve broadband noise reduction. This paper presents a novel ANR headset in which the external noise transmitted to the user's ear via earshell vibration is reduced by controlling the vibration of the earshell using force actuators acting against an inertial mass or the earshell headband. Model-based theoretical analysis using velocity feedback control showed that current piezoelectric actuators provide sufficient force but require lower stiffness for improved low-frequency performance. Control simulations based on experimental data from a laboratory headset showed that good performance can potentially be achieved in practice by a robust feedback controller, while a single-frequency real-time control experiment verified that noise reduction can be achieved using earshell vibration control", "label": ["active noise-reducing headset", "earshell vibration control", "aviation communication", "consumer audio", "passive attenuation", "broadband noise reduction", "external noise transmission", "force actuators", "inertial mass", "velocity feedback control", "piezoelectric actuators", "stiffness", "robust feedback controller", "single-frequency real-time control"], "stemmed_label": ["activ noise-reduc headset", "earshel vibrat control", "aviat commun", "consum audio", "passiv attenu", "broadband nois reduct", "extern nois transmiss", "forc actuat", "inerti mass", "veloc feedback control", "piezoelectr actuat", "stiff", "robust feedback control", "single-frequ real-tim control"]}
{"doc": "A design to cost system for innovative product development Presents a prototype object-oriented and rule-based system for product cost modelling and design for automation at an early design stage. The developed system comprises a computer aided design (CAD) solid modelling system, a material selection module, a knowledge-based system (KBS), a process optimization module, a design for assembly module, a cost estimation module and a user interface. Two manufacturing processes, namely machining and injection moulding processes, were considered in the developed system. The main function of the system, besides estimating the product cost, is to generate initial process planning, including the generation and selection of machining processes, their sequence and their machining parameters, and to recommend the most economical assembly technique for a product and provide design improvement suggestions based on a design feasibility technique. In addition, a feature-by-feature cost estimation report is generated using the proposed system to highlight the features of high manufacturing cost. Two case studies were used to validate the developed system", "label": ["design to cost system", "innovative product development", "object-oriented rule-based system", "product cost modelling", "design for automation", "computer aided design solid modelling system", "material selection module", "knowledge-based system", "process optimization module", "design for assembly module", "cost estimation module", "user interface", "machining", "injection moulding", "process planning", "feature-by-feature cost estimation report", "fuzzy logic", "object-oriented programming", "concurrent engineering"], "stemmed_label": ["design to cost system", "innov product develop", "object-ori rule-bas system", "product cost model", "design for autom", "comput aid design solid model system", "materi select modul", "knowledge-bas system", "process optim modul", "design for assembl modul", "cost estim modul", "user interfac", "machin", "inject mould", "process plan", "feature-by-featur cost estim report", "fuzzi logic", "object-ori program", "concurr engin"]}
{"doc": "A server-side program for delivering experiments with animations A server-side program for animation experiments is presented. The program is capable of delivering an experiment composed of discrete animation sequences in various file formats, collecting a discrete or continuous response from the observer, evaluating the appropriateness of the response, and ensuring that the user is not proceeding at an unreasonable rate. Most parameters of the program are controllable by experimenter-edited text files or simple switches in the program code, thereby minimizing the need for programming to create new experiments. A simple demonstration experiment is discussed and is freely available", "label": ["server-side program", "animation experiment delivery", "discrete animation sequences", "file formats", "web based psychological experiments", "internet", "experimenter-edited text files"], "stemmed_label": ["server-sid program", "anim experi deliveri", "discret anim sequenc", "file format", "web base psycholog experi", "internet", "experimenter-edit text file"]}
{"doc": "A survey of interactive mesh-cutting techniques and a new method for implementing generalized interactive mesh cutting using virtual tools In our experience, mesh-cutting methods can be distinguished by how their solutions address the following major issues: definition of the cut path, primitive removal and re-meshing, number of new primitives created, when re-meshing is performed, and representation of the cutting tool. Many researchers have developed schemes for interactive mesh cutting with the goals of reducing the number of new primitives created, creating new primitives with good aspect ratios, avoiding a disconnected mesh structure between primitives in the cut path, and representing the path traversed by the tool as accurately as possible. The goal of this paper is to explain how, by using a very simple framework, one can build a generalized cutting scheme. This method allows for any arbitrary cut to be made within a virtual object, and can simulate cutting surface, layered surface or tetrahedral objects using a virtual scalpel, scissors, or loop cautery tool. This method has been implemented in a real-time, haptic-rate surgical simulation system allowing arbitrary cuts to be made on high-resolution patient-specific models", "label": ["generalized interactive mesh cutting", "virtual tools", "cut path definition", "re-meshing", "cutting tool", "disconnected mesh structure", "virtual object", "tetrahedral objects", "layered surface", "real-time system", "haptic-rate surgical simulation system", "high-resolution patient-specific models", "rendering", "haptic interfaces"], "stemmed_label": ["gener interact mesh cut", "virtual tool", "cut path definit", "re-mesh", "cut tool", "disconnect mesh structur", "virtual object", "tetrahedr object", "layer surfac", "real-tim system", "haptic-r surgic simul system", "high-resolut patient-specif model", "render", "haptic interfac"]}
{"doc": "Nonlinear systems arising from nonisothermal, non-Newtonian Hele-Shaw flows in the presence of body forces and sources In this paper, we first give a formal derivation of several systems of equations for injection moulding. This is done starting from the basic equations for nonisothermal, non-Newtonian flows in a three-dimensional domain. We derive systems for both (T/sup 0/, p/sup 0/) and (T/sup 1/, p/sup 1/) in the presence of body forces and sources. We find that body forces and sources have a nonlinear effect on the systems. We also derive a nonlinear \"Darcy law\". Our formulation includes not only the pressure gradient, but also body forces and sources, which play the role of a nonlinearity. Later, we prove the existence of weak solutions to certain boundary value problems and initial-boundary value problems associated with the resulting equations for (T/sup 0/, p/sup 0/) but in a more general mathematical setting", "label": ["injection moulding", "body forces", "sources", "darcy law", "nonlinear systems", "boundary value problems", "hele-shaw flows"], "stemmed_label": ["inject mould", "bodi forc", "sourc", "darci law", "nonlinear system", "boundari valu problem", "hele-shaw flow"]}
{"doc": "Spectral characteristics of the linear systems over a bounded time interval Consideration was given to the spectral characteristics of the linear dynamic systems over a bounded time interval. Singular characteristics of standard dynamic blocks, transcendental characteristic equations, and partial spectra of the singular functions were studied. Relationship between the spectra under study and the classical frequency characteristic was demonstrated", "label": ["spectral characteristics", "bounded time interval", "linear dynamic systems", "singular characteristics", "standard dynamic blocks", "transcendental characteristic equations", "partial spectra", "singular functions", "frequency characteristic"], "stemmed_label": ["spectral characterist", "bound time interv", "linear dynam system", "singular characterist", "standard dynam block", "transcendent characterist equat", "partial spectra", "singular function", "frequenc characterist"]}
{"doc": "Optical encoding of color three-dimensional correlation Three-dimensional (3D) correlation of color images, considering the color distribution as the third dimension, has been shown to be useful for color pattern recognition tasks. Nevertheless, 3D correlation cannot be directly performed on an optical correlator, that can only process two-dimensional (2D) signals. We propose a method to encode 3D functions onto 2D ones in such a way that the Fourier transform and correlation of these signals, that can be optically performed, encode the 3D Fourier transform and correlation of the 3D signals. The theory for the encoding is given and experimental results obtained in an optical correlator are shown", "label": ["optical encoding", "color three-dimensional correlation", "3d correlation", "color images", "color distribution", "color pattern recognition tasks", "optical correlator", "3d function encoding", "fourier transform", "3d fourier transform"], "stemmed_label": ["optic encod", "color three-dimension correl", "3d correl", "color imag", "color distribut", "color pattern recognit task", "optic correl", "3d function encod", "fourier transform", "3d fourier transform"]}
{"doc": "Virtual borders, real laws Internet activity and treaties National governments are working to tame activity on the Internet. They have worked steadily to extend control over online activities that they believe affect their interests, even when the activities occur outside their borders. These usually involve what governments regard as their domain: protecting public order, enforcing commercial laws, and, occasionally, protecting consumer interests. Methods have included assertions or legal jurisdiction based on where material is accessible instead of where it originates, and the blocking of sites, service providers, or entire high level domains from access by citizens. Such instances are mentioned in this article. Whilst larger companies are able to defend themselves against overseas lawsuits, individuals and smaller organizations lack the resources to defend what are often normal business activities at home, but could violate the laws of local jurisdictions in countries around the world. The problems of libel are discussed as are the blocking of certain sites by certain countries. Efforts to draw up Internet treaties are also mentioned", "label": ["national governments", "internet activity", "online activities", "public order protection", "commercial laws enforcement", "legal jurisdiction", "consumer interests protection", "internet sites blocking", "lawsuits", "internet treaties"], "stemmed_label": ["nation govern", "internet activ", "onlin activ", "public order protect", "commerci law enforc", "legal jurisdict", "consum interest protect", "internet site block", "lawsuit", "internet treati"]}
{"doc": "A formal model of computing with words Classical automata are formal models of computing with values. Fuzzy automata are generalizations of classical automata where the knowledge about the system's next state is vague or uncertain. It is worth noting that like classical automata, fuzzy automata can only process strings of input symbols. Therefore, such fuzzy automata are still (abstract) devices for computing with values, although a certain vagueness or uncertainty are involved in the process of computation. We introduce a new kind of fuzzy automata whose inputs are instead strings of fuzzy subsets of the input alphabet. These new fuzzy automata may serve as formal models of computing with words. We establish an extension principle from computing with values to computing with words. This principle indicates that computing with words can be implemented with computing with values with the price of a big amount of extra computations", "label": ["formal model", "computing with words", "fuzzy automata", "fuzzy subsets", "input alphabet", "extension principle", "pushdown automata"], "stemmed_label": ["formal model", "comput with word", "fuzzi automata", "fuzzi subset", "input alphabet", "extens principl", "pushdown automata"]}
{"doc": "On optimality in auditory information processing We study limits for the detection and estimation of weak sinusoidal signals in the primary part of the mammalian auditory system using a stochastic Fitzhugh-Nagumo model and an action-recovery model for synaptic depression. Our overall model covers the chain from a hair cell to a point just after the synaptic connection with a cell in the cochlear nucleus. The information processing performance of the system is evaluated using so-called phi -divergences from statistics that quantify \"dissimilarity\" between probability measures and are intimately related to a number of fundamental limits in statistics and information theory (IT). We show that there exists a set of parameters that can optimize several important phi -divergences simultaneously and that this set corresponds to a constant quiescent firing rate (QFR) of the spiral ganglion neuron. The optimal value of the QFR is frequency dependent but is essentially independent of the amplitude of the signal (for small amplitudes). Consequently, optimal processing according to several standard IT criteria can be accomplished for this model if and only if the parameters are \"tuned\" to values that correspond to one and the same QFR. This offers a new explanation for the QFR and can provide new insight into the role played by several other parameters of the peripheral auditory system", "label": ["weak sinusoidal signals", "mammalian auditory system", "stochastic fitzhugh-nagumo model", "action-recovery model", "peripheral auditory system", "quiescent firing rate", "spiral ganglion neuron", "brain"], "stemmed_label": ["weak sinusoid signal", "mammalian auditori system", "stochast fitzhugh-nagumo model", "action-recoveri model", "peripher auditori system", "quiescent fire rate", "spiral ganglion neuron", "brain"]}
{"doc": "Three-dimensional geometrical optics code for indoor propagation This paper presents a program, GO 3D, for computing the fields of a transmitter in an indoor environment using geometrical optics. The program uses an \"image tree\" data structure to construct the images needed to compute all the rays carrying fields above a preset \"threshold\" value, no matter how many reflections are needed. The paper briefly describes the input file required to define wall construction, the floor plan, the transmitter, and the receiver locations. A case study consisting of a long corridor with a small room on one side is used to demonstrate the features of the GO 3D program", "label": ["three-dimensional geometrical optics", "3d geometrical optics code", "indoor propagation", "image tree data structure", "image construction", "wall construction", "floor plan", "transmitter", "receiver locations", "ray tracing", "data visualisation"], "stemmed_label": ["three-dimension geometr optic", "3d geometr optic code", "indoor propag", "imag tree data structur", "imag construct", "wall construct", "floor plan", "transmitt", "receiv locat", "ray trace", "data visualis"]}
{"doc": "The culture of usability Now that most of us agree that usability testing is an integral investment in site development, it's time to recognize that the standard approach falls short. It is possible to do less work and get better results while spending less money. By bringing usability testing in-house and breaking tests into more manageable sessions, you can vastly improve your online offering without affecting your profit margin", "label": ["usability testing program", "web site"], "stemmed_label": ["usabl test program", "web site"]}
{"doc": "A 120-mW 3-D rendering engine with 6-Mb embedded DRAM and 3.2-GB/s runtime reconfigurable bus for PDA chip A low-power three-dimensional (3-D) rendering engine is implemented as part of a mobile personal digital assistant (PDA) chip. Six-megabit embedded DRAM macros attached to 8-pixel-parallel rendering logic are logically localized with a 3.2-GB/s runtime reconfigurable bus, reducing the area by 25 compared with conventional local frame-buffer architectures. The low power consumption is achieved by polygon-dependent access to the embedded DRAM macros with line-block mapping providing read-modify-write data transaction. The 3-D rendering engine with 2.22-Mpolygons/s drawing speed was fabricated using 0.18- mu m CMOS embedded memory logic technology. Its area is 24 mm/sup 2/ and its power consumption is 120 mW", "label": ["low-power 3d rendering engine", "three-dimensional rendering engine", "mobile pda chip", "mobile personal digital assistant chip", "embedded dram macros", "8-pixel-parallel rendering logic", "reconfigurable bus", "low power consumption", "polygon-dependent access", "line-block mapping", "read-modify-write data transaction", "cmos embedded memory logic technology", "3d graphics rendering", "120 mw", "6 mbit", "3.2 gb/s", "0.18 micron"], "stemmed_label": ["low-pow 3d render engin", "three-dimension render engin", "mobil pda chip", "mobil person digit assist chip", "embed dram macro", "8-pixel-parallel render logic", "reconfigur bu", "low power consumpt", "polygon-depend access", "line-block map", "read-modify-writ data transact", "cmo embed memori logic technolog", "3d graphic render", "120 mw", "6 mbit", "3.2 gb/", "0.18 micron"]}
{"doc": "The ultimate control group Empirical research on the organization of firms requires that firms be classified on the basis of their control structures. This should be done in a way that can potentially be made operational. It is easy to identify the ultimate controller of a hierarchical organization, and the literature has largely focused on this case. However, many organizational structures mix hierarchy with collective choice procedures such as voting, or use circular structures under which superiors are accountable to their subordinates. The author develops some analytic machinery that can be used to map the authority structures of such organizations, and show that under mild restrictions there is a well-defined ultimate control group. The results are consistent with intuitions about the nature of control in familiar economic settings", "label": ["ultimate control group", "hierarchical organization", "organizational structures", "authority structures", "committees", "control rights", "firm organization"], "stemmed_label": ["ultim control group", "hierarch organ", "organiz structur", "author structur", "committe", "control right", "firm organ"]}
{"doc": "A nonlinear modulation strategy for hybrid AC/DC power systems A nonlinear control strategy to improve transient stability of a multi-machine AC power system with several DC links terminated in the presence of large disturbances is presented. The approach proposed in this paper is based on differential geometric theory, and the HVDC systems are taken as a variable admittance connected at the inverter or rectifier AC bus. After deriving the analytical description of the relationship between the variable admittance and active power flows of each generator, the traditional generator dynamic equations can thus be expressed with the variable admittance of HVDC systems as an additional state variable and changed to an affine form, which is suitable for global linearization method being used to determine its control variable. An important feature of the proposed method is that, the modulated DC power is an adaptive and non-linear function of AC system states, and it can be realized by local feedback and less transmitted data from, adjacent generators. The design procedure is tested on a dual-infeed hybrid AC/DC system", "label": ["nonlinear control strategy", "transient stability", "multi-machine ac power system", "dc links", "nonlinear modulation strategy", "hybrid ac/dc power systems", "differential geometric theory", "hvdc systems", "variable admittance", "inverter", "rectifier ac bus", "active power flows", "generator dynamic equations", "affine form", "global linearization method", "local feedback", "adjacent generators", "dual-infeed hybrid ac/dc system"], "stemmed_label": ["nonlinear control strategi", "transient stabil", "multi-machin ac power system", "dc link", "nonlinear modul strategi", "hybrid ac/dc power system", "differenti geometr theori", "hvdc system", "variabl admitt", "invert", "rectifi ac bu", "activ power flow", "gener dynam equat", "affin form", "global linear method", "local feedback", "adjac gener", "dual-infe hybrid ac/dc system"]}
{"doc": "Numerical representation of binary relations with a multiplicative error function This paper studies the case of the representation of a binary relation via a numerical function with threshold (error) depending on both compared alternatives. The error is considered to be multiplicative, its value being either directly or inversely proportional to the values of the numerical function. For the first case, it is proved that a binary relation is a semiorder. Moreover, any semiorder can be represented in this form. In the second case, the corresponding binary relation is an interval order", "label": ["numerical representation", "binary relations", "multiplicative error function", "numerical function", "threshold", "error", "semiorder", "interval order"], "stemmed_label": ["numer represent", "binari relat", "multipl error function", "numer function", "threshold", "error", "semiord", "interv order"]}
{"doc": "An optimal control algorithm based on reachability set approximation and linearization The terminal functional of a general control system is refined by studying an analogous problem for a variational system and regularization. A sequential refinement method is designed by combining the local approximation of the reachability set and reduction. The corresponding algorithm has relaxation properties. An illustrative example is given", "label": ["determinate systems", "optimal control algorithm", "reachability set approximation", "linearization", "terminal functional", "variational system", "regularization", "sequential refinement method", "local approximation", "relaxation properties"], "stemmed_label": ["determin system", "optim control algorithm", "reachabl set approxim", "linear", "termin function", "variat system", "regular", "sequenti refin method", "local approxim", "relax properti"]}
{"doc": "The impact of EAD adoption on archival programs: a pilot survey of early implementers The article reports the results of a survey conducted to assess the impact that the implementation of Encoded Archival Description (EAD) has on archival programs. By gathering data related to the funding, staffing, and evaluation of EAD programs and about institutional goals for EAD implementation, the study explored how EAD has affected the operations of the institutions which are utilizing it and the extent to which EAD has become a part of regular repository functions", "label": ["ead adoption", "archival programs", "encoded archival description", "funding", "staffing", "ead programs", "institutional goals", "ead implementation", "regular repository functions", "archival descriptive standards", "diffusion of innovation"], "stemmed_label": ["ead adopt", "archiv program", "encod archiv descript", "fund", "staf", "ead program", "institut goal", "ead implement", "regular repositori function", "archiv descript standard", "diffus of innov"]}
{"doc": "Support vector machines model for classification of thermal error in machine tools This paper addresses a change in the concept of machine tool thermal error prediction which has been hitherto carried out by directly mapping them with the temperature of critical elements on the machine. The model developed herein using support vector machines, a powerful data-training algorithm, seeks to account for the impact of specific operating conditions, in addition to temperature variation, on the effective prediction of thermal errors. Several experiments were conducted to study the error pattern, which was found to change significantly with variation in operating conditions. This model attempts to classify the error based on operating conditions. Once classified, the error is then predicted based on the temperature states. This paper also briefly describes the concept of the implementation of such a comprehensive model along with an on-line error assessment and calibration system in a PC-based open-architecture controller environment, so that it could be employed in regular production for the purpose of periodic calibration of machine tools", "label": ["svm", "support vector machines model", "thermal error classification", "machine tool thermal error prediction", "critical element temperature", "data-training algorithm", "error pattern", "online error assessment", "online calibration system", "pc-based open-architecture controller environment"], "stemmed_label": ["svm", "support vector machin model", "thermal error classif", "machin tool thermal error predict", "critic element temperatur", "data-train algorithm", "error pattern", "onlin error assess", "onlin calibr system", "pc-base open-architectur control environ"]}
{"doc": "Prospects for quantitative computed tomography imaging in the presence of foreign metal bodies using statistical image reconstruction X-ray computed tomography (CT) images of patients bearing metal intracavitary applicators or other metal foreign objects exhibit severe artifacts including streaks and aliasing. We have systematically evaluated via computer simulations the impact of scattered radiation, the polyenergetic spectrum, and measurement noise on the performance of three reconstruction algorithms: conventional filtered backprojection (FBP), deterministic iterative deblurring, and a new iterative algorithm, alternating minimization (AM), based on a CT detector model that includes noise, scatter, and polyenergetic spectra. Contrary to the dominant view of the literature, FBP streaking artifacts are due mostly to mismatches between FBP's simplified model of CT detector response and the physical process of signal acquisition. Artifacts on AM images are significantly mitigated as this algorithm substantially reduces detector-model mismatches. However, metal artifacts are reduced to acceptable levels only when prior knowledge of the metal object in the patient, including its pose, shape, and attenuation map, are used to constrain AM's iterations. AM image reconstruction, in combination with object-constrained CT to estimate the pose of metal objects in the patient, is a promising approach for effectively mitigating metal artifacts and making quantitative estimation of tissue attenuation coefficients a clinical possibility", "label": ["quantitative computed tomography imaging", "foreign metal bodies", "statistical image reconstruction", "metal artifact reduction", "brachytherapy", "medical diagnostic imaging", "signal acquisition physical process", "object-constrained ct", "iterative algorithm", "alternating minimization", "ct detector model", "noise", "scatter", "polyenergetic spectra", "clinical possibility", "deterministic iterative deblurring", "filtered backprojection"], "stemmed_label": ["quantit comput tomographi imag", "foreign metal bodi", "statist imag reconstruct", "metal artifact reduct", "brachytherapi", "medic diagnost imag", "signal acquisit physic process", "object-constrain ct", "iter algorithm", "altern minim", "ct detector model", "nois", "scatter", "polyenerget spectra", "clinic possibl", "determinist iter deblur", "filter backproject"]}
{"doc": "Estimation of error in curvature computation on multi-scale free-form surfaces A novel technique for multi-scale curvature computation on a free-form 3-D surface is presented. This is achieved by convolving local parametrisations of the surface with 2-D Gaussian filters iteratively. In our technique, semigeodesic coordinates are constructed at each vertex of the mesh. Smoothing results are shown for 3-D surfaces with different shapes indicating that surface noise is eliminated and surface details are removed gradually. A number of evolution properties of 3-D surfaces are described. Next, the surface Gaussian and mean curvature values are estimated accurately at multiple scales which are then mapped to colours and displayed directly on the surface. The performance of the technique when selecting different directions as an arbitrary direction for the geodesic at each vertex are also presented. The results indicate that the error observed for the estimation of Gaussian and mean curvatures is quite low after only one iteration. Furthermore, as the surface is smoothed iteratively, the error is further reduced. The results also show that the estimation error of Gaussian curvature is less than that of mean curvature. Our experiments demonstrate that estimation of smoothed surface curvatures are very accurate and not affected by the arbitrary direction of the first geodesic line when constructing semigeodesic coordinates. Our technique is independent of the underlying triangulation and is also more efficient than volumetric diffusion techniques since 2-D rather than 3-D convolutions are employed. Finally, the method presented here is a generalisation of the Curvature Scale Space method for 2-D contours. The CSS method has outperformed comparable techniques within the MPEG-7 evaluation framework. As a result, it has been selected for inclusion in the MPEG-7 package of standards", "label": ["multi-scale curvature computation", "free-form 3d surface", "local parametrisations", "2d gaussian filters", "surface noise", "evolution properties", "surface gaussian values", "mean curvature values", "semigeodesic coordinates", "underlying triangulation", "volumetric diffusion techniques", "convolutions", "curvature scale space method", "mpeg-7 evaluation framework"], "stemmed_label": ["multi-scal curvatur comput", "free-form 3d surfac", "local parametris", "2d gaussian filter", "surfac nois", "evolut properti", "surfac gaussian valu", "mean curvatur valu", "semigeodes coordin", "underli triangul", "volumetr diffus techniqu", "convolut", "curvatur scale space method", "mpeg-7 evalu framework"]}
{"doc": "MACLP: multi agent constraint logic programming Multi agent systems (MAS) have become the key technology for decomposing complex problems in order to solve them more efficiently, or for problems distributed in nature. However, many industrial applications, besides their distributed nature, also involve a large number of parameters and constraints, i.e. they are combinatorial. Solving such particularly hard problems efficiently requires programming tools that combine MAS technology with a programming schema that facilitates the modeling and solution of constraints. This paper presents MACLP (multi agent constraint logic programming), a logic programming platform for building, in a declarative way, multi agent systems with constraint-solving capabilities. MACLP extends CSPCONS, a logic programming system that permits distributed program execution through communicating sequential Prolog processes with constraints, by providing all the necessary facilities for communication between agents. These facilities abstract from the programmer all the low-level details of the communication and allow him to focus on the development of the agent itself", "label": ["multi agent constraint logic programming", "multi agent systems", "parameters", "combinatorial problems", "hard problems", "constraint solving", "distributed program execution", "communicating sequential prolog processes"], "stemmed_label": ["multi agent constraint logic program", "multi agent system", "paramet", "combinatori problem", "hard problem", "constraint solv", "distribut program execut", "commun sequenti prolog process"]}
{"doc": "A humanist's legacy in medical informatics: visions and accomplishments of Professor Jean-Raoul Scherrer The objective is to report on the work of Prof. Jean-Raoul Scherrer, and show how his humanist vision, medical skills and scientific background have enabled and shaped the development of medical informatics over the last 30 years. Starting with the mainframe-based patient-centred hospital information system DIOGENE in the 70s, Prof. Scherrer developed, implemented and evolved innovative concepts of man-machine interfaces, distributed and federated environments, leading the way with information systems that obstinately focused on the support of care providers and patients. Through a rigorous design of terminologies and ontologies, the DIOGENE data would then serve as a basis for the development of clinical research, data mining, and lead to innovative natural language processing techniques. In parallel, Prof. Scherrer supported the development of medical image management, ranging from a distributed picture archiving and communication systems (PACS) to molecular imaging of protein electrophoreses. Recognizing the need for improving the quality and trustworthiness of medical information of the Web, Prof. Scherrer created the Health-On-the Net (HON) foundation. These achievements, made possible thanks to his visionary mind, deep humanism, creativity, generosity and determination, have made of Prof. Scherrer a true pioneer and leader of the human-centered, patient-oriented application of information technology for improving healthcare", "label": ["professor jean-raoul scherrer", "medical informatics", "mainframe based patient centered hospital information system", "medical image management", "pacs", "internet", "diogene system", "man-machine interfaces", "distributed systems", "federated systems", "data mining", "natural language processing"], "stemmed_label": ["professor jean-raoul scherrer", "medic informat", "mainfram base patient center hospit inform system", "medic imag manag", "pac", "internet", "diogen system", "man-machin interfac", "distribut system", "feder system", "data mine", "natur languag process"]}
{"doc": "How to avoid merger pitfalls Paul Diamond of consultancy KPMG explains why careful IT asset management is crucial to the success of mergers", "label": ["consultancy", "kpmg", "it asset management", "mergers"], "stemmed_label": ["consult", "kpmg", "it asset manag", "merger"]}
{"doc": "Nuclear magnetic resonance molecular photography A procedure is described for storing a two-dimensional (2D) pattern consisting of 32*32=1024 bits in a spin state of a molecular system and then retrieving the stored information as a stack of nuclear magnetic resonance spectra. The system used is a nematic liquid crystal, the protons of which act as spin clusters with strong intramolecular interactions. The technique used is a programmable multifrequency irradiation with low amplitude. When it is applied to the liquid crystal, a large number of coherent long-lived /sup 1/H response signals can be excited, resulting in a spectrum showing many sharp peaks with controllable frequencies and amplitudes. The spectral resolution is enhanced by using a second weak pulse with a 90 degrees phase shift, so that the 1024 bits of information can be retrieved as a set of well-resolved pseudo-2D spectra reproducing the input pattern", "label": ["nmr molecular photography", "2d pattern", "molecular system spin state", "information storage", "nematic liquid crystal", "spin clusters", "strong intramolecular interactions", "programmable multifrequency irradiation", "low amplitude", "coherent long-lived /sup 1/h response signals", "spectral resolution", "second weak pulse", "pseudo-2d spectra", "spin echoes", "hilbert spaces", "high-content molecular information processing", "coupled spins", "dipole-dipole interactions", "spin-locking", "proton spin", "spin dynamics", "1024 bit"], "stemmed_label": ["nmr molecular photographi", "2d pattern", "molecular system spin state", "inform storag", "nemat liquid crystal", "spin cluster", "strong intramolecular interact", "programm multifrequ irradi", "low amplitud", "coher long-liv /sup 1/h respons signal", "spectral resolut", "second weak puls", "pseudo-2d spectra", "spin echo", "hilbert space", "high-cont molecular inform process", "coupl spin", "dipole-dipol interact", "spin-lock", "proton spin", "spin dynam", "1024 bit"]}
{"doc": "Noninvasive myocardial activation time imaging: a novel inverse algorithm applied to clinical ECG mapping data Linear approaches like the minimum-norm least-square algorithm show insufficient performance when it comes to estimating the activation time map on the surface of the heart from electrocardiographic (ECG) mapping data. Additional regularization has to be considered leading to a nonlinear problem formulation. The Gauss-Newton approach is one of the standard mathematical tools capable of solving this kind of problem. To our experience, this algorithm has specific drawbacks which are caused by the applied regularization procedure. In particular, under clinical conditions the amount of regularization cannot be determined clearly. For this reason, we have developed an iterative algorithm solving this nonlinear problem by a sequence of regularized linear problems. At each step of iteration, an individual L-curve is computed. Subsequent iteration steps are performed with the individual optimal regularization parameter. This novel approach is compared with the standard Gauss-Newton approach. Both methods are applied to simulated ECG mapping data as well as to single beat sinus rhythm data from two patients recorded in the catheter laboratory. The proposed approach shows excellent numerical and computational performance, even under clinical conditions at which the Gauss-Newton approach begins to break down", "label": ["noninvasive myocardial activation time imaging", "electrodiagnostics", "activation time imaging", "l-curve method", "noninvasive electrocardiography", "tikhonov regularization", "gauss-newton approach", "individual optimal regularization parameter", "catheter laboratory", "clinical conditions", "iteration steps", "heart surface", "regularization procedure", "inverse algorithm", "clinical ecg mapping data"], "stemmed_label": ["noninvas myocardi activ time imag", "electrodiagnost", "activ time imag", "l-curv method", "noninvas electrocardiographi", "tikhonov regular", "gauss-newton approach", "individu optim regular paramet", "cathet laboratori", "clinic condit", "iter step", "heart surfac", "regular procedur", "invers algorithm", "clinic ecg map data"]}
{"doc": "Modularity in technology and organization The paper is an attempt to raid both the literature on modular design and the literature on property rights to create the outlines of a modularity theory of the firm. Such a theory will look at firms, and other organizations, in terms of the partitioning of rights-understood as protected spheres of authority-among cooperating parties. It will assert that organizations reflect nonmodular structures, that is, structures in which decision rights, rights of alienation, and residual claims to income do not all reside in the same hands", "label": ["modularity", "technology", "organization", "property rights", "partitioning of rights", "authority", "cooperating parties", "nonmodular structures", "decision rights", "rights of alienation", "transaction costs"], "stemmed_label": ["modular", "technolog", "organ", "properti right", "partit of right", "author", "cooper parti", "nonmodular structur", "decis right", "right of alien", "transact cost"]}
{"doc": "Loudspeaker voice-coil inductance losses: circuit models, parameter estimation, and effect on frequency response When the series resistance is separated and treated as a separate element, it is shown that losses in an inductor require the ratio of the flux to MMF in the core to be frequency dependent. For small-signal operation, this dependence leads to a circuit model composed of a lossless inductor and a resistor in parallel, both of which are frequency dependent. Mathematical expressions for these elements are derived under the assumption that the ratio of core flux to MMF varies as omega /sup n-1/, where n is a constant. A linear regression technique is described for extracting the model parameters from measured data. Experimental data are presented to justify the model for the lossy inductance of a loudspeaker voice-coil. A SPICE example is presented to illustrate the effects of voice-coil inductor losses on the frequency response of a typical driver", "label": ["loudspeaker voice-coil inductance losses", "circuit models", "parameter estimation", "frequency response", "series resistance", "small-signal operation", "linear regression", "lossy inductance", "spice", "loudspeaker driver", "lossless inductor", "core flux to mmf ratio"], "stemmed_label": ["loudspeak voice-coil induct loss", "circuit model", "paramet estim", "frequenc respons", "seri resist", "small-sign oper", "linear regress", "lossi induct", "spice", "loudspeak driver", "lossless inductor", "core flux to mmf ratio"]}
{"doc": "Designing a new urban Internet The parallel between designing a Web site and the construction of a building is a familiar one, but how often do we think of the Internet as having parks and streets? It would be absurd to say that the Internet could ever take the place of real, livable communities; however, it is safe to say that the context for using the Internet is on a path of change. As the Internet evolves beyond a simple linkage of disparate Web sites and applications, the challenge for Information Architects is establishing a process by which to structure, organize, and design networked environments. The principles that guide New Urbanism can offer much insight into networked electronic environment design. At the core of every New Urbanism principle is the idea of \"wholeness\"-of making sure that neighborhoods and communities are knit together in a way that supports civic activities, economic development, efficient ecosystems, aesthetic beauty, and human interaction", "label": ["web site", "internet", "information architects", "private-public sector cooperation", "global information networks", "networked environments", "networked electronic environment design", "communities"], "stemmed_label": ["web site", "internet", "inform architect", "private-publ sector cooper", "global inform network", "network environ", "network electron environ design", "commun"]}
{"doc": "Four factors influencing the fair market value of out-of print books. 2 Fot pt.1 see ibid., p.71-8 (2002). Data from the fifty-six titles examined qualitatively in the Patterson study are examined quantitatively. In addition to the four factors of edition, condition, dust jacket, and autograph that were hypothesized to influence the value of a book, four other factors for which information was available in the data were examined", "label": ["out-of-print books", "quantitative analysis", "fair market value", "pricing", "economics", "publisher"], "stemmed_label": ["out-of-print book", "quantit analysi", "fair market valu", "price", "econom", "publish"]}
{"doc": "Reaching for five nines: ActiveWatch and SiteSeer Every Web admin's dream is achieving the fabled five nines-99.999 percent uptime. To attain such availability, your Web site must be down no more than about five minutes per year. Technologies like RAID, clustering, and load balancing make this easier, but to actually track uptime, maintain auditable records, and discover patterns in failures to prevent downtime in the future, you'll need to set up external monitoring. Because your Internet connection is a key factor in measuring uptime, you must monitor your site from the Internet itself, beyond your firewall. You could monitor with custom software on remote hosts, or you could use one of the two reasonably priced services available: Mercury Interactive's ActiveWatch and Freshwater Software's SiteSeer. (Freshwater Software has been a subsidiary of Mercury Interactive for about a year now.) The two services offer a slightly different mix of features and target different markets. Both services offer availability and performance monitoring from several remote locations, alerts to email or pager, and periodic reports. They differ in what's most easily monitored, and in the way you interact with the services", "label": ["web site", "uptime tracking", "auditable records", "failure pattern discovery", "downtime", "external monitoring", "internet connection", "mercury interactive activewatch", "freshwater software siteseer", "performance monitoring", "availability monitoring", "remote locations", "email alerts", "pager alerts", "periodic reports"], "stemmed_label": ["web site", "uptim track", "audit record", "failur pattern discoveri", "downtim", "extern monitor", "internet connect", "mercuri interact activewatch", "freshwat softwar sites", "perform monitor", "avail monitor", "remot locat", "email alert", "pager alert", "period report"]}
{"doc": "The Canadian National Site Licensing Project In January 2000, a consortium of 64 universities in Canada signed a historic inter-institutional agreement that launched the Canadian National Site Licensing Project (CNSLP), a three-year pilot project aimed at bolstering the research and innovation capacity of the country's universities. CNSLP tests the feasibility of licensing, on a national scale, electronic versions of scholarly publications; in its initial phases the project is focused on full-text electronic journals and research databases in science, engineering, health and environmental disciplines. This article provides an overview of the CNSLP initiative, summarizes organizational and licensing accomplishments to date, and offers preliminary observations on challenges and opportunities for subsequent phases of the project", "label": ["canadian national site licensing project", "inter-institutional agreement", "research and innovation", "cnslp", "academic libraries", "information resources", "electronic scholarly publications", "full-text electronic journals", "research databases"], "stemmed_label": ["canadian nation site licens project", "inter-institut agreement", "research and innov", "cnslp", "academ librari", "inform resourc", "electron scholarli public", "full-text electron journal", "research databas"]}
{"doc": "Union outreach - a pilgrim's progress As the American labor movement continues on its path toward reorganization and rejuvenation, archivists are challenged to ensure that the organizational, political, and cultural changes labor unions are experiencing are fully documented. The article examines the need for labor archivists to reach out actively to unions and the problems they face in getting their message across, not only to union leadership but also to union members. Outreach by labor archivists is vital on three critical fronts: the need to secure union funding in support of labor archival programs; obtaining union cooperation in reviewing and amending obsolete deposit agreements; and coordinating efforts with unions to save the records of closing district and local union offices. Attempting to resolve these outstanding issues, labor archivists are pulled between two distinct institutional cultures (one academic in nature, the other enmeshed in a union bureaucracy) and often have their own labor archival programs compromised by the internal dynamics and politics inherent in administering large academic libraries and unions. If labor archivists are to be successful, they must find their collective voice within the labor movement and establish their relevancy to unions during a period of momentous change and restructuring. Moreover, archivists need to give greater thought to designing and implementing outreach programs that bridge the fundamental \"disconnect\" between union bureaucracies and the rank and file, and unions and the public", "label": ["american labor movement", "archivists", "political changes", "cultural changes", "labor unions", "labor archivists", "union leadership", "union members", "union funding", "labor archival programs", "union cooperation", "obsolete deposit agreements", "union offices", "institutional cultures", "union bureaucracy", "internal dynamics", "large academic libraries", "collective voice"], "stemmed_label": ["american labor movement", "archivist", "polit chang", "cultur chang", "labor union", "labor archivist", "union leadership", "union member", "union fund", "labor archiv program", "union cooper", "obsolet deposit agreement", "union offic", "institut cultur", "union bureaucraci", "intern dynam", "larg academ librari", "collect voic"]}
{"doc": "Enterprise in focus at NetSec 2002 NetSec 2002 took place in San Francisco, amid industry reflection on the balance to be struck between combatting cyber-terrorism and safeguarding civil liberties post-9.11. The author reports on the punditry and the pedagogy at the CSI event, focusing on security in the enterprise", "label": ["netsec 2002", "csi", "enterprise security"], "stemmed_label": ["netsec 2002", "csi", "enterpris secur"]}
{"doc": "Fully automatic algorithm for region of interest location in camera calibration We present an automatic method for region of interest (ROI) location in camera calibration used in computer vision inspection. An intelligent ROI location algorithm based on the Radon transform is developed to automate the calibration process. The algorithm remains robust even if the anchor target has a notable rotation angle in the target plane. This method functions well although the anchor target is not carefully positioned. Several improvement methods are studied to avoid the algorithm's huge time/space consumption problem. The algorithm runs about 100 times faster if these improvement methods are applied. Using this method fully automatic camera calibration is achieved without human interactive ROI specification. Experiments show that this algorithm can help to calibrate the intrinsic parameters of the zoom lens and the camera parameters quickly and automatically", "label": ["fully automatic algorithm", "interest location", "camera calibration", "region of interest location", "computer vision inspection", "roi location algorithm", "radon transform", "calibration process", "rotation angle", "time/space consumption problem", "fully automatic camera calibration", "human interactive specification", "intrinsic parameters", "zoom lens", "camera parameters"], "stemmed_label": ["fulli automat algorithm", "interest locat", "camera calibr", "region of interest locat", "comput vision inspect", "roi locat algorithm", "radon transform", "calibr process", "rotat angl", "time/spac consumpt problem", "fulli automat camera calibr", "human interact specif", "intrins paramet", "zoom len", "camera paramet"]}
{"doc": "Blitzograms - interactive histograms As computers become ever faster, more and more procedures that were once viewed as iterative will continue to become instantaneous. The blitzogram is the application of this trend to histograms, which the author hopes will lead to a better tacit understanding of probability distributions among both students and managers. And this is not just an academic exercise. Commercial Monte Carlo simulation packages like @RISK and Crystal Ball, and my INSIGHT.xla are widely available", "label": ["blitzogram", "histograms", "probability distributions", "mba", "operations research", "management science", "statistics"], "stemmed_label": ["blitzogram", "histogram", "probabl distribut", "mba", "oper research", "manag scienc", "statist"]}
{"doc": "New paradigms for interactive 3D volume segmentation We present a new virtual reality-based interaction metaphor for semi-automatic segmentation of medical 3D volume data. The mouse-based, manual initialization of deformable surfaces in 3D represents a major bottleneck in interactive segmentation. In our multi-modal system we enhance this process with additional sensory feedback. A 3D haptic device is used to extract the centreline of a tubular structure. Based on the obtained path a cylinder with varying diameter is generated, which in turn is used as the initial guess for a deformable surface", "label": ["interactive 3d volume segmentation", "virtual reality", "interaction metaphor", "medical image segmentation", "mouse", "deformable surfaces", "interactive segmentation", "multi-modal system", "sensory feedback", "3d haptic device", "tubular structure", "varying diameter cylinder", "deformable surface", "haptic interaction"], "stemmed_label": ["interact 3d volum segment", "virtual realiti", "interact metaphor", "medic imag segment", "mous", "deform surfac", "interact segment", "multi-mod system", "sensori feedback", "3d haptic devic", "tubular structur", "vari diamet cylind", "deform surfac", "haptic interact"]}
{"doc": "Project Euclid and the role of research libraries in scholarly publishing Project Euclid, a joint electronic journal publishing initiative of Cornell University Library and Duke University Press is discussed in the broader contexts of the changing patterns of scholarly communication and the publishing scene of mathematics. Specific aspects of the project such as partnerships and the creation of an economic model are presented as well as what it takes to be a publisher. Libraries have gained important and relevant experience through the creation and management of digital libraries, but they need to develop further skills if they want to adopt a new role in the life cycle of scholarly communication", "label": ["project euclid", "joint electronic journal publishing initiative", "cornell university library", "duke university press", "scholarly communication", "mathematics", "partnerships", "economic model", "scholarly publishing", "research libraries"], "stemmed_label": ["project euclid", "joint electron journal publish initi", "cornel univers librari", "duke univers press", "scholarli commun", "mathemat", "partnership", "econom model", "scholarli publish", "research librari"]}
{"doc": "The evolution of information systems: Their impact on organizations and structures Information systems and organization structures have been highly interconnected with each other. Over the years, information systems architectures as well as organization structures have evolved from centralized to more decentralized forms. This research looks at the evolution of both information systems and organization structures. In the process, it looks into the impact of computers on organizations, and examines the ways organization structures have changed, in association with changes in information system architectures. It also suggests logical linkages between information system architectures and their \"fit\" with certain organization structures and strategies. It concludes with some implications for emerging and future organizational forms, and provides a quick review of the effect of the Internet on small businesses traditionally using stand-alone computers", "label": ["information systems evolution", "information system architectures"], "stemmed_label": ["inform system evolut", "inform system architectur"]}
{"doc": "Identification of states of complex systems with estimation of admissible measurement errors on the basis of fuzzy information The problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered. Some estimates of a maximally admissible degree of measurement error are obtained that make it possible, using the apparatus of fuzzy set theory, to correctly identify the current state of a system", "label": ["complex systems states identification", "admissible measurement errors", "fuzzy information", "informative attributes", "measurement error", "fuzzy set theory"], "stemmed_label": ["complex system state identif", "admiss measur error", "fuzzi inform", "inform attribut", "measur error", "fuzzi set theori"]}
{"doc": "Fusion of qualitative bond graph and genetic algorithms: A fault diagnosis application In this paper, the problem of fault diagnosis via integration of genetic algorithms (GA's) and qualitative bond graphs (QBG's) is addressed. We suggest that GA's can be used to search for possible fault components among a system of qualitative equations. The QBG is adopted as the modeling scheme to generate a set of qualitative equations. The qualitative bond graph provides a unified approach for modeling engineering systems, in particular, mechatronic systems. In order to demonstrate the performance of the proposed algorithm, we have tested the proposed algorithm on an in-house designed and built floating disc experimental setup. Results from fault diagnosis in the floating disc system are presented and discussed. Additional measurements will be required to localize the fault when more than one fault candidate is inferred. Fault diagnosis is activated by a fault detection mechanism when a discrepancy between measured abnormal behavior and predicted system behavior is observed. The fault detection mechanism is not presented here", "label": ["qualitative bond graph", "genetic algorithms", "fault diagnosis", "fault components", "qualitative equations", "engineering systems", "mechatronic systems", "floating disc", "measured abnormal behavior", "predicted system behavior"], "stemmed_label": ["qualit bond graph", "genet algorithm", "fault diagnosi", "fault compon", "qualit equat", "engin system", "mechatron system", "float disc", "measur abnorm behavior", "predict system behavior"]}
{"doc": "Efficient two-level image thresholding method based on Bayesian formulation and the maximum entropy principle An efficient method for two-level thresholding is proposed based on the Bayes formula and the maximum entropy principle, in which no assumptions of the image histogram are made. An alternative criterion is derived based on maximizing entropy and used for speeding up the searching algorithm. Five forms of conditional probability distributions-simple, linear, parabola concave, parabola convex, and S-function-are employed and compared to each other for optimal threshold determination. The effect of precision on optimal threshold determination is discussed and a trade-off precision epsilon =0.001 is selected experimentally. Our experiments demonstrate that the proposed method achieves a significant improvement in speed from 26 to 57 times faster than the exhaustive search method", "label": ["two-level image thresholding method", "bayesian formulation", "maximum entropy principle", "image histogram", "entropy", "searching algorithm", "conditional probability distributions", "parabola concave", "parabola convex", "s-function", "optimal threshold determination", "trade-off precision", "image segmentation", "image thresholding"], "stemmed_label": ["two-level imag threshold method", "bayesian formul", "maximum entropi principl", "imag histogram", "entropi", "search algorithm", "condit probabl distribut", "parabola concav", "parabola convex", "s-function", "optim threshold determin", "trade-off precis", "imag segment", "imag threshold"]}
{"doc": "Nuts and bolts: implementing descriptive standards to enable virtual collections To date, online archival information systems have relied heavily on legacy finding aids for data to encode and provide to end users, despite fairly strong indications in the archival literature that such legacy data is problematic even as a mediated access tool. Archivists have only just begun to study the utility of archival descriptive data for end users in unmediated settings such as via the Web. The ability of future archival information systems to respond to the expectations and needs of end users is inextricably linked to archivists getting their collective data house in order. The General International Standard Archival Description (ISAD(G)) offers the profession a place from which to start extricating ourselves from the idiosyncracies of our legacy data and description practices", "label": ["descriptive standards", "virtual collections", "online archival information systems", "end users", "archival literature", "legacy data", "mediated access tool", "archivists", "archival descriptive data", "archival information systems", "collective data house", "general international standard archival description", "isad", "online archive of california", "oac"], "stemmed_label": ["descript standard", "virtual collect", "onlin archiv inform system", "end user", "archiv literatur", "legaci data", "mediat access tool", "archivist", "archiv descript data", "archiv inform system", "collect data hous", "gener intern standard archiv descript", "isad", "onlin archiv of california", "oac"]}
{"doc": "The p-p rearrangement and failure-tolerance of double p-ary multirings and generalized hypercubes It is shown that an arbitrary grouped p-element permutation can be implemented in a conflict-free way through the commutation of channels on the double p-ary multiring or the double p-ary hypercube. It is revealed that in arbitrary single-element permutations, these commutators display the property of the (p-1)-nodal failure-tolerance and the generalized hypercube displays in addition the property of the (p-1)-channel failure-tolerance", "label": ["p-p rearrangement", "failure-tolerance", "double p-ary multirings", "generalized hypercubes", "p-element permutation", "conflict-free implementation", "single-element permutations", "commutators"], "stemmed_label": ["p-p rearrang", "failure-toler", "doubl p-ari multir", "gener hypercub", "p-element permut", "conflict-fre implement", "single-el permut", "commut"]}
{"doc": "Adaptive filtering for noise reduction in hue saturation intensity color space Even though the hue saturation intensity (HSI) color model has been widely used in color image processing and analysis, the conversion formulas from the RGB color model to HSI are nonlinear and complicated in comparison with the conversion formulas of other color models. When an RGB image is degraded by random Gaussian noise, this nonlinearity leads to a nonuniform noise distribution in HSI, making accurate image analysis more difficult. We have analyzed the noise characteristics of the HSI color model and developed an adaptive spatial filtering method to reduce the magnitude of noise and the nonuniformity of noise variance in the HSI color space. With this adaptive filtering method, the filter kernel for each pixel is dynamically adjusted, depending on the values of intensity and saturation. In our experiments we have filtered the saturation and hue components and generated edge maps from color gradients. We have found that by using the adaptive filtering method, the minimum error rate in edge detection improves by approximately 15", "label": ["adaptive filtering", "noise reduction", "hue saturation intensity color space", "color image processing", "color image analysis", "rgb color model", "random gaussian noise", "nonuniform noise distribution", "accurate image analysis", "adaptive spatial filtering method", "nonuniformity", "noise variance", "hsi color space", "filter kernel", "pixel", "saturation", "intensity", "generated edge maps", "color gradients", "edge detection", "minimum error rate"], "stemmed_label": ["adapt filter", "nois reduct", "hue satur intens color space", "color imag process", "color imag analysi", "rgb color model", "random gaussian nois", "nonuniform nois distribut", "accur imag analysi", "adapt spatial filter method", "nonuniform", "nois varianc", "hsi color space", "filter kernel", "pixel", "satur", "intens", "gener edg map", "color gradient", "edg detect", "minimum error rate"]}
{"doc": "The effects of emotions on bounded rationality: a comment on Kaufman Bruce Kaufman's article (1999), \"Emotional arousal as a source of bounded rationality\", objective is to present an additional source of bounded rationality, one that is not due to cognitive constraints, but to high emotional arousal. In doing so, Kaufman is following a long tradition of thinkers who have contrasted emotion with reason, claiming, for the most part, that emotions are a violent force hindering rational thinking. This paper aims to challenge Kaufman's unidimensional idea regarding the connection between high emotional arousal and decision making", "label": ["bounded rationality", "decision making", "rational thinking", "psychology", "emotion", "yerkes-dodson law"], "stemmed_label": ["bound ration", "decis make", "ration think", "psycholog", "emot", "yerkes-dodson law"]}
{"doc": "Online masquerade: whose e-mail is it? E-mails carrying viruses like the recent Klez worm use deceptively simple techniques and known vulnerabilities to spread from one computer to another with ease", "label": ["e-mail", "klez worm", "viruses", "vulnerabilities"], "stemmed_label": ["e-mail", "klez worm", "virus", "vulner"]}
{"doc": "Variable structure intelligent control for PM synchronous servo motor drive The variable structure control (VSC) of discrete time systems based on intelligent control is presented in this paper. A novel approach is proposed for the state estimation. A linear observer is firstly designed. Then a neural network is used for compensating uncertainty. The parameter of the VSC scheme is adjusted online by a neural network. Practical operating results from a PM synchronous motor (PMSM) illustrate the effectiveness and practicability of the proposed approach", "label": ["pm synchronous servo motor drive", "variable structure intelligent control", "control design", "discrete time systems", "state estimation", "linear observer", "neural network", "uncertainty compensation", "control performance"], "stemmed_label": ["pm synchron servo motor drive", "variabl structur intellig control", "control design", "discret time system", "state estim", "linear observ", "neural network", "uncertainti compens", "control perform"]}
{"doc": "Real-time enterprise solutions for discrete manufacturing and consumer goods Customer satisfaction and a focus on core competencies have dominated the thinking of a whole host of industries in recent years. However, one outcome, the outsourcing of noncore activities, has made the production of goods-from order entry to final delivery-more and more complex. Suppliers, subsuppliers, producers and customers are therefore busy adopting a new, more collaborative approach. This is mainly taking the form of order-driven planning and scheduling of production, but it is also being steered by a need to reduce inventories and working capital as well as a desire to increase throughput and optimize production", "label": ["real-time enterprise solutions", "discrete manufacturing", "consumer goods", "customer satisfaction", "core competencies", "order-driven planning", "production scheduling", "working capital reduction", "inventories reduction"], "stemmed_label": ["real-tim enterpris solut", "discret manufactur", "consum good", "custom satisfact", "core compet", "order-driven plan", "product schedul", "work capit reduct", "inventori reduct"]}
{"doc": "Generalized confidence sets for a statistically indeterminate random vector A problem is considered for the construction of confidence sets for a random vector, the information on distribution parameters of which is incomplete. To obtain exact estimates and a detailed analysis of the problem, the notion is introduced of a generalized confidence set for a statistically indeterminate random vector. Properties of generalized confidence sets are studied. It is shown that the standard method of estimation, which relies on the unification of confidence sets, leads in many cases to wider confidence estimates. For a normally distributed random vector with an inaccurately known mean value, generalized confidence sets are built tip and the dependence of sizes of a generalized confidence set on the forms and parameters of a set of possible mean values is examined", "label": ["generalized confidence sets", "statistically indeterminate random vector", "distribution parameters", "normally distributed random vector"], "stemmed_label": ["gener confid set", "statist indetermin random vector", "distribut paramet", "normal distribut random vector"]}
{"doc": "Fresh voices, big ideas IBM internship program IBM is matching up computer-science and MBA students with its business managers in an 11-week summer internship program and challenging them to develop innovative technology ideas", "label": ["internship program", "ibm business managers", "mba college students", "computer-science students", "patents"], "stemmed_label": ["internship program", "ibm busi manag", "mba colleg student", "computer-sci student", "patent"]}
{"doc": "Tracking nonparameterized object contours in video We propose a new method for contour tracking in video. The inverted distance transform of the edge map is used as an edge indicator function for contour detection. Using the concept of topographical distance, the watershed segmentation can be formulated as a minimization. This new viewpoint gives a way to combine the results of the watershed algorithm on different surfaces. In particular, our algorithm determines the contour as a combination of the current edge map and the contour, predicted from the tracking result in the previous frame. We also show that the problem of background clutter can be relaxed by taking the object motion into account. The compensation with object motion allows to detect and remove spurious edges in background. The experimental results confirm the expected advantages of the proposed method over the existing approaches", "label": ["contour tracking", "nonparameterized object contours", "edge indicator function", "topographical distance", "watershed segmentation", "minimization", "background clutter", "object motion", "motion analysis", "video", "inverted distance transform", "edge map", "motion estimation", "edge detection"], "stemmed_label": ["contour track", "nonparameter object contour", "edg indic function", "topograph distanc", "watersh segment", "minim", "background clutter", "object motion", "motion analysi", "video", "invert distanc transform", "edg map", "motion estim", "edg detect"]}
{"doc": "ConChat: a context-aware chat program ConChat is a context-aware chat program that enriches electronic communication by providing contextual information and resolving potential semantic conflicts between users.ConChat uses contextual information to improve electronic communication. Using contextual cues, users can infer during a conversation what the other person is doing and what is happening in his or her immediate surroundings. For example, if a user learns that the other person is talking with somebody else or is involved in some urgent activity, he or she knows to expect a slower response. Conversely, if the user learns that the other person is sitting in a meeting directly related to the conversation, he or she then knows to respond more quickly. Also, by informing users about the other person's context and tagging potentially ambiguous chat messages, ConChat explores how context can improve electronic communication by reducing semantic conflicts", "label": ["context-aware chat program", "conchat", "contextual information", "semantic conflicts", "contextual cues"], "stemmed_label": ["context-awar chat program", "conchat", "contextu inform", "semant conflict", "contextu cue"]}
{"doc": "The impact of the Internet on public library use: an analysis of the current consumer market for library and Internet services The potential impact of the Internet on the public's demand for the services and resources of public libraries is an issue of critical importance. The research reported in this article provides baseline data concerning the evolving relationship between the public's use of the library and its use of the Internet. The authors developed a consumer model of the American adult market for information services and resources, segmented by use (or nonuse) of the public library and by access (or lack of access) to, and use (or nonuse) of, the Internet. A national Random Digit Dialing telephone survey collected data to estimate the size of each of six market segments, and to describe their usage choices between the public library and the Internet. The analyses presented in this article provide estimates of the size and demographics of each of the market segments; describe why people are currently using the public library and the Internet; identify the decision criteria people use in their choices of which provider to use; identify areas in which libraries and the Internet appear to be competing and areas in which they appear to be complementary; and identify reasons why people choose not to use the public library and/or the Internet. The data suggest that some differentiation between the library and the Internet is taking place, which may very well have an impact on consumer choices between the two. Longitudinal research is necessary to fully reveal trends in these usage choices, which have implications for all types of libraries in planning and policy development", "label": ["internet", "public libraries", "baseline data", "consumer model", "american adult market", "national random digit dialing telephone survey", "decision criteria", "public library", "longitudinal research"], "stemmed_label": ["internet", "public librari", "baselin data", "consum model", "american adult market", "nation random digit dial telephon survey", "decis criteria", "public librari", "longitudin research"]}
{"doc": "Challenges and trends in discrete manufacturing Over 50 years ago, the 100,000 workers at Ford's Rouge automobile factory turned out 1200 cars per day. Nowadays, Ford's plant on that same site still produces 800 cars each day but with just 3000 workers. Similar stories abound in the manufacturing industries; technology revolution and evolution; a shift from vertical integration, better business and production practices and improved industrial relations-all have changed manufacturing beyond recognition. So what are the current challenges and trends in manufacturing? Certainly, the relentless advance of technology will continue, as will user pressure for more customized design or improved environmental friendliness. Some trends are already with us and more, as yet indiscernible, will come. But one major, fundamental shift now resounding throughout industry is the way in which information involving every single aspect of the manufacturing process is being integrated into one seamless system", "label": ["discrete manufacturing", "challenges", "automobile factory", "technology revolution", "technology evolution", "business practices", "production practices", "industrial relations", "trends", "seamless manufacturing process"], "stemmed_label": ["discret manufactur", "challeng", "automobil factori", "technolog revolut", "technolog evolut", "busi practic", "product practic", "industri relat", "trend", "seamless manufactur process"]}
{"doc": "Organization design: The continuing influence of information technology Drawing from an information processing perspective, this paper examines how information technology (IT) has been a catalyst in the development of new forms of organizational structures. The article draws a historical linkage between the relative stability of an organization's task environment starting after the Second World War to the present environmental instability that now characterizes many industries. Specifically, the authors suggest that advances in IT have enabled managers to adapt existing forms and create new models for organizational design that better fit requirements of an unstable environment. Time has seemingly borne out this hypothesis as the bureaucratic structure evolved to the matrix to the network and now to the emerging shadow structure. IT has gone from a support mechanism to a substitute for organizational structures in the form of the shadow structure. The article suggests that the evolving and expanding role of IT will continue for organizations that face unstable environments", "label": ["organization design", "information processing perspective", "organizational structures", "organization task environment", "environmental instability", "information technology"], "stemmed_label": ["organ design", "inform process perspect", "organiz structur", "organ task environ", "environment instabl", "inform technolog"]}
{"doc": "Power electronics spark new simulation challenges This article discusses some of the changes that have taken place in power systems and explores some of the inherent requirements for simulation technologies in order to keep up with this rapidly changing environment. The authors describe how energy utilities are realizing that, with the appropriate tools, they can train and sustain engineers who can maintain a great insight into system dynamics", "label": ["power system computer simulation", "power electronics", "simulation challenges", "simulation technologies", "electric utilities"], "stemmed_label": ["power system comput simul", "power electron", "simul challeng", "simul technolog", "electr util"]}
{"doc": "Security crisis management - the basics Of the more pervasive problems in any kind of security event is how the security event is managed from the inception to the end. There's a lot written about how to manage a specific incident or how to deal with a point problem such as a firewall log, but little tends to be written about how to deal with the management of a security event as part of corporate crisis management. This article discusses the basics of security crisis management and of the logical steps required to ensure that a security crisis does not get out of hand", "label": ["security crisis management", "security event", "firewall log", "corporate crisis management"], "stemmed_label": ["secur crisi manag", "secur event", "firewal log", "corpor crisi manag"]}
{"doc": "Design and implementation of a 3-D mapping system for highly irregular shaped objects with application to semiconductor manufacturing The basic technology for a robotic system is developed to automate the packing of polycrystalline silicon nuggets into fragile fused silica crucible in Czochralski (melt pulling) semiconductor wafer production. The highly irregular shapes of the nuggets and the packing constraints make this a difficult and challenging task. It requires the delicate manipulation and packing of highly irregular polycrystalline silicon nuggets into a fragile fused silica crucible. For this application, a dual optical 3-D surface mapping system that uses active laser triangulation has been developed and successfully tested. One part of the system measures the geometry profile of a nugget being packed and the other the profile of the nuggets already in the crucible. A resolution of 1 mm with 15-KHz sampling frequency is achieved. Data from the system are used by the packing algorithm, which determines optimal nugget placement. The key contribution is to describe the design and implementation of an efficient and robust 3-D imaging system to map highly irregular shaped objects using conventional components in context of real commercial manufacturing processes", "label": ["3d mapping system", "highly irregular shaped objects", "semiconductor manufacturing", "robotic system", "polycrystalline silicon nuggets", "fragile fused silica crucible", "sampling frequency", "packing algorithm", "optical nugget placement", "robust 3-d imaging system", "irregular shaped objects", "commercial manufacturing processes", "czochralski semiconductor wafer production", "dual optical 3d surface mapping system", "highly irregular polycrystalline silicon nuggets", "active laser triangulation"], "stemmed_label": ["3d map system", "highli irregular shape object", "semiconductor manufactur", "robot system", "polycrystallin silicon nugget", "fragil fuse silica crucibl", "sampl frequenc", "pack algorithm", "optic nugget placement", "robust 3-d imag system", "irregular shape object", "commerci manufactur process", "czochralski semiconductor wafer product", "dual optic 3d surfac map system", "highli irregular polycrystallin silicon nugget", "activ laser triangul"]}
{"doc": "All-optical logic NOR gate using two-cascaded semiconductor optical amplifiers The authors present a novel all-optical logic NOR gate using two-cascaded semiconductor optical. amplifiers (SOAs) in a counterpropagating feedback configuration. This configuration accentuates the gain nonlinearity due to the mutual gain modulation of the two SOAs. The all-optical NOR gate feasibility has been demonstrated delivering an extinction ratio higher than 12 dB over a wide range of wavelength", "label": ["all-optical logic nor gate", "two-cascaded semiconductor optical amplifiers", "soa", "counterpropagating feedback configuration", "gain nonlinearity", "mutual gain modulation", "extinction ratio", "wide wavelength range"], "stemmed_label": ["all-opt logic nor gate", "two-cascad semiconductor optic amplifi", "soa", "counterpropag feedback configur", "gain nonlinear", "mutual gain modul", "extinct ratio", "wide wavelength rang"]}
{"doc": "Designing and delivering a university course - a process (or operations) management perspective With over 30 years of academic experience in both engineering and management faculties, involving trial and error experimentation in teaching as well as reading relevant literature and observing other instructors in action, the author has accumulated a number of ideas, regarding the preparation and delivery of a university course, that should be of interest to other instructors. This should be particularly the case for those individuals who have had little or no teaching experience (e.g. those whose graduate education was recently completed at research-oriented institutions providing little guidance with respect to teaching). A particular perspective is used to convey the ideas, namely one of viewing the preparation and delivery of a course as two major processes that should provide outputs or outcomes that are of value to a number of customers, in particular, students", "label": ["university course delivery", "management perspective", "academic experience", "management faculties", "engineering faculties", "search-oriented institutions"], "stemmed_label": ["univers cours deliveri", "manag perspect", "academ experi", "manag faculti", "engin faculti", "search-ori institut"]}
{"doc": "Naomi Campbell: drugs, distress and the Data Protection Act In the first case of its kind, Naomi Campbell successfully sued Mirror Group Newspapers for damage and distress caused by breach of the Data Protection Act 1998. Partner N. Wildish and assistant M. Turle of City law firm Field Fisher Waterhouse discuss the case and the legal implications of which online publishers should be aware", "label": ["drugs", "distress", "data protection act", "naomi campbell", "online publishers"], "stemmed_label": ["drug", "distress", "data protect act", "naomi campbel", "onlin publish"]}
{"doc": "Simulation of evacuation processes using a bionics-inspired cellular automaton model for pedestrian dynamics We present simulations of evacuation processes using a recently introduced cellular automaton model for pedestrian dynamics. This model applies a bionics approach to describe the interaction between the pedestrians using ideas from chemotaxis. Here we study a rather simple situation, namely the evacuation from a large room with one or two doors. It is shown that the variation of the model parameters allows to describe different types of behaviour, from regular to panic. We find a non-monotonic dependence of the evacuation times on the coupling constants. These times depend on the strength of the herding behaviour, with minimal evacuation times for some intermediate values of the couplings, i.e., a proper combination of herding and use of knowledge about the shortest way to the exit", "label": ["evacuation processes simulation", "chemotaxis", "nonmonotonic dependence", "coupling constants", "herding behaviour", "bionics-inspired cellular automaton model", "pedestrian dynamics"], "stemmed_label": ["evacu process simul", "chemotaxi", "nonmonoton depend", "coupl constant", "herd behaviour", "bionics-inspir cellular automaton model", "pedestrian dynam"]}
{"doc": "A new subspace identification approach based on principal component analysis Principal component analysis (PCA) has been widely used for monitoring complex industrial processes with multiple variables and diagnosing process and sensor faults. The objective of this paper is to develop a new subspace identification algorithm that gives consistent model estimates under the errors-in-variables (EIV) situation. In this paper, we propose a new subspace identification approach using principal component analysis. PCA naturally falls into the category of EIV formulation, which resembles total least squares and allows for errors in both process input and output. We propose to use PCA to determine the system observability subspace, the matrices and the system order for an EIV formulation. Standard PCA is modified with instrumental variables in order to achieve consistent estimates of the system matrices. The proposed subspace identification method is demonstrated using a simulated process and a real industrial process for model identification and order determination. For comparison the MOESP algorithm and N4SID algorithm are used as benchmarks to demonstrate the advantages of the proposed PCA based subspace model identification (SMI) algorithm", "label": ["subspace identification approach", "principal component analysis", "pca", "complex industrial process monitoring", "process fault diagnosis", "sensor fault diagnosis", "errors-in-variables situation", "eiv situation", "total least-squares approximation", "system observability subspace", "consistent system matrix estimates", "moesp algorithm", "n4sid algorithm", "subspace model identification", "smi"], "stemmed_label": ["subspac identif approach", "princip compon analysi", "pca", "complex industri process monitor", "process fault diagnosi", "sensor fault diagnosi", "errors-in-vari situat", "eiv situat", "total least-squar approxim", "system observ subspac", "consist system matrix estim", "moesp algorithm", "n4sid algorithm", "subspac model identif", "smi"]}
{"doc": "Causes of the decline of the business school management science course The business school management science course is suffering serious decline. The traditional model- and algorithm-based course fails to meet the needs of MBA programs and students. Poor student mathematical preparation is a reality, and is not an acceptable justification for poor teaching outcomes. Management science Ph.D.s are often poorly prepared to teach in a general management program, having more experience and interest in algorithms than management. The management science profession as a whole has focused its attention on algorithms and a narrow subset of management problems for which they are most applicable. In contrast, MBA's rarely encounter problems that are suitable for straightforward application of management science tools, living instead in a world where problems are ill-defined, data is scarce, time is short, politics is dominant, and rational \"decision makers\" are non-existent. The root cause of the profession's failure to address these issues seems to be (in Russell Ackoff's words) a habit of professional introversion that caused the profession to be uninterested in what MBA's really do on the job and how management science can help them", "label": ["business school management science course", "mba programs", "mba students", "management science", "profession"], "stemmed_label": ["busi school manag scienc cours", "mba program", "mba student", "manag scienc", "profess"]}
{"doc": "Computational capacity of an odorant discriminator: the linear separability of curves We introduce and study an artificial neural network inspired by the probabilistic receptor affinity distribution model of olfaction. Our system consists of N sensory neurons whose outputs converge on a single processing linear threshold element. The system's aim is to model discrimination of a single target odorant from a large number of background odorants within a range of odorant concentrations. We show that this is possible provided does not exceed a critical value p/sub c/ and calculate the critical capacity alpha c=p/sub c//N. The critical capacity depends on the range of concentrations in which the discrimination is to be accomplished. If the olfactory bulb may be thought of as a collection of such processing elements, each responsible for the discrimination of a single odorant, our study provides a quantitative analysis of the potential computational properties of the olfactory bulb. The mathematical formulation of the problem we consider is one of determining the capacity for linear separability of continuous curves, embedded in a large-dimensional space. This is accomplished here by a numerical study, using a method that signals whether the discrimination task is realizable, together with a finite-size scaling analysis", "label": ["artificial neural network", "receptor affinity distribution", "olfaction", "linear threshold element", "sensory neurons", "linear separability", "odorant discriminator"], "stemmed_label": ["artifici neural network", "receptor affin distribut", "olfact", "linear threshold element", "sensori neuron", "linear separ", "odor discrimin"]}
{"doc": "Restoration of broadband imagery steered with a liquid-crystal optical phased array In many imaging applications, it is highly desirable to replace mechanical beam-steering components (i.e., mirrors and gimbals) with a nonmechanical device. One such device is a nematic liquid crystal optical phased array (LCOPA). An LCOPA can implement a blazed phase grating to steer the incident light. However, when a phase grating is used in a broadband imaging system, two adverse effects can occur. First, dispersion will cause different incident wavelengths arriving at the same angle to be steered to different output angles, causing chromatic aberrations in the image plane. Second, the device will steer energy not only to the first diffraction order, but to others as well. This multiple-order effect results in multiple copies of the scene appearing in the image plane. We describe a digital image restoration technique designed to overcome these degradations. The proposed postprocessing technique is based on a Wiener deconvolution filter. The technique, however, is applicable only to scenes containing objects with approximately constant reflectivities over the spectral region of interest. Experimental results are presented to demonstrate the effectiveness of this technique", "label": ["broadband imagery", "liquid-crystal optical phased array steering", "imaging applications", "mechanical beam-steering components", "mirrors", "gimbals", "nonmechanical device", "nematic liquid crystal optical phased array", "blazed phase grating", "incident light steering", "broadband imaging system", "dispersion", "incident wavelengths", "output angles", "chromatic aberrations", "image plane", "optical phased array", "first diffraction order", "multiple-order effect", "halogen lamp", "multiple copies", "digital image restoration technique", "postprocessing technique", "wiener deconvolution filter", "approximately constant reflectivities", "spectral region of interest"], "stemmed_label": ["broadband imageri", "liquid-cryst optic phase array steer", "imag applic", "mechan beam-steer compon", "mirror", "gimbal", "nonmechan devic", "nemat liquid crystal optic phase array", "blaze phase grate", "incid light steer", "broadband imag system", "dispers", "incid wavelength", "output angl", "chromat aberr", "imag plane", "optic phase array", "first diffract order", "multiple-ord effect", "halogen lamp", "multipl copi", "digit imag restor techniqu", "postprocess techniqu", "wiener deconvolut filter", "approxim constant reflect", "spectral region of interest"]}
{"doc": "Nonlinear modeling and adaptive fuzzy control of MCFC stack To improve availability and performance of fuel cells, the operating temperature of the molten carbonate fuel cells (MCFC) stack should be controlled within a specified range. However, most existing models of MCFC are not ready to be applied in synthesis. In the paper, a radial basis function neural networks identification model of a MCFC stack is developed based on the input-output sampled data. An adaptive fuzzy control procedure for the temperature of the MCFC stack is also developed. The parameters of the fuzzy control system are regulated by back-propagation algorithm, and the rule database of the fuzzy system is also adaptively adjusted by the nearest-neighbor-clustering algorithm. Finally using the neural networks model of MCFC stack, the simulation results of the control algorithm are presented. The results show the effectiveness of the proposed modeling and design procedures for the MCFC stack based on neural networks identification and the novel adaptive fuzzy control", "label": ["nonlinear modeling", "adaptive fuzzy control", "mcfc stack", "fuel cells", "molten carbonate fuel cells stack", "radial basis function neural networks identification model", "input-output sampled data", "backpropagation algorithm", "rule database", "nearest-neighbor-clustering algorithm"], "stemmed_label": ["nonlinear model", "adapt fuzzi control", "mcfc stack", "fuel cell", "molten carbon fuel cell stack", "radial basi function neural network identif model", "input-output sampl data", "backpropag algorithm", "rule databas", "nearest-neighbor-clust algorithm"]}
{"doc": "Analysis and efficient implementation of a linguistic fuzzy c-means The paper is concerned with a linguistic fuzzy c-means (FCM) algorithm with vectors of fuzzy numbers as inputs. This algorithm is based on the extension principle and the decomposition theorem. It turns out that using the extension principle to extend the capability of the standard membership update equation to deal with a linguistic vector has a huge computational complexity. In order to cope with this problem, an efficient method based on fuzzy arithmetic and optimization has been developed and analyzed. We also carefully examine and prove that the algorithm behaves in a way similar to the FCM in the degenerate linguistic case. Synthetic data sets and the iris data set have been used to illustrate the behavior of this linguistic version of the FCM", "label": ["linguistic fuzzy c-means algorithm", "fuzzy numbers", "extension principle", "decomposition theorem", "computational complexity", "fuzzy arithmetic", "optimization", "linguistic vectors"], "stemmed_label": ["linguist fuzzi c-mean algorithm", "fuzzi number", "extens principl", "decomposit theorem", "comput complex", "fuzzi arithmet", "optim", "linguist vector"]}
{"doc": "Robust fuzzy controlled photovoltaic power inverter with Taguchi method This paper presents design and implementation of a robust fuzzy controlled photovoltaic (PV) power inverter with Taguchi tuned scaling factors. To achieve fast transient response, small steady-state error and system robustness, a robust fuzzy controller is adopted, in which its input and output scaling factors are determined efficiently by using the Taguchi-tuning algorithm. The proposed system can operate in different modes, grid-connection mode and stand-alone mode, and can accommodate wide load variations. Simulation results and hardware measurements obtained from a prototype with a microcontroller (Intel 80196KC) are presented to verify the theoretical discussions, and its adaptivity, robustness and feasibility", "label": ["robust fuzzy controlled photovoltaic power inverter", "taguchi method", "tuned scaling factors", "transient response", "steady-state error", "system robustness", "output scaling factors", "grid-connection mode", "stand-alone mode", "load variations", "microcontroller", "adaptivity", "feasibility"], "stemmed_label": ["robust fuzzi control photovolta power invert", "taguchi method", "tune scale factor", "transient respons", "steady-st error", "system robust", "output scale factor", "grid-connect mode", "stand-alon mode", "load variat", "microcontrol", "adapt", "feasibl"]}
{"doc": "Descriptological foundations of programming Descriptological foundations of programming are constructed. An explication of the concept of a descriptive process is given. The operations of introduction and elimination of abstraction at the level of processes are refined. An intensional concept of a bipolar function is introduced. An explication of the concept of introduction and extraction of abstraction at the bipole level is given. On this basis, a complete set of descriptological operations is constructed", "label": ["descriptological foundations", "programming", "descriptive process", "intensional concept", "bipolar function", "bipole level"], "stemmed_label": ["descriptolog foundat", "program", "descript process", "intension concept", "bipolar function", "bipol level"]}
{"doc": "Academic libraries and community: making the connection I explore the theme of academic libraries serving and reaching out to the broader community. I highlight interesting projects reported on in the literature (such as the Through Our Parents' Eyes project) and report on others. I look at challenges to community partnerships and recommendations for making them succeed. Although I focus on links with the broader community, I also took at methods for increasing cooperation among various units on campus, so that the needs of campus community groups-such as distance education students or disabled students-are effectively addressed. Though academic libraries are my focus, we can learn a lot from the community building efforts of public libraries", "label": ["academic libraries", "community partnerships", "campus community groups", "distance education students", "disabled students", "public libraries"], "stemmed_label": ["academ librari", "commun partnership", "campu commun group", "distanc educ student", "disabl student", "public librari"]}
{"doc": "Designing a screening experiment for highly reliable products Within a reasonable life-testing time, how to improve the reliability of highly reliable products is one of the great challenges. By using a resolution III experiment together with degradation test, Tseng et al. (1995) presented a case study of improving the reliability of fluorescent lamps. However, in conducting such an experiment, they did not address the problem of how to choose the optimal settings of variables, such as sample size, inspection frequency, and termination time for each run, which are influential to the correct identification of significant factors and the experimental cost. Assuming that the product's degradation paths satisfy Wiener processes, this paper proposes a systematic approach to the aforementioned problem. First, an identification rule is proposed. Next, under the constraints of a minimum probability of correct decision and a maximum probability of incorrect decision of the proposed identification rule, the optimum test plan can be obtained by minimizing the total experimental cost. An example is provided to illustrate the proposed method", "label": ["screening experiment", "highly reliable products", "resolution iii design", "degradation tests", "wiener process", "inspection frequency", "termination time", "optimal test plan", "fluorescent lamps", "minimum probability of correct decision", "maximum probability of incorrect decision", "identification rule"], "stemmed_label": ["screen experi", "highli reliabl product", "resolut iii design", "degrad test", "wiener process", "inspect frequenc", "termin time", "optim test plan", "fluoresc lamp", "minimum probabl of correct decis", "maximum probabl of incorrect decis", "identif rule"]}
{"doc": "Assessment of the macrocyclic effect for the complexation of crown-ethers with alkali cations using the substructural molecular fragments method The Substructural Molecular Fragments method (Solov'ev, V. P.; Varnek, A. A.; Wipff, G. J. Chem. Inf. Comput. Sci. 2000, 40, 847-858) was applied to assess stability constants (logK) of the complexes of crown-ethers, polyethers, and glymes with Na/sup +/, K/sup +/, and Cs/sup +/ in methanol. One hundred forty-seven computational models including different fragment sets coupled with linear or nonlinear fitting equations were applied for the data sets containing 69 (Na/sup +/), 123 (K/sup +/), and 31 (Cs/sup +/) compounds. To account for the \"macrocyclic effect\" for crown-ethers, an additional \"cyclicity\" descriptor was used. \"Predicted\" stability constants both for macrocyclic compounds and for their open-chain analogues are in good agreement with the experimental data reported earlier and with those studied experimentally in this work. The macrocyclic effect as a function of cation and ligand is quantitatively estimated for all studied crown-ethers", "label": ["substructural molecular fragments method", "stability constants", "complexation", "crown-ethers", "alkali cations", "macrocyclic effect", "computational models", "different fragment sets", "nonlinear fitting equations", "linear fitting equations", "cyclicity descriptor", "open-chain analogues", "data mining", "structure-property tool", "molecular graph decomposition", "quantitative structure-properties relationship", "augmented atom", "trail program", "statistical parameters", "thermodynamic parameters"], "stemmed_label": ["substructur molecular fragment method", "stabil constant", "complex", "crown-eth", "alkali cation", "macrocycl effect", "comput model", "differ fragment set", "nonlinear fit equat", "linear fit equat", "cyclic descriptor", "open-chain analogu", "data mine", "structure-properti tool", "molecular graph decomposit", "quantit structure-properti relationship", "augment atom", "trail program", "statist paramet", "thermodynam paramet"]}
{"doc": "Fault-tolerant Hamiltonian laceability of hypercubes It is known that every hypercube Q/sub n/ is a bipartite graph. Assume that n or=2 and F is a subset of edges with |F| or=n-2. We prove that there exists a Hamiltonian path in Q/sub n/-F between any two vertices of different partite sets. Moreover, there exists a path of length 2/sup n/-2 between any two vertices of the same partite set. Assume that n or=3 and F is a subset of edges with |F| or=n-3. We prove that there exists a Hamiltonian path in Q/sub n/- v -F between any two vertices in the partite set without v. Furthermore, all bounds are tight", "label": ["fault-tolerant hamiltonian laceability", "hypercubes", "bipartite graph", "edge subset", "hamiltonian path", "vertices", "partite sets", "tight bounds"], "stemmed_label": ["fault-toler hamiltonian laceabl", "hypercub", "bipartit graph", "edg subset", "hamiltonian path", "vertic", "partit set", "tight bound"]}
{"doc": "A multimodal data collection tool using REALbasic and Mac OS X This project uses REALbasic 3.5 in the Mac OS X environment for development of a configuration tool that builds a data collection procedure for investigating the effectiveness of sonified graphs. The advantage of using REALbasic with the Mac OS X system is that it provides rapid development of stimulus presentation, direct recording of data to files, and control over other procedural issues. The program can be made to run natively on the new Mac OS X system, older Mac OS systems, and Windows (98SE, ME, 2000 PRO). With modification, similar programs could be used to present any number of visual/auditory stimulus combinations, complete with questions for each stimulus", "label": ["multimodal data collection tool", "realbasic", "mac os x environment", "configuration tool", "data collection", "sonified graphs", "visual data comprehension", "psychology", "visual stimulus", "auditory stimulus", "stimulus presentation", "direct data recording", "windows"], "stemmed_label": ["multimod data collect tool", "realbas", "mac os x environ", "configur tool", "data collect", "sonifi graph", "visual data comprehens", "psycholog", "visual stimulu", "auditori stimulu", "stimulu present", "direct data record", "window"]}
{"doc": "Centroid detection based on optical correlation We propose three correlation-based methods to simultaneously detect the centroids of multiple objects in an input scene. The first method is based on the modulus of the moment function, the second method is based on squaring the moment function, and the third method works with a single intensity filter. These methods are invariant to changes in the position, orientation, and scale of the object and result in good noise-smoothing performance. We use spatial light modulators (SLMs) to directly implement the input of the image and filter information for the purpose of these approaches. We present results showing simulations from different approaches and provide comparisons between optical-correlation- and digital-moment-based methods. Experimental results corresponding to an optical correlator using SLMs for the centroid detection are also presented", "label": ["optical correlation", "centroid detection", "correlation-based methods", "centroids", "multiple objects", "input scene", "moment function modulus", "moment function squaring", "single intensity filter", "position", "orientation", "scale", "noise-smoothing performance", "spatial light modulators", "digital-moment-based methods", "optical correlator"], "stemmed_label": ["optic correl", "centroid detect", "correlation-bas method", "centroid", "multipl object", "input scene", "moment function modulu", "moment function squar", "singl intens filter", "posit", "orient", "scale", "noise-smooth perform", "spatial light modul", "digital-moment-bas method", "optic correl"]}
{"doc": "Data storage: re-format. Closely tracking a fast-moving sector In the past few years the data center market has changed dramatically, forcing many companies into consolidation or bankruptcy. Gone are the days when companies raised millions of dollars to acquire large industrial buildings and transform them into glittering, high-tech palaces filled with the latest telecommunication and data technology. Whereas manufacturers of communication technology deliver the racked equipment in these, often mission-critical, facilities, ABB focuses mainly on the building infrastructure. Besides the very important redundant power supply, ABB also provides the redundant air conditioning and the security system", "label": ["building management", "data centers", "building infrastructure", "mission-critical facilities", "abb", "engineering management", "project management", "installation", "commissioning", "redundant power supply", "redundant air conditioning", "security system"], "stemmed_label": ["build manag", "data center", "build infrastructur", "mission-crit facil", "abb", "engin manag", "project manag", "instal", "commiss", "redund power suppli", "redund air condit", "secur system"]}
{"doc": "The service side of systems librarianship Describes the role of a systems librarian at a small academic library. Although online catalogs and the Internet are making library accessibility more convenient, the need for library buildings and professionals has not diminished. Typical duties of a systems librarian and the effects of new technology on librarianship are discussed. Services provided to other constituencies on campus and the blurring relationship between the library and computer services are also presented", "label": ["systems librarianship", "service side", "small academic library", "online catalogs", "internet"], "stemmed_label": ["system librarianship", "servic side", "small academ librari", "onlin catalog", "internet"]}
{"doc": "Plenoptic image editing This paper presents a new class of interactive image editing operations designed to maintain consistency between multiple images of a physical 3D scene. The distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the (unknown) 3D scene had itself been modified. The modified scene can then be viewed interactively from any other camera viewpoint and under different scene illuminations. The approach is useful first as a power-assist that enables a user to quickly modify many images by editing just a few, and second as a means for constructing and editing image-based scene representations by manipulating a set of photographs. The approach works by extending operations like image painting, scissoring, and morphing so that they alter a scene's plenoptic function in a physically-consistent way, thereby affecting scene appearance from all viewpoints simultaneously. A key element in realizing these operations is a new volumetric decomposition technique for reconstructing an scene's plenoptic function from an incomplete set of camera viewpoints", "label": ["interactive image editing operations", "multiple images", "physical 3d scene", "modified scene", "camera viewpoint", "image-based scene representations", "image painting", "scissoring", "morphing", "plenoptic function", "volumetric decomposition technique", "plenoptic image editing"], "stemmed_label": ["interact imag edit oper", "multipl imag", "physic 3d scene", "modifi scene", "camera viewpoint", "image-bas scene represent", "imag paint", "scissor", "morph", "plenopt function", "volumetr decomposit techniqu", "plenopt imag edit"]}
{"doc": "Don't always believe what you Reed optimisation techniques for Web sites and trade mark infringement On 20 May 2002, Mr Justice Pumfrey gave judgment in the case of (1) Reed Executive Plc (2) Reed Solutions Plc versus (1) Reed Business Information Limited (2) Reed Elsevier (UK) Limited (3) totaljobs.com Limited. The case explored for the first time in any detail the extent to which the use of various optimisation techniques for Web sites could give rise to new forms of trade mark infringement and passing off. The author reports on the case and offers his comments", "label": ["reed executive plc", "reed solutions plc", "reed business information limited", "reed elsevier (uk) limited", "totaljobs.com limited", "optimisation techniques", "web sites", "trade mark infringement", "passing off"], "stemmed_label": ["reed execut plc", "reed solut plc", "reed busi inform limit", "reed elsevi (uk) limit", "totaljobs.com limit", "optimis techniqu", "web site", "trade mark infring", "pass off"]}
{"doc": "Lung metastasis detection and visualization on CT images: a knowledge-based method A solution to the problem of lung metastasis detection on computed tomography (CT) scans of the thorax is presented. A knowledge-based top-down approach for image interpretation is used. The method is inspired by the manner in which a radiologist and radiotherapist interpret CT images before radiotherapy is planned. A two-dimensional followed by a three-dimensional analysis is performed. The algorithm first detects the thorax contour, the lungs and the ribs, which further help the detection of metastases. Thus, two types of tumors are detected: nodules and metastases located at the lung extremities. A method to visualize the anatomical structures segmented is also presented. The system was tested on 20 patients (988 total images) from the Oncology Department of La Chaux-de-Fonds Hospital and the results show that the method is reliable as a computer-aided diagnostic tool for clinical purpose in an oncology department", "label": ["lung metastasis detection", "data visualization", "ct images", "computed tomography", "knowledge-based top-down approach", "two-dimensional analysis", "three-dimensional analysis", "computer-aided diagnostic tool", "oncology", "medical imaging", "knowledge representation", "thorax", "image interpretation"], "stemmed_label": ["lung metastasi detect", "data visual", "ct imag", "comput tomographi", "knowledge-bas top-down approach", "two-dimension analysi", "three-dimension analysi", "computer-aid diagnost tool", "oncolog", "medic imag", "knowledg represent", "thorax", "imag interpret"]}
{"doc": "Using the Small Business Innovation Research Program to turn your ideas into products The US Government's Small Business Innovation Research Program helps small businesses transform new ideas into commercial products. The program provides an ideal means for businesses and universities to obtaining funding for cooperative projects. Rules and information for the program are readily available, and I will give a few helpful hints to provide guidance", "label": ["small business innovation research program", "commercial product development", "businesses", "universities", "funding", "cooperative projects", "us government", "usa"], "stemmed_label": ["small busi innov research program", "commerci product develop", "busi", "univers", "fund", "cooper project", "us govern", "usa"]}
{"doc": "Combining spatial and scale-space techniques for edge detection to provide a spatially adaptive wavelet-based noise filtering algorithm New methods for detecting edges in an image using spatial and scale-space domains are proposed. A priori knowledge about geometrical characteristics of edges is used to assign a probability factor to the chance of any pixel being on an edge. An improved double thresholding technique is introduced for spatial domain filtering. Probabilities that pixels belong to a given edge are assigned based on pixel similarity across gradient amplitudes, gradient phases and edge connectivity. The scale-space approach uses dynamic range compression to allow wavelet correlation over a wider range of scales. A probabilistic formulation is used to combine the results obtained from filtering in each domain to provide a final edge probability image which has the advantages of both spatial and scale-space domain methods. Decomposing this edge probability image with the same wavelet as the original image permits the generation of adaptive filters that can recognize the characteristics of the edges in all wavelet detail and approximation images regardless of scale. These matched filters permit significant reduction in image noise without contributing to edge distortion. The spatially adaptive wavelet noise-filtering algorithm is qualitatively and quantitatively compared to a frequency domain and two wavelet based noise suppression algorithms using both natural and computer generated noisy images", "label": ["spatial techniques", "scale-space techniques", "edge detection", "spatially adaptive wavelet-based noise filtering algorithm", "a priori knowledge", "geometrical characteristics", "probability factor", "double thresholding technique", "spatial domain filtering", "pixel similarity", "gradient amplitudes", "gradient phases", "edge connectivity", "dynamic range compression", "wavelet correlation", "probabilistic formulation", "final edge probability image", "adaptive filters", "approximation images", "matched filters", "image noise", "spatially adaptive wavelet noise-filtering algorithm", "noise suppression"], "stemmed_label": ["spatial techniqu", "scale-spac techniqu", "edg detect", "spatial adapt wavelet-bas nois filter algorithm", "a priori knowledg", "geometr characterist", "probabl factor", "doubl threshold techniqu", "spatial domain filter", "pixel similar", "gradient amplitud", "gradient phase", "edg connect", "dynam rang compress", "wavelet correl", "probabilist formul", "final edg probabl imag", "adapt filter", "approxim imag", "match filter", "imag nois", "spatial adapt wavelet noise-filt algorithm", "nois suppress"]}
{"doc": "Improving the predicting power of partial order based QSARs through linear extensions Partial order theory (POT) is an attractive and operationally simple method that allows ordering of compounds, based on selected structural and/or electronic descriptors (modeled order), or based on their end points, e.g., solubility (experimental order). If the modeled order resembles the experimental order, compounds that are not experimentally investigated can be assigned a position in the model that eventually might lead to a prediction of an end-point value. However, in the application of POT in quantitative structure-activity relationship modeling, only the compounds directly comparable to the noninvestigated compounds are applied. To explore the possibilities of improving the methodology, the theory is extended by application of the so-called linear extensions of the model order. The study show that partial ordering combined with linear extensions appears as a promising tool providing probability distribution curves in the range of possible end-point values for compounds not being experimentally investigated", "label": ["quantitative structure-activity relationships", "partial order theory", "predicting power improvement", "linear extensions", "structural descriptors", "electronic descriptors", "modeled order", "end points", "graphical representation", "combinatorial rule", "most probable linear order", "partially ordered set", "hasse diagram", "solubilities", "organic compounds"], "stemmed_label": ["quantit structure-act relationship", "partial order theori", "predict power improv", "linear extens", "structur descriptor", "electron descriptor", "model order", "end point", "graphic represent", "combinatori rule", "most probabl linear order", "partial order set", "hass diagram", "solubl", "organ compound"]}
{"doc": "Outlier resistant adaptive matched filtering Robust adaptive matched filtering (AMF) whereby outlier data vectors are censored from the covariance matrix estimate is considered in a maximum likelihood estimation (MLE) setting. It is known that outlier data vectors whose steering vector is highly correlated with the desired steering vector, can significantly degrade the performance of AMF algorithms such as sample matrix inversion (SMI) or fast maximum likelihood (FML). Four new algorithms that censor outliers are presented which are derived via approximation to the MLE solution. Two algorithms each are related to using the SMI or the FML to estimate the unknown underlying covariance matrix. Results are presented using computer simulations which demonstrate the relative effectiveness of the four algorithms versus each other and also versus the SMI and FML algorithms in the presence of outliers and no outliers. It is shown that one of the censoring algorithms, called the reiterative censored fast maximum likelihood (CFML) technique is significantly superior to the other three censoring methods in stressful outlier scenarios", "label": ["outlier resistant adaptive matched filtering", "covariance matrix estimate", "maximum likelihood estimation setting", "steering vector", "sample matrix inversion", "fast maximum likelihood", "censoring algorithms", "reiterative censored fast maximum likelihood"], "stemmed_label": ["outlier resist adapt match filter", "covari matrix estim", "maximum likelihood estim set", "steer vector", "sampl matrix invers", "fast maximum likelihood", "censor algorithm", "reiter censor fast maximum likelihood"]}
{"doc": "Verifying resonant grounding in distribution systems The authors describe RESFAL, a software tool that can check on the behavior of distribution network resonant grounding systems with regard to compensation coil tuning and to fault detection", "label": ["resfal software tool", "resonant grounding systems", "compensation coil tuning", "fault detection", "computer simulation", "power distribution systems"], "stemmed_label": ["resfal softwar tool", "reson ground system", "compens coil tune", "fault detect", "comput simul", "power distribut system"]}
{"doc": "Novel denoising algorithm for obtaining a superresolved position estimation We present a new algorithm that uses the randomness of the noise pattern to achieve high positioning accuracy by applying a modified averaging operation. Using the suggested approach, noise sensitivity of the positioning accuracy can be significantly reduced. This new improved algorithm can improve the performances of tracking systems used for military as well as civil applications. The concept is demonstrated theoretically as well as by optical experiment", "label": ["denoising algorithm", "superresolved position estimation", "noise pattern randomness", "high positioning accuracy", "modified averaging operation", "noise sensitivity", "tracking systems", "military applications", "civil applications", "optical experiment"], "stemmed_label": ["denois algorithm", "superresolv posit estim", "nois pattern random", "high posit accuraci", "modifi averag oper", "nois sensit", "track system", "militari applic", "civil applic", "optic experi"]}
{"doc": "Waiting for the wave to crest wavelength services Wavelength services have been hyped ad nauseam for years. But despite their quick turn-up time and impressive margins, such services have yet to live up to the industry's expectations. The reasons for this lukewarm reception are many, not the least of which is the confusion that still surrounds the technology, but most industry observers are still convinced that wavelength services with ultimately flourish", "label": ["wavelength services", "fiber optic networks", "looking glass networks", "pointeast research"], "stemmed_label": ["wavelength servic", "fiber optic network", "look glass network", "pointeast research"]}
{"doc": "The effects of asynchronous computer-mediated group interaction on group processes This article reports a study undertaken to investigate some of the social psychological processes underlying computer-supported group discussion in natural computer-mediated contexts. Based on the concept of deindividuation, it was hypothesized that personal identifiability and group identity would be important factors that affect the perceptions and behavior of members of computer-mediated groups. The degree of personal identifiability and the strength of group identity were manipulated across groups of geographically dispersed computer users who took part in e-mail discussions during a 2-week period. The results do not support the association between deindividuation and uninhibited behavior cited in much previous research. Instead, the data provide some support for a social identity perspective of computer-mediated communication, which explains the higher levels uninhibited in identifiable computer-mediated groups. However, predictions based on social identity theory regarding group polarization and group cohesion were not supported. Possible explanations for this are discussed and further research is suggested to resolve these discrepancies", "label": ["asynchronous computer-mediated group interaction", "group processes", "social issues", "psychology", "deindividuation", "internet", "personal identifiability", "group identity", "geographically dispersed computer users", "e-mail discussions", "social identity theory", "group polarization", "group cohesion"], "stemmed_label": ["asynchron computer-medi group interact", "group process", "social issu", "psycholog", "deindividu", "internet", "person identifi", "group ident", "geograph dispers comput user", "e-mail discuss", "social ident theori", "group polar", "group cohes"]}
{"doc": "Relevance of Web documents: ghosts consensus method The dominant method currently used to improve the quality of Internet search systems is often called \"digital democracy.\" Such an approach implies the utilization of the majority opinion of Internet users to determine the most relevant documents: for example, citation index usage for sorting of search results (google.com) or an enrichment of a query with terms that are asked frequently in relation with the query's theme. \"Digital democracy\" is an effective instrument in many cases, but it has an unavoidable shortcoming, which is a matter of principle: the average intellectual and cultural level of Internet users is very low; everyone knows what kind of information is dominant in Internet query statistics. Therefore, when one searches the Internet by means of \"digital democracy\" systems, one gets answers that reflect an underlying assumption that the user's mind potential is very low, and that his cultural interests are not demanding. Thus, it is more correct to use the term \"digital ochlocracy\" to refer to Internet search systems with \"digital democracy.\" Based on the well-known mathematical mechanism of linear programming, we propose a method to solve the indicated problem", "label": ["internet search systems", "digital democracy", "majority opinion", "citation index usage", "search results", "internet query statistics", "digital ochlocracy", "linear programming", "ghosts consensus method", "world wide web"], "stemmed_label": ["internet search system", "digit democraci", "major opinion", "citat index usag", "search result", "internet queri statist", "digit ochlocraci", "linear program", "ghost consensu method", "world wide web"]}
{"doc": "Fault-tolerant computer-aided control systems with multiversion-threshold adaptation: adaptation methods, reliability estimation, and choice of an architecture For multiversion majority-redundant computer-aided control systems, systematization of adaptation methods that are stable to hardware and software failures, a method for estimating their reliability from an event graph model, and a method for selecting a standard architecture with regard for reliability requirements are studied", "label": ["fault-tolerant computer-aided control systems", "multiversion-threshold adaptation", "reliability estimation", "architecture", "multiversion majority-redundant computer-aided control systems", "hardware failure stability", "software failure stability", "event graph model"], "stemmed_label": ["fault-toler computer-aid control system", "multiversion-threshold adapt", "reliabl estim", "architectur", "multivers majority-redund computer-aid control system", "hardwar failur stabil", "softwar failur stabil", "event graph model"]}
{"doc": "Diffraction limit for a circular mask with a periodic rectangular apertures array A mask with periodic apertures imaging system is adopted very widely and plays a leading role in modern technology for uses such as pinhole cameras, coded imaging systems, optical information processing, etc. because of its high resolution, its infinite depth of focus, and its usefulness over a broad frequency spectra ranging from visible light to X-rays and gamma rays. While the masks with periodic apertures investigated in the literature are limited only to far-field diffraction, they do not take the shift of apertures within the mask into consideration. Therefore the derivation of the far-field diffraction for a single aperture cannot be applied to a mask with periodic apertures. The far-field diffraction formula modified for a multiaperture mask has been proposed in the past, the analysis remains too complicated to offer some practical guidance for mask design. We study a circular mask with periodic rectangular apertures and develop an easier way to interpret it. First, the near-field diffraction intensity of a circular aperture is calculated by means of Lommel's function. Then the convolution of the circular mask diffraction with periodic rectangular apertures is put together, and we can present a simple mathematical tool to analyze the mask properties including the intensity distribution, blurring aberration, and the criterion of defining the far- or near-field diffraction. This concept can also be expanded to analyze different types of masks with the arbitrarily shaped apertures", "label": ["diffraction limit", "circular mask", "periodic rectangular apertures array", "pinhole cameras", "coded imaging systems", "optical information processing", "high resolution", "infinite depth of focus", "broad frequency spectra", "visible light", "x rays", "gamma rays", "periodic apertures", "far-field diffraction", "mask", "single aperture", "far-field diffraction formula", "multiaperture mask", "periodic rectangular apertures", "convolution", "circular mask diffraction", "near-field diffraction", "arbitrarily shaped apertures"], "stemmed_label": ["diffract limit", "circular mask", "period rectangular apertur array", "pinhol camera", "code imag system", "optic inform process", "high resolut", "infinit depth of focu", "broad frequenc spectra", "visibl light", "x ray", "gamma ray", "period apertur", "far-field diffract", "mask", "singl apertur", "far-field diffract formula", "multiapertur mask", "period rectangular apertur", "convolut", "circular mask diffract", "near-field diffract", "arbitrarili shape apertur"]}
{"doc": "Psychology and the Internet This article presents an overview of the way that the Internet is being used to assist psychological research and mediate psychological practice. It shows how psychologists are using the Internet to examine the interactions between people and computers, and highlights some of the ways that this research is important to the design and development of useable and acceptable computer systems. In particular, this introduction reviews the research presented at the International Conference on Psychology and the Internet held in the United Kingdom. The final part introduces the eight articles in this special edition. The articles are representative of the breadth of research being conducted on psychology and the Internet: there are two on methodological issues, three on group processes, one on organizational implications, and two on social implications of Internet use", "label": ["internet", "psychological research", "human-computer interactions", "usability", "social implications", "psychology", "organizational implications", "group processes", "methodological issues", "online research"], "stemmed_label": ["internet", "psycholog research", "human-comput interact", "usabl", "social implic", "psycholog", "organiz implic", "group process", "methodolog issu", "onlin research"]}
{"doc": "A framework for evaluating the data-hiding capacity of image sources An information-theoretic model for image watermarking and data hiding is presented in this paper. Previous theoretical results are used to characterize the fundamental capacity limits of image watermarking and data-hiding systems. Capacity is determined by the statistical model used for the host image, by the distortion constraints on the data hider and the attacker, and by the information available to the data hider, to the attacker, and to the decoder. We consider autoregressive, block-DCT, and wavelet statistical models for images and compute data-hiding capacity for compressed and uncompressed host-image sources. Closed-form expressions are obtained under sparse-model approximations. Models for geometric attacks and distortion measures that are invariant to such attacks are considered", "label": ["data-hiding capacity", "image sources", "information-theoretic model", "watermarking", "capacity limits", "statistical model", "distortion constraints", "autoregressive statistical models", "block-dct statistical models", "wavelet statistical models", "compressed host-image sources", "uncompressed host-image sources", "closed-form expressions", "sparse-model approximations", "geometric attacks", "distortion measures"], "stemmed_label": ["data-hid capac", "imag sourc", "information-theoret model", "watermark", "capac limit", "statist model", "distort constraint", "autoregress statist model", "block-dct statist model", "wavelet statist model", "compress host-imag sourc", "uncompress host-imag sourc", "closed-form express", "sparse-model approxim", "geometr attack", "distort measur"]}
{"doc": "Accessible streaming content Make sure your Web site is offering quality service to all your users. The article provides some tips and tactics for making your streaming media accessible. Accessibility of streaming content for people with disabilities is often not part of the spec for multimedia projects, but it certainly affects your quality of service. Most of the resources available on Web accessibility deal with HTML. Fortunately, rich media and streaming content developers have a growing number of experts to turn to for information and assistance. The essentials of providing accessible streaming content are simple: blind and visually impaired people need audio to discern important visual detail and interface elements, while deaf and hard-of-hearing people need text to access sound effects and dialog. Actually implementing these principles is quite a challenge, though. Now due to a relatively new law in the US, known as Section 508, dealing with accessibility issues is becoming an essential part of publishing on the Web", "label": ["web site", "quality service", "streaming media", "content providers", "united states", "accessible streaming content", "disabled users", "multimedia projects", "web accessibility", "html", "streaming content developers", "visually impaired people", "blind people", "visual detail", "interface elements", "deaf people", "hard-of-hearing people", "sound effects", "section 508", "accessibility issues", "web publishing"], "stemmed_label": ["web site", "qualiti servic", "stream media", "content provid", "unit state", "access stream content", "disabl user", "multimedia project", "web access", "html", "stream content develop", "visual impair peopl", "blind peopl", "visual detail", "interfac element", "deaf peopl", "hard-of-hear peopl", "sound effect", "section 508", "access issu", "web publish"]}
{"doc": "Mathematical fundamentals of constructing fuzzy Bayesian inference techniques Problems and an associated technique for developing a Bayesian approach to decision-making in the case of fuzzy data are presented. The concept of fuzzy and pseudofuzzy quantities is introduced and main operations with pseudofuzzy quantities are considered. The basic relationships and the principal concepts of the Bayesian decision procedure based on the modus-ponens rule are proposed. Some problems concerned with the practical realization of the fuzzy Bayesian method are considered", "label": ["mathematical fundamentals", "fuzzy bayesian inference techniques", "decision making", "pseudofuzzy quantities", "modus-ponens rule"], "stemmed_label": ["mathemat fundament", "fuzzi bayesian infer techniqu", "decis make", "pseudofuzzi quantiti", "modus-ponen rule"]}
{"doc": "Effective moving cast shadow detection for monocular color traffic image sequences For an accurate scene analysis using monocular color traffic image sequences, a robust segmentation of moving vehicles from the stationary background is generally required. However, the presence of moving cast shadow may lead to an inaccurate vehicle segmentation, and as a result, may lead to further erroneous scene analysis. We propose an effective method for the detection of moving cast shadow. By observing the characteristics of cast shadow in the luminance, chrominance, gradient density, and geometry domains, a combined probability map, called a shadow confidence score (SCS), is obtained. From the edge map of the input image, each edge pixel is examined to determine whether it belongs to the vehicle region based on its neighboring SCSs. The cast shadow is identified as those regions with high SCSs, which are outside the convex hull of the selected vehicle edge pixels. The proposed method is tested on 100 vehicle images taken under different lighting conditions (sunny and cloudy), viewing angles (roadside and overhead), vehicle sizes (small, medium, and large), and colors (similar to the road and not). The results indicate that an average error rate of around 14 is obtained while the lowest error rate is around 3 for large vehicles", "label": ["effective moving cast shadow detection", "monocular color traffic image sequences", "accurate scene analysis", "robust segmentation", "moving vehicles", "stationary background", "moving cast shadow", "inaccurate vehicle segmentation", "erroneous scene analysis", "luminance", "chrominance", "gradient density", "geometry domains", "combined probability map", "shadow confidence score", "input image", "cast shadow", "convex hull", "selected vehicle edge pixels", "lighting conditions", "vehicle images", "sunny", "cloudy", "viewing angles", "vehicle sizes", "average error rate", "image segmentation"], "stemmed_label": ["effect move cast shadow detect", "monocular color traffic imag sequenc", "accur scene analysi", "robust segment", "move vehicl", "stationari background", "move cast shadow", "inaccur vehicl segment", "erron scene analysi", "lumin", "chromin", "gradient densiti", "geometri domain", "combin probabl map", "shadow confid score", "input imag", "cast shadow", "convex hull", "select vehicl edg pixel", "light condit", "vehicl imag", "sunni", "cloudi", "view angl", "vehicl size", "averag error rate", "imag segment"]}
{"doc": "Multilayered image representation: application to image compression The main contribution of this work is a new paradigm for image representation and image compression. We describe a new multilayered representation technique for images. An image is parsed into a superposition of coherent layers: piecewise smooth regions layer, textures layer, etc. The multilayered decomposition algorithm consists in a cascade of compressions applied successively to the image itself and to the residuals that resulted from the previous compressions. During each iteration of the algorithm, we code the residual part in a lossy way: we only retain the most significant structures of the residual part, which results in a sparse representation. Each layer is encoded independently with a different transform, or basis, at a different bitrate, and the combination of the compressed layers can always be reconstructed in a meaningful way. The strength of the multilayer approach comes from the fact that different sets of basis functions complement each others: some of the basis functions will give reasonable account of the large trend of the data, while others will catch the local transients, or the oscillatory patterns. This multilayered representation has a lot of beautiful applications in image understanding, and image and video coding. We have implemented the algorithm and we have studied its capabilities", "label": ["image compression", "multilayered representation", "image representation", "piecewise smooth regions layer", "textures layer", "multilayered decomposition algorithm", "residual part", "sparse representation", "basis functions", "wavelet transforms", "cosine transforms", "transform coding"], "stemmed_label": ["imag compress", "multilay represent", "imag represent", "piecewis smooth region layer", "textur layer", "multilay decomposit algorithm", "residu part", "spars represent", "basi function", "wavelet transform", "cosin transform", "transform code"]}
{"doc": "Quantum computing with solids Science and technology could be revolutionized by quantum computers, but building them from solid-state devices will not be easy. The author outlines the challenges in scaling up the technology from lab experiments to practical devices", "label": ["quantum computers", "solid-state devices"], "stemmed_label": ["quantum comput", "solid-st devic"]}
{"doc": "Geometrically invariant watermarking using feature points This paper presents a new approach for watermarking of digital images providing robustness to geometrical distortions. The weaknesses of classical watermarking methods to geometrical distortions are outlined first. Geometrical distortions can be decomposed into two classes: global transformations such as rotations and translations and local transformations such as the StirMark attack. An overview of existing self-synchronizing schemes is then presented. Theses schemes can use periodical properties of the mark, invariant properties of transforms, template insertion, or information provided by the original image to counter geometrical distortions. Thereafter, a new class of watermarking schemes using the image content is presented. We propose an embedding and detection scheme where the mark is bound with a content descriptor defined by salient points. Three different types of feature points are studied and their robustness to geometrical transformations is evaluated to develop an enhanced detector. The embedding of the signature is done by extracting feature points of the image and performing a Delaunay tessellation on the set of points. The mark is embedded using a classical additive scheme inside each triangle of the tessellation. The detection is done using correlation properties on the different triangles. The performance of the presented scheme is evaluated after JPEG compression, geometrical attack and transformations. Results show that the fact that the scheme is robust to these different manipulations. Finally, in our concluding remarks, we analyze the different perspectives of such content-based watermarking scheme", "label": ["geometrically invariant watermarking", "feature points", "digital images", "geometrical distortions", "global transformations", "rotations", "translations", "local transformations", "stirmark attack", "self-synchronizing schemes", "periodical properties", "invariant properties", "transforms", "template insertion", "image content", "embedding", "detection scheme", "content descriptor", "feature extraction", "delaunay tessellation", "additive scheme", "correlation properties", "jpeg compression", "geometrical attack"], "stemmed_label": ["geometr invari watermark", "featur point", "digit imag", "geometr distort", "global transform", "rotat", "translat", "local transform", "stirmark attack", "self-synchron scheme", "period properti", "invari properti", "transform", "templat insert", "imag content", "embed", "detect scheme", "content descriptor", "featur extract", "delaunay tessel", "addit scheme", "correl properti", "jpeg compress", "geometr attack"]}
{"doc": "From a biological to a computational model for the autonomous behavior of an animat Endowing an autonomous system like a robot with intelligent behavior is difficult for several reasons. First, behavior is such a wide topic that a general framework paradigm of inspiration must be chosen in order to obtain a consistent model. Such a framework can be, for example, biological modeling or an artificial intelligence approach. Second, a general framework is not sufficient to determine a fully specified program to be implemented in a robot. Many choices, tuning and tests must be carried out before obtaining a robust system. A biological model is presented, based on the definition of cortex-like automata, representing elementary functions in the perceptive, motor or associative domain. These automata are connected in a network whose architecture, functioning and learning rules are described in a cortical framework. Second, the computational model derived from that biological model is specified. The way units exchange and compute variables through links is explained, with reference to corresponding biological elements. It is then easier to report experiments allowing an autonomous system to learn regularities of a simple environment and to exploit them to satisfy some internal drives. Even if additional biological hints can be added, this model allow us to better understand how a biological model can be implemented and how biological properties can emerge from a distributed set of units", "label": ["autonomous system", "robot", "autonomous behavior", "intelligent behavior", "animat", "tuning", "tests", "robust system", "biological model", "cortex-like automata", "elementary functions", "perceptive domain", "associative domain", "motor domain", "learning rules", "architecture", "computational model", "variable computation", "variable exchange", "links", "regularity learning", "simple environment", "internal drives"], "stemmed_label": ["autonom system", "robot", "autonom behavior", "intellig behavior", "animat", "tune", "test", "robust system", "biolog model", "cortex-lik automata", "elementari function", "percept domain", "associ domain", "motor domain", "learn rule", "architectur", "comput model", "variabl comput", "variabl exchang", "link", "regular learn", "simpl environ", "intern drive"]}
{"doc": "The perils of privacy The recent string of failures among dotcom companies has heightened fears of privacy abuse. What should happen to the names and addresses on a customer list if these details were obtained under a privacy policy which specified no disclosure to any third party? Should the personal data in the list be deemed to be an asset of a failing company which can be transferred to any future (third party) purchaser for its purposes? Or should the privacy policy take precedence over the commercial concerns of the purchaser?", "label": ["privacy abuse", "customer list", "privacy policy", "disclosure"], "stemmed_label": ["privaci abus", "custom list", "privaci polici", "disclosur"]}
{"doc": "Automated breath detection on long-duration signals using feedforward backpropagation artificial neural networks A new breath-detection algorithm is presented, intended to automate the analysis of respiratory data acquired during sleep. The algorithm is based on two independent artificial neural networks (ANN/sub insp/ and ANN/sub expi/) that recognize, in the original signal, windows of interest where the onset of inspiration and expiration occurs. Postprocessing consists in finding inside each of these windows of interest minimum and maximum corresponding to each inspiration and expiration. The ANN/sub insp/ and ANN/sub expi/ correctly determine respectively 98.0 and 98.7 of the desired windows, when compared with 29 820 inspirations and 29 819 expirations detected by a human expert, obtained from three entire-night recordings. Postprocessing allowed determination of inspiration and expiration onsets with a mean difference with respect to the same human expert of (mean +or- SD) 34 +or- 71 ms for inspiration and 5 +or- 46 ms for expiration. The method proved to be effective in detecting the onset of inspiration and expiration in full night continuous recordings. A comparison of five human experts performing the same classification task yielded that the automated algorithm was undifferentiable from these human experts, failing within the distribution of human expert results. Besides being applicable to adult respiratory volume data, the presented algorithm was also successfully applied to infant sleep data, consisting of uncalibrated rib cage and abdominal movement recordings. A comparison with two previously published algorithms for breath detection in respiratory volume signal shows that the presented algorithm has a higher specificity, while presenting similar or higher positive predictive values", "label": ["respiratory movements", "automated breath detection", "postprocessing", "inspiration", "expiration", "automated algorithm", "human experts", "entire-night recordings", "uncalibrated rib cage", "abdominal movement recordings", "infant sleep data", "adult respiratory volume data", "long-duration signals", "feedforward backpropagation artificial neural networks", "34 ms", "5 ms"], "stemmed_label": ["respiratori movement", "autom breath detect", "postprocess", "inspir", "expir", "autom algorithm", "human expert", "entire-night record", "uncalibr rib cage", "abdomin movement record", "infant sleep data", "adult respiratori volum data", "long-dur signal", "feedforward backpropag artifici neural network", "34 ms", "5 ms"]}
{"doc": "Neighborhood operator systems and approximations This paper presents a framework for the study of generalizing the standard notion of equivalence relation in rough set approximation space with various categories of k-step neighborhood systems. Based on a binary relation on a finite universe, six families of binary relations are obtained, and the corresponding six classes of k-step neighborhood systems are derived. Extensions of Pawlak's (1982) rough set approximation operators based on such neighborhood systems are proposed. Properties of neighborhood operator systems and rough set approximation operators are investigated, and their connections are examined", "label": ["neighborhood operator systems", "equivalence relation", "rough set approximation space", "k-step neighborhood systems", "binary relation", "finite universe"], "stemmed_label": ["neighborhood oper system", "equival relat", "rough set approxim space", "k-step neighborhood system", "binari relat", "finit univers"]}
{"doc": "Aim for the enterprise: Microsoft Project 2002 A long-time favorite of project managers, Microsoft Project 2002 is making its enterprise debut. Its new Web-based collaboration tools and improved scalability with OLAP support make it much easier to manage multiple Web projects with disparate workgroups and budgets", "label": ["microsoft project 2002", "web-based collaboration tools", "scalability", "olap support", "multiple web project management", "workgroups", "budgets"], "stemmed_label": ["microsoft project 2002", "web-bas collabor tool", "scalabl", "olap support", "multipl web project manag", "workgroup", "budget"]}
{"doc": "Factors contributing to preservice teachers' discomfort in a Web-based course structured as an inquiry A report is given of a qualitative emergent design study of a Science, Technology, Society Interaction (STS) Web-enhanced course. Students' discomfort during the pilot test provided insight into the intellectual scaffolding that preservice secondary science teachers needed to optimize their performance when required to develop understanding through open-ended inquiry in a Web environment. Eight factors identified contributed to student discomfort: computer skills, paradigm shifts, trust, time management, thinking about their own thinking, systematic inquiry, self-assessment, and scientific discourse. These factors suggested developing understanding through inquiry by conducting a self-designed, open-ended, systematic inquiry required autonomous learning involving metacognitive skills and time management skills. To the extent in which students either came into the course with this scaffolding, or developed it during the course, they were successful in learning about STS and its relationship to science teaching. Changes in the Web site made to accommodate learners' needs as they surfaced are described", "label": ["preservice teacher discomfort", "web-based course", "qualitative emergent design study", "science technology society interaction course", "web-enhanced course", "student discomfort", "intellectual scaffolding", "preservice secondary science teachers", "open-ended inquiry", "web environment", "computer skills", "paradigm shifts", "trust", "time management", "thinking", "systematic inquiry", "self-assessment", "scientific discourse", "autonomous learning", "metacognitive skills", "time management skills", "sts", "science teaching"], "stemmed_label": ["preservic teacher discomfort", "web-bas cours", "qualit emerg design studi", "scienc technolog societi interact cours", "web-enhanc cours", "student discomfort", "intellectu scaffold", "preservic secondari scienc teacher", "open-end inquiri", "web environ", "comput skill", "paradigm shift", "trust", "time manag", "think", "systemat inquiri", "self-assess", "scientif discours", "autonom learn", "metacognit skill", "time manag skill", "st", "scienc teach"]}
{"doc": "Information architecture: looking ahead It may be a bit strange to consider where the field of information architecture (IA) is headed. After all, many would argue that it's too new to be considered as a field at all, or that it is mislabeled, and by no means is there a widely accepted definition of what information architecture actually is. Practicing information architects probably number in the thousands, and this vibrant group is already building various forms of communal infrastructure, ranging from an IA journal and a self-organizing \"library\" of resources to a passel of local professional groups and degree-granting academic programs. So the profession has achieved a beachhead that will enable it to stabilize and perhaps even grow during these difficult times", "label": ["information architecture", "information architects", "communal infrastructure", "local professional groups", "degree-granting academic programs"], "stemmed_label": ["inform architectur", "inform architect", "commun infrastructur", "local profession group", "degree-gr academ program"]}
{"doc": "A framework of electronic tendering for government procurement: a lesson learned in Taiwan To render government procurement efficient, transparent, nondiscriminating, and accountable, an electronic government procurement system is required. Accordingly, Taiwan government procurement law (TGPL) states that suppliers may employ electronic devices to forward a tender. This investigation demonstrates how the electronic government procurement system functions and reengineers internal procurement processes, which in turn benefits both government bodies and vendors. The system features explored herein include posting/receiving bids via the Internet, vendor registration, certificate authorization, contract development tools, bid/request for proposal (RFP) development, online bidding, and online payment, all of which can be integrated easily within most existing information infrastructures", "label": ["electronic tendering", "electronic government procurement system", "taiwan government procurement law", "reengineering", "internal procurement processes", "internet bids", "vendor registration", "certificate authorization", "contract development tools", "request for proposal development", "rfp development", "online bidding", "online payment", "certification authority", "payment gateway", "public key infrastructure"], "stemmed_label": ["electron tender", "electron govern procur system", "taiwan govern procur law", "reengin", "intern procur process", "internet bid", "vendor registr", "certif author", "contract develop tool", "request for propos develop", "rfp develop", "onlin bid", "onlin payment", "certif author", "payment gateway", "public key infrastructur"]}
{"doc": "Online longitudinal survey research: viability and participation This article explores the viability of conducting longitudinal survey research using the Internet in samples exposed to trauma. A questionnaire battery assessing psychological adjustment following adverse life experiences was posted online. Participants who signed up to take part in the longitudinal aspect of the study were contacted 3 and 6 months after initial participation to complete the second and third waves of the research. Issues of data screening and sample attrition rates are considered and the demographic profiles and questionnaire scores of those who did and did not take part in the study during successive time points are compared. The results demonstrate that it is possible to conduct repeated measures survey research online and that the similarity in characteristics between those who do and do not take part during successive time points mirrors that found in traditional pencil-and-paper trauma surveys", "label": ["online longitudinal survey research", "internet", "trauma", "questionnaire", "psychological adjustment", "data screening", "sample attrition rates", "demographic profiles", "world wide web", "psychology research"], "stemmed_label": ["onlin longitudin survey research", "internet", "trauma", "questionnair", "psycholog adjust", "data screen", "sampl attrit rate", "demograph profil", "world wide web", "psycholog research"]}
{"doc": "Control of combustion processes in an internal combustion engine by low-temperature plasma A new method of operation of internal combustion engines enhances power and reduces fuel consumption and exhaust toxicity. Low-temperature plasma control combines working processes of thermal engines and steam machines into a single process", "label": ["combustion processes", "internal combustion engine", "low-temperature plasma", "fuel consumption", "exhaust toxicity", "working processes", "thermal engines", "steam machines"], "stemmed_label": ["combust process", "intern combust engin", "low-temperatur plasma", "fuel consumpt", "exhaust toxic", "work process", "thermal engin", "steam machin"]}
{"doc": "7 key tests in choosing your Web site firm Most legal firms now have a Web site and are starting to evaluate the return on their investment. The paper looks at factors involved when choosing a firm to help set up or improve a Web site. (1) Look for a company that combines technical skills and business experience. (2) Look for a company that offers excellent customer service. (3) Check that the Web site firm is committed to developing and proactively updating the Web site. (4) Make sure the firm has a proven track record and a good portfolio. (5) Look for a company with both a breadth as well as depth of skills. (6) Make sure the firm can deliver work on target, in budget and to specification. (7) Ensure that you will enjoy working and feel comfortable with the Web site firm staff", "label": ["web site", "customer service", "proactive updating", "legal firms", "return on investment", "technical skills", "business experience"], "stemmed_label": ["web site", "custom servic", "proactiv updat", "legal firm", "return on invest", "technic skill", "busi experi"]}
{"doc": "The set of stable polynomials of linear discrete systems: its geometry The multidimensional stability domain of linear discrete systems is studied. Its configuration is determined from the parameters of its intersection with coordinate axes, coordinate planes, and certain auxiliary planes. Counterexamples for the discrete variant of the Kharitonov theorem are given", "label": ["stable polynomials", "kharitonov theorem", "characteristic polynomial", "linear discrete systems", "geometry", "multidimensional stability domain"], "stemmed_label": ["stabl polynomi", "kharitonov theorem", "characterist polynomi", "linear discret system", "geometri", "multidimension stabil domain"]}
{"doc": "Extended depth-of-focus imaging of chlorophyll fluorescence from intact leaves Imaging dynamic changes in chlorophyll a fluorescence provides a valuable means with which to examine localised changes in photosynthetic function. Microscope-based systems provide excellent spatial resolution which allows the response of individual cells to be measured. However, such systems have a restricted depth of focus and, as leaves are inherently uneven, only a small proportion of each image at any given focal plane is in focus. In this report we describe the development of algorithms, specifically adapted for imaging chlorophyll fluorescence and photosynthetic function in living plant cells, which allow extended-focus images to be reconstructed from images taken in different focal planes. We describe how these procedures can be used to reconstruct images of chlorophyll fluorescence and calculated photosynthetic parameters, as well as producing a map of leaf topology. The robustness of this procedure is demonstrated using leaves from a number of different plant species", "label": ["chlorophyll fluorescence", "intact leaves", "extended depth-of-focus imaging", "leaf topology map", "plant species", "calculated photosynthetic parameters", "individual cells response", "microscope-based systems", "charge-coupled device", "maximum fluorescence yield", "minimum fluorescence yield", "variable fluorescence", "numerical aperture", "primary quinone acceptor", "spatial resolution", "algorithms development", "extended-focus images reconstruction", "biophysical research technique"], "stemmed_label": ["chlorophyl fluoresc", "intact leav", "extend depth-of-focu imag", "leaf topolog map", "plant speci", "calcul photosynthet paramet", "individu cell respons", "microscope-bas system", "charge-coupl devic", "maximum fluoresc yield", "minimum fluoresc yield", "variabl fluoresc", "numer apertur", "primari quinon acceptor", "spatial resolut", "algorithm develop", "extended-focu imag reconstruct", "biophys research techniqu"]}
{"doc": "Dynamical transition to periodic motions of a recurrent bus induced by nonstops We study the dynamical behavior of a recurrent bus on a circular route with many bus stops when the recurrent bus passes some bus stops without stopping. The recurrent time (one period) is described in terms of a nonlinear map. It is shown that the recurrent bus exhibits the complex periodic behaviors. The dynamical transitions to periodic motions occur by increasing nonstops. The periodic motions depend on the property of an attractor of the nonlinear map. The period n of the attractor varies sensitively with the number of nonstops", "label": ["dynamical transition", "periodic motions", "recurrent bus", "nonstops", "circular route", "recurrent time", "nonlinear map", "complex periodic behaviors", "attractor"], "stemmed_label": ["dynam transit", "period motion", "recurr bu", "nonstop", "circular rout", "recurr time", "nonlinear map", "complex period behavior", "attractor"]}
{"doc": "Streaming, disruptive interference and power-law behavior in the exit dynamics of confined pedestrians We analyze the exit dynamics of pedestrians who are initially confined in a room. Pedestrians are modeled as cellular automata and compete to escape via a known exit at the soonest possible time. A pedestrian could move forward, backward, left or right within each iteration time depending on adjacent cell vacancy and in accordance with simple rules that determine the compulsion to move and physical capability relative to his neighbors. The arching signatures of jamming were observed and the pedestrians exited in bursts of various sizes. Power-law behavior is found in the burst-size frequency distribution for exit widths w greater than one cell dimension (w 1). The slope of the power-law curve varies with w from -1.3092 (w = 2) to -1.0720 (w = 20). Streaming which is a diffusive behavior, arises in large burst sizes and is more likely in a single-exit room with w = 1 and leads to a counterintuitive result wherein an average exit throughput Q is obtained that is higher than with w = 2, 3, or 4. For a two-exit room (w = 1), Q is not greater than twice the yield of a single-exit room. If the doors are not separated far enough ( 4w), Q becomes even significantly less due to a collective slow-down that emerges among pedestrians crossing in each other's path (disruptive interference effect). For the same w and door number, Q is also higher with relaxed pedestrians than with anxious ones", "label": ["streaming", "cellular automata", "iteration time", "adjacent cell vacancy", "arching signatures", "jamming", "burst-size frequency distribution", "collective slow-down", "self-organised criticality", "disruptive interference", "power-law behavior", "exit dynamics", "confined pedestrians"], "stemmed_label": ["stream", "cellular automata", "iter time", "adjac cell vacanc", "arch signatur", "jam", "burst-siz frequenc distribut", "collect slow-down", "self-organis critic", "disrupt interfer", "power-law behavior", "exit dynam", "confin pedestrian"]}
{"doc": "Guidelines, the Internet, and personal health: insights from the Canadian HEALNet experience The objectives are to summarize the insights gained in collaborative research in a Canadian Network of Centres of Excellence, devoted to the promotion of evidence-based practice, and to relate this experience to Internet support of health promotion and consumer health informatics. A subjective review of insights is undertaken. Work directed the development of systems incorporating guidelines, care maps, etc., for use by professionals met with limited acceptance. Evidence-based tools for health care consumers are a desirable complement but require radically different content and delivery modes. In addition to evidence-based material offered by professionals, a wide array of Internet-based products and services provided by consumers for consumers emerged and proved a beneficial complement. The consumer-driven products and services provided via the Internet are a potentially important and beneficial complement of traditional health services. They affect the health consumer-provider roles and require changes in healthcare practices", "label": ["collaborative research", "canadian network of centres of excellence", "evidence-based practice", "internet support", "health promotion", "consumer health informatics", "personal health", "health consumer-provider roles"], "stemmed_label": ["collabor research", "canadian network of centr of excel", "evidence-bas practic", "internet support", "health promot", "consum health informat", "person health", "health consumer-provid role"]}
{"doc": "Explanations for the perpetration of and reactions to deception in a virtual community Cases of identity deception on the Internet are not uncommon. Several cases of a revealed identity deception have been reported in the media. The authors examine a case of deception in an online community composed primarily of information technology professionals. In this case, an established community member (DF) invented a character (Nowheremom) whom he fell in love with and who was eventually killed in a tragic accident. When other members of the community eventually began to question Nowheremom's actual identity, DF admitted that he invented her. The discussion board was flooded with reactions to DF's revelation. The authors propose several explanations for the perpetration of identity deception, including psychiatric illness, identity play, and expressions of true self. They also analyze the reactions of community members and propose three related explanations (social identity, deviance, and norm violation) to account for their reactions. It is argued that virtual communities' reactions to such threatening events provide invaluable clues for the study of group processes on the Internet", "label": ["virtual community", "identity deception", "internet", "online community", "information technology professionals", "psychiatric illness", "group processes", "social processes", "web sites", "psychology", "bulletin boards"], "stemmed_label": ["virtual commun", "ident decept", "internet", "onlin commun", "inform technolog profession", "psychiatr ill", "group process", "social process", "web site", "psycholog", "bulletin board"]}
{"doc": "A digital-to-analog converter based on differential-quad switching A high-conversion-rate high-resolution oversampling digital-to-analog converter (DAC) for direct digital modulation is addressed in this paper. A new type of switching scheme, called differential-quad switching, is presented. To verify the feasibility of this scheme, essential parts with some auxiliary circuitry for interfacing were fabricated in a 0.8- mu m CMOS technology. Measured results show that the switching scheme provides 11-b resolution at 100 MSamples/s and 6-b at 1 GSamples/s. The degradation in signal-to-noise ratio is not observed for the variation of the supply voltage down to 1.5 V, which means the proposed scheme is suitable for low-voltage applications", "label": ["high-conversion-rate dac", "high-resolution dac", "oversampling dac", "digital-to-analog converter", "cmos technology", "direct digital modulation", "differential-quad switching", "signal-to-noise ratio", "snr", "1.5 v", "0.8 micron"], "stemmed_label": ["high-conversion-r dac", "high-resolut dac", "oversampl dac", "digital-to-analog convert", "cmo technolog", "direct digit modul", "differential-quad switch", "signal-to-nois ratio", "snr", "1.5 v", "0.8 micron"]}
{"doc": "Edit distance of run-length encoded strings Let X and Y be two run-length encoded strings, of encoded lengths k and l, respectively. We present a simple O(|X|l+|Y|k) time algorithm that computes their edit distance", "label": ["run-length encoded strings", "encoded lengths", "algorithm", "edit distance", "computation time"], "stemmed_label": ["run-length encod string", "encod length", "algorithm", "edit distanc", "comput time"]}
{"doc": "Content standards for electronic books: the OEBF publication structure and the role of public interest participation In the emerging world of electronic publishing how we create, distribute, and read books will be in a large part determined by an underlying framework of content standards that establishes the range of technological opportunities and constraints for publishing and reading systems. But efforts to develop content standards based on sound engineering models must skillfully negotiate competing and sometimes apparently irreconcilable objectives if they are to produce results relevant to the rapidly changing course of technology. The Open eBook Forum's Publication Structure, an XML-based specification for electronic books, is an example of the sort of timely and innovative problem solving required for successful real-world standards development. As a result of this effort, the electronic book industry will not only happen sooner and on a larger scale than it would have otherwise, but the electronic books it produces will be more functional, more interoperable, and more accessible to all readers. Public interest participants have a critical role in this process", "label": ["electronic publishing", "electronic books", "content standards", "oebf publication structure", "public interest participation", "open ebook forum publication structure", "xml-based specification"], "stemmed_label": ["electron publish", "electron book", "content standard", "oebf public structur", "public interest particip", "open ebook forum public structur", "xml-base specif"]}
{"doc": "Information architecture: notes toward a new curriculum There are signs that information architecture is coalescing into a field of professional practice. However, if it is to become a profession, it must develop a means of educating new information architects. Lessons from other fields suggest that professional education typically evolves along a predictable path, from apprenticeships to trade schools to college- and university-level education. Information architecture education may develop more quickly to meet the growing demands of the information society. Several pedagogical approaches employed in other fields may be adopted for information architecture education, as long as the resulting curricula provide an interdisciplinary approach and balance instruction in technical and design skills with consideration of theoretical concepts. Key content areas are information organization, graphic. design, computer science, user and usability studies, and communication. Certain logistics must be worked out, including where information architecture studies should be housed and what kinds of degrees should be offered and at what levels. The successful information architecture curriculum will be flexible and adaptable in order to meet the changing needs of students and the marketplace", "label": ["professional practice", "information organization", "graphic design", "computer science", "usability studies", "information architects", "professional education", "pedagogical approaches", "information architecture education"], "stemmed_label": ["profession practic", "inform organ", "graphic design", "comput scienc", "usabl studi", "inform architect", "profession educ", "pedagog approach", "inform architectur educ"]}
{"doc": "Symbiosis or alienation: advancing the university press/research library relationship through electronic scholarly communication University presses and research libraries have a long tradition of collaboration. The rapidly expanding electronic scholarly communication environment offers important new opportunities for cooperation and for innovative new models of publishing. The economics of libraries and scholarly publishers have strained the working relationship and promoted debates on important information policy issues. This article explores the context for advancing the partnership, cites examples of joint efforts in electronic publishing, and presents an action plan for working together", "label": ["university press/research library relationship", "electronic scholarly communication", "economics", "information policy", "electronic publishing"], "stemmed_label": ["univers press/research librari relationship", "electron scholarli commun", "econom", "inform polici", "electron publish"]}
{"doc": "Accurate modeling of lossy nonuniform transmission lines by using differential quadrature methods This paper discusses an efficient numerical approximation technique, called the differential quadrature method (DQM), which has been adapted to model lossy uniform and nonuniform transmission lines. The DQM can quickly compute the derivative of a function at any point within its bounded domain by estimating a weighted linear sum of values of the function at a small set of points belonging to the domain. Using the DQM, the frequency-domain Telegrapher's partial differential equations for transmission lines can be discretized into a set of easily solvable algebraic equations. DQM reduces interconnects into multiport models whose port voltages and currents are related by rational formulas in the frequency domain. Although the rationalization process in DQM is comparable with the Pade approximation of asymptotic waveform evaluation (AWE) applied to transmission lines, the derivation mechanisms in these two disparate methods are significantly different. Unlike AWE, which employs a complex moment-matching process to obtain rational approximation, the DQM requires no approximation of transcendental functions, thereby avoiding the process of moment generation and moment matching. Due to global sampling of points in the DQM approximation, it requires far fewer grid points in order to build accurate discrete models than other numerical methods do. The DQM-based time-domain model can be readily integrated in a circuit simulator like SPICE", "label": ["lossy nonuniform transmission lines", "differential quadrature method", "numerical approximation technique", "frequency-domain telegrapher pde", "partial differential equations", "algebraic equations", "interconnects", "multiport models", "multiconductor transmission lines", "rationalization process", "time-domain model"], "stemmed_label": ["lossi nonuniform transmiss line", "differenti quadratur method", "numer approxim techniqu", "frequency-domain telegraph pde", "partial differenti equat", "algebra equat", "interconnect", "multiport model", "multiconductor transmiss line", "ration process", "time-domain model"]}
{"doc": "Autofocus system for microscope A technique is developed for microscope autofocusing, which is called the eccentric light beam approach with high resolution, wide focusing range, and compact construction. The principle is described. The theoretical formula of the eccentric light beam approach deduced can be applied not only to an object lens whose objective plane is just at the focal plane, but also to an object lens whose objective plane is not at the focal plane. The experimental setup uses a semiconductor laser device as the light source. The laser beam that enters into the microscope is eccentric with the main light axis. A defocused signal is acquired by a symmetrical silicon photocell for the change of the reflected light position caused by differential amplification and processed by a microprocessor. Then the electric signal is power-amplified and drives a dc motor, which moves a fine working platform to an automatic focus of the microscope. The result of the experiments shows a +or-0.1- mu m precision of autofocusing for a range of +or-500- mu m defocusing. The system has high reliability and can meet the requirements of various accurate micro measurement systems", "label": ["autofocus system", "microscope autofocusing", "eccentric light beam approach", "object lens", "objective plane", "semiconductor laser", "main light axis", "defocused signal", "symmetrical silicon photocell", "reflected light position", "differential amplification", "microprocessor", "power-amplified electric signal", "dc motor", "fine working platform", "high reliability", "micro measurement systems"], "stemmed_label": ["autofocu system", "microscop autofocus", "eccentr light beam approach", "object len", "object plane", "semiconductor laser", "main light axi", "defocus signal", "symmetr silicon photocel", "reflect light posit", "differenti amplif", "microprocessor", "power-amplifi electr signal", "dc motor", "fine work platform", "high reliabl", "micro measur system"]}
{"doc": "One-step digit-set-restricted modified signed-digit adder using an incoherent correlator based on a shared content-addressable memory An efficient one-step digit-set-restricted modified signed-digit (MSD) adder based on symbolic substitution is presented. In this technique, carry propagation is avoided by introducing reference digits to restrict the intermediate carry and sum digits to 1,0 and 0,1 , respectively. The proposed technique requires significantly fewer minterms and simplifies system complexity compared to the reported one-step MSD addition techniques. An incoherent correlator based on an optoelectronic shared content-addressable memory processor is suggested to perform the addition operation. In this technique, only one set of minterms needs to be stored, independent of the operand length", "label": ["one-step digit-set-restricted modified signed-digit adder", "incoherent correlator", "shared content-addressable memory", "symbolic substitution", "reference digits", "intermediate carry", "sum digits", "minterms", "system complexity", "optoelectronic shared content-addressable memory processor", "addition operation", "operand length"], "stemmed_label": ["one-step digit-set-restrict modifi signed-digit adder", "incoher correl", "share content-address memori", "symbol substitut", "refer digit", "intermedi carri", "sum digit", "minterm", "system complex", "optoelectron share content-address memori processor", "addit oper", "operand length"]}
{"doc": "Managing safety and strategic stocks to improve materials requirements planning performance This paper provides a methodology for managing safety and strategic stocks in materials requirements planning (MRP) environments to face uncertainty in market demand. A set of recommended guidelines suggest where to position, how to dimension and when to replenish both safety and strategic stocks. Trade-offs between stock positioning and dimensioning and between stock positioning and replenishment order triggering are outlined. The study reveals also that most of the decisions are system specific, so that they should be evaluated in a quantitative manner through simulation. A case study is reported, where the benefits from adopting the new proposed methodology lie in achieving the target service level even under peak demand conditions, with the value of safety stocks as a whole growing only by about 20 per cent", "label": ["mrp", "materials requirements planning", "market demand", "strategic stocks", "safety stocks", "inventory management", "variance control", "stock replenishment", "service level", "peak demand"], "stemmed_label": ["mrp", "materi requir plan", "market demand", "strateg stock", "safeti stock", "inventori manag", "varianc control", "stock replenish", "servic level", "peak demand"]}
{"doc": "Stabilization of a linear object by frequency-modulated pulsed signals A control system consisting of an unstable continuous linear part and a pulse-frequency modulator in the feedback circuit is studied. Conditions for the boundedness of the solutions of the system under any initial data are determined", "label": ["discrete systems", "stabilization", "frequency-modulated pulsed signals", "linear stationary object", "control system", "feedback circuit", "solution boundedness"], "stemmed_label": ["discret system", "stabil", "frequency-modul puls signal", "linear stationari object", "control system", "feedback circuit", "solut bounded"]}
{"doc": "Innovative phase unwrapping algorithm: hybrid approach We present a novel algorithm based on a hybrid of the global and local treatment of a wrapped map. The proposed algorithm is especially effective for the unwrapping of speckle-coded interferogram contour maps. In contrast to earlier unwrapping algorithms by region, we propose a local discontinuity-restoring criterion to serve as the preprocessor or postprocessor of our hybrid algorithm, which makes the unwrapping by region much easier and more efficient. With this hybrid algorithm, a robust, stable, and especially time effective phase unwrapping can be achieved. Additionally, the criterion and limitation of this hybrid algorithm are fully described. The robustness, stability, and speed of this hybrid algorithm are also studied. The proposed algorithm can be easily upgraded with minor modifications to solve the unwrapping problem of maps with phase inconsistency. Both numerical simulation and experimental applications demonstrate the effectiveness of the proposed algorithm", "label": ["phase unwrapping algorithm", "global treatment", "local treatment", "wrapped map", "speckle-coded interferogram contour maps", "unwrapping algorithms", "local discontinuity-restoring criterion", "postprocessor", "hybrid algorithm", "robust stable time effective phase unwrapping", "unwrapping problem", "phase inconsistency", "numerical simulation", "interferogram analysis", "light interferometry"], "stemmed_label": ["phase unwrap algorithm", "global treatment", "local treatment", "wrap map", "speckle-cod interferogram contour map", "unwrap algorithm", "local discontinuity-restor criterion", "postprocessor", "hybrid algorithm", "robust stabl time effect phase unwrap", "unwrap problem", "phase inconsist", "numer simul", "interferogram analysi", "light interferometri"]}
{"doc": "All-optical XOR gate using semiconductor optical amplifiers without additional input beam The novel design of an all-optical XOR gate by using cross-gain modulation of semiconductor optical amplifiers has been suggested and demonstrated successfully at 10 Gb/s. Boolean AB and AB of the two input signals A and B have been obtained and combined to achieve the all-optical XOR gate. No additional input beam such as a clock signal or continuous wave light is used in this new design, which is required in other all-optical XOR gates", "label": ["semiconductor optical amplifiers", "all-optical-xor gate", "design", "cross-gain modulation", "boolean logic", "10 gbit/s"], "stemmed_label": ["semiconductor optic amplifi", "all-optical-xor gate", "design", "cross-gain modul", "boolean logic", "10 gbit/"]}
{"doc": "Correlation of intuitionistic fuzzy sets by centroid method In this paper, we propose a method to calculate the correlation coefficient of intuitionistic fuzzy sets by means of \"centroid\". This value obtained from our formula tell us not only the strength of relationship between the intuitionistic fuzzy sets, but also whether the intuitionistic fuzzy sets are positively or negatively related. This approach looks better than previous methods which only evaluate the strength of the relation. Furthermore, we extend the \"centroid\" method to interval-valued intuitionistic fuzzy sets. The value of the correlation coefficient between interval-valued intuitionistic fuzzy sets lies in the interval -1, 1 , as computed from our formula", "label": ["correlation coefficient", "intuitionistic fuzzy sets", "centroid method", "interval-valued intuitionistic fuzzy sets"], "stemmed_label": ["correl coeffici", "intuitionist fuzzi set", "centroid method", "interval-valu intuitionist fuzzi set"]}
{"doc": "Adaptive digital watermarking using fuzzy logic techniques Digital watermarking has been proposed for copyright protection in our digital society. We propose an adaptive digital watermarking scheme based on the human visual system model and a fuzzy logic technique. The fuzzy logic approach is employed to obtain the different strengths and lengths of a watermark by the local characteristics of the image in our proposed scheme. In our experiments, this scheme provides a more robust and imperceptible watermark", "label": ["adaptive digital watermarking", "fuzzy logic techniques", "copyright protection", "digital society", "human visual system model", "local characteristics", "imperceptible watermark", "robust watermark", "image processing"], "stemmed_label": ["adapt digit watermark", "fuzzi logic techniqu", "copyright protect", "digit societi", "human visual system model", "local characterist", "impercept watermark", "robust watermark", "imag process"]}
{"doc": "Teaching management science with spreadsheets: From decision models to decision support The 1990s were a decade of enormous change for management science (MS) educators. While the outlook at the beginning of the decade was somewhat bleak, the renaissance in MS education brought about by the use of spreadsheets as the primary delivery vehicle for quantitative modeling techniques has resulted in a much brighter future. This paper takes inventory of the current state of MS education and suggests some promising new directions in the area of decision support systems for MS educators to consider for the future", "label": ["management science", "ms education", "spreadsheets", "quantitative modeling", "decision support systems"], "stemmed_label": ["manag scienc", "ms educ", "spreadsheet", "quantit model", "decis support system"]}
{"doc": "Toward an Experimental Timing Standards Lab: benchmarking precision in the real world Much discussion has taken place over the relative merits of various platforms and operating systems for real-time data collection. Most would agree that, provided great care is taken, many are capable of millisecond timing precision. However, to date, much of this work has focused on the theoretical aspects of raw performance. It is our belief that researchers would be better informed if they could place confidence limits on their own specific paradigms in situ and without modification. To this end, we have developed a millisecond precision test rig that can control and time experiments on a second presentation machine. We report on the specialist hardware and software used. We elucidate the importance of the approach in relation to real-world experimentation", "label": ["benchmarking precision", "experimental timing standards lab", "performance evaluation", "operating systems", "event generation software", "real-time data collection", "millisecond timing precision"], "stemmed_label": ["benchmark precis", "experiment time standard lab", "perform evalu", "oper system", "event gener softwar", "real-tim data collect", "millisecond time precis"]}
{"doc": "What you get is what you see Web performance monitoring To get the best possible performance from your Web infrastructure, you'll need a complete view. Don't neglect the big picture because you're too busy concentrating on details. The increasing complexity of Web sites and the content they provide has consequently increased the complexity of the infrastructure that supports them. But with some knowledge of networking, a handful of useful tools, and the insight that those tools provide, designing and operating for optimal performance and reliability is within your grasp", "label": ["web performance", "web sites", "web infrastructure", "networking", "reliability"], "stemmed_label": ["web perform", "web site", "web infrastructur", "network", "reliabl"]}
{"doc": "Optimal linear control in stabilizer design The most common method of improving stability of the power system is the synthesis of the turbine and generator control systems, because of the high effectiveness and relatively low cost of these elements. The synthesis and construction of the effective synchronous generator and turbine controller is a very difficult task. This paper describes the seven step mu -synthesis approach to PSS design enabling the synchronous generator to remain stable over a wide range of system operating conditions", "label": ["mu -synthesis approach", "pss design", "optimal linear control", "synchronous generator control system synthesis", "turbine control system synthesis"], "stemmed_label": ["mu -synthesi approach", "pss design", "optim linear control", "synchron gener control system synthesi", "turbin control system synthesi"]}
{"doc": "Median partitioning: a novel method for the selection of representative subsets from large compound pools A method termed median partitioning (MP) has been developed to select diverse sets of molecules from large compound pools. Unlike many other methods for subset selection, the MP approach does not depend on pairwise comparison of molecules and can therefore be applied to very large compound collections. The only time limiting step is the calculation of molecular descriptors for database compounds. MP employs arrays of property descriptors with little correlation to divide large compound pools into partitions from which representative molecules can be selected. In each of n subsequent steps, a population of molecules is divided into subpopulations above and below the median value of a property descriptor until a desired number of 2/sup n/ partitions are obtained. For descriptor evaluation and selection, an entropy formulation was embedded in a genetic algorithm. MP has been applied to generate a subset of the Available Chemicals Directory, and the results have been compared with cell-based partitioning", "label": ["median partitioning", "large compound pools", "representative subset selection", "molecules", "time limiting step", "molecular descriptors", "database compounds", "property descriptor array", "entropy formulation", "genetic algorithm", "available chemicals directory", "cell-based partitioning"], "stemmed_label": ["median partit", "larg compound pool", "repres subset select", "molecul", "time limit step", "molecular descriptor", "databas compound", "properti descriptor array", "entropi formul", "genet algorithm", "avail chemic directori", "cell-bas partit"]}
