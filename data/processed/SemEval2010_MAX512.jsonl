{"doc": "Efficient discovery of grid services is essential for the success of grid computing . The standardization of grids based on web services has resulted in the need for scalable web service discovery mechanisms to be deployed in grids Even though UDDI has been the de facto industry standard for web-services discovery , imposed requirements of tight-replication among registries and lack of autonomous control has severely hindered its widespread deployment and usage . With the advent of grid computing the scalability issue of UDDI will become a roadblock that will prevent its deployment in grids . In this paper we present our distributed web-service discovery architecture , called DUDE (Distributed UDDI Deployment Engine) . DUDE leverages DHT (Distributed Hash Tables) as a rendezvous mechanism between multiple UDDI registries . DUDE enables consumers to query multiple registries , still at the same time allowing organizations to have autonomous control over their registries. . Based on preliminary prototype on PlanetLab , we believe that DUDE architecture can support effective distribution of UDDI registries thereby making UDDI more robust and also addressing its scaling issues . Furthermore , The DUDE architecture for scalable distribution can be applied beyond UDDI to any Grid Service Discovery mechanism.", "label": ["bamboo dht code", "uddi registry", "md", "distributed web-service discovery architecture", "longest available prefix", "autonomous control", "discovery", "scalability issue", "dht based uddi registry hierarchy", "grid computing", "grid service discovery", "deployment issue", "uddi", "web service", "soft state", "query", "dht", "case-insensitive search", "qos-based service discovery"], "stemmed_label": ["bamboo dht code", "uddi registri", "md", "distribut web-servic discoveri architectur", "longest avail prefix", "autonom control", "discoveri", "scalabl issu", "dht base uddi registri hierarchi", "grid comput", "grid servic discoveri", "deploy issu", "uddi", "web servic", "soft state", "queri", "dht", "case-insensit search", "qos-bas servic discoveri"]}
{"doc": "In order to monitor a region for traffic traversal , sensors can be deployed to perform collaborative target detection . Such a sensor network achieves a certain level of detection performance with an associated cost of deployment . This paper addresses this problem by proposing path exposure as a measure of the goodness of a deployment and presents an approach for sequential deployment in steps . It illustrates that the cost of deployment can be minimized to achieve the desired detection performance by appropriately choosing the number of sensors deployed in each step.", "label": ["exposure", "sensor number", "path exposure", "deployment", "target detection", "target decay", "sequential deployment", "value fusion", "sensor network", "random sensor placement", "number of sensor", "minimum exposure", "sensor field", "collaborative target detection"], "stemmed_label": ["exposur", "sensor number", "path exposur", "deploy", "target detect", "target decay", "sequenti deploy", "valu fusion", "sensor network", "random sensor placement", "number of sensor", "minimum exposur", "sensor field", "collabor target detect"]}
{"doc": "Real-time services have been supported by and large on circuitswitched networks . Recent trends favour services ported on packet-switched networks . For audio conferencing , we need to consider many issues - scalability , quality of the conference application , floor control and load on the clients/servers - to name a few . In this paper , we describe an audio service framework designed to provide a Virtual Conferencing Environment (VCE) . The system is designed to accommodate a large number of end users speaking at the same time and spread across the Internet . The framework is based on Conference Servers 14 , which facilitate the audio handling , while we exploit the SIP capabilities for signaling purposes . Client selection is based on a recent quantifier called \"Loudness Number\" that helps mimic a physical face-to-face conference . We deal with deployment issues of the proposed solution both in terms of scalability and interactivity , while explaining the techniques we use to reduce the traffic . We have implemented a Conference Server (CS) application on a campus-wide network at our Institute.", "label": ["voip", "simultaneous speaker", "conference server", "partial mixing", "loudness number", "sufficiency of three simultaneous speaker", "audio service framework", "sip", "vce", "real-time audio", "packet-switched network", "virtual conferencing environment", "voip conferencing system", "voice activity detection", "vad technique", "three simultaneous speaker sufficiency"], "stemmed_label": ["voip", "simultan speaker", "confer server", "partial mix", "loud number", "suffici of three simultan speaker", "audio servic framework", "sip", "vce", "real-tim audio", "packet-switch network", "virtual conferenc environ", "voip conferenc system", "voic activ detect", "vad techniqu", "three simultan speaker suffici"]}
{"doc": "The Slammer , which is currently the fastest computer worm in recorded history , was observed to infect 90 percent of all vulnerable Internets hosts within 10 minutes . Although the main action that the Slammer worm takes is a relatively unsophisticated replication of itself , it still spreads so quickly that human response was ineffective . Most proposed countermeasures strategies are based primarily on rate detection and limiting algorithms . However , such strategies are being designed and developed to effectively contain worms whose behaviors are similar to that of Slammer . In our work , we put forth the hypothesis that next generation worms will be radically different , and potentially such techniques will prove ineffective . Specifically , we propose to study a new generation of worms called Swarm Worms , whose behavior is predicated on the concept of emergent intelligence . Emergent Intelligence is the behavior of systems , very much like biological systems such as ants or bees , where simple local interactions of autonomous members , with simple primitive actions , gives rise to complex and intelligent global behavior . In this manuscript we will introduce the basic principles behind the idea of Swarm Worms , as well as the basic structure required in order to be considered a swarm worm . In addition , we will present preliminary results on the propagation speeds of one such swarm worm , called the ZachiK worm . We will show that ZachiK is capable of propagating at a rate 2 orders of magnitude faster than similar worms without swarm capabilities . Categories and Subject Descriptors", "label": ["swarm worm", "internet worm", "distributed intelligence", "pre-generated target list", "swarm intelligence", "intrusion detection", "malware", "local communication mechanism", "emergent intelligence", "slammer worm", "emergent behavior", "zachik", "prng method", "countermeasure system"], "stemmed_label": ["swarm worm", "internet worm", "distribut intellig", "pre-gener target list", "swarm intellig", "intrus detect", "malwar", "local commun mechan", "emerg intellig", "slammer worm", "emerg behavior", "zachik", "prng method", "countermeasur system"]}
{"doc": "In this paper we compare two approaches to the design of protocol frameworks - tools for implementing modular network protocols . The most common approach uses events as the main abstraction for a local interaction between protocol modules . We argue that an alternative approach , that is based on service abstraction , is more suitable for expressing modular protocols . It also facilitates advanced features in the design of protocols , such as dynamic update of distributed protocols . We then describe an experimental implementation of a service-based protocol framework in Java.", "label": ["communication", "request", "modularity", "dynamic protocol replacement", "service interface", "event-based framework", "stack", "distributed system", "protocol framework", "reply", "module", "network", "distributed algorithm"], "stemmed_label": ["commun", "request", "modular", "dynam protocol replac", "servic interfac", "event-bas framework", "stack", "distribut system", "protocol framework", "repli", "modul", "network", "distribut algorithm"]}
{"doc": "A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages , whether planned or unplanned . In this paper we advocate a cooperative , context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner . We specifically seek to achieve high availability of data center services in the face of both planned and unanticipated outages of data center facilities . We make use of server virtualization technologies to enable the replication and migration of server functions . We propose new network functions to enable server migration and replication across wide area networks (e.g. , the Internet) , and finally show the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives.", "label": ["internet-based service", "data center migration", "network support", "voip", "storage replication", "synchronous replication", "asynchronous replication", "lan", "wan", "database", "storage", "virtual server", "voice-over-ip"], "stemmed_label": ["internet-bas servic", "data center migrat", "network support", "voip", "storag replic", "synchron replic", "asynchron replic", "lan", "wan", "databas", "storag", "virtual server", "voice-over-ip"]}
{"doc": "This paper proposes , implements , and evaluates in terms of worst case performance , an online metrics collection strategy to facilitate application adaptation via object mobility using a mobile object framework and supporting middleware . The solution is based upon an abstract representation of the mobile object system , which holds containers aggregating metrics for each specific component including host managers , runtimes and mobile objects . A key feature of the solution is the specification of multiple configurable criteria to control the measurement and propagation of metrics through the system . The MobJeX platform was used as the basis for implementation and testing with a number of laboratory tests conducted to measure scalability , efficiency and the application of simple measurement and propagation criteria to reduce collection overhead.", "label": ["mobile object framework", "data", "metricscontainer", "metric collection", "measurement", "propagation and delivery", "performance and scalability", "mobile object", "java", "framework", "proxy", "object-oriented application", "adaptation", "mobjex"], "stemmed_label": ["mobil object framework", "data", "metricscontain", "metric collect", "measur", "propag and deliveri", "perform and scalabl", "mobil object", "java", "framework", "proxi", "object-ori applic", "adapt", "mobjex"]}
{"doc": "The co-allocation architecture was developed in order to enable parallel downloading of datasets from multiple servers . Several co-allocation strategies have been coupled and used to exploit rate differences among various client-server links and to address dynamic rate fluctuations by dividing files into multiple blocks of equal sizes . However , a major obstacle , the idle time of faster servers having to wait for the slowest server to deliver the final block , makes it important to reduce differences in finishing time among replica servers . In this paper , we propose a dynamic coallocation scheme , namely Recursive-Adjustment Co-Allocation scheme , to improve the performance of data transfer in Data Grids . Our approach reduces the idle time spent waiting for the slowest server and decreases data transfer completion time . We also provide an effective scheme for reducing the cost of reassembling data blocks.", "label": ["co-allocation strategy", "server", "performance", "resource management protocol", "replication", "replica", "large dataset", "grid computing", "co-allocation", "data grid", "globus", "gridftp", "data transfer", "replica selection", "data grid application", "distributed resource"], "stemmed_label": ["co-alloc strategi", "server", "perform", "resourc manag protocol", "replic", "replica", "larg dataset", "grid comput", "co-alloc", "data grid", "globu", "gridftp", "data transfer", "replica select", "data grid applic", "distribut resourc"]}
{"doc": "The problem of localization of wireless sensor nodes has long been regarded as very difficult to solve , when considering the realities of real world environments . In this paper , we formally describe , design , implement and evaluate a novel localization system , called Spotlight . Our system uses the spatio-temporal properties of well controlled events in the network (e.g. , light) , to obtain the locations of sensor nodes . We demonstrate that a high accuracy in localization can be achieved without the aid of expensive hardware on the sensor nodes , as required by other localization systems . We evaluate the performance of our system in deployments of Mica2 and XSM motes . Through performance evaluations of a real system deployed outdoors , we obtain a 20cm localization error . A sensor network , with any number of nodes , deployed in a 2500m2 area , can be localized in under 10 minutes , using a device that costs less than $1000 . To the best of our knowledge , this is the first report of a sub-meter localization error , obtained in an outdoor environment , without equipping the wireless sensor nodes with specialized ranging hardware.", "label": ["range-based localization", "performance", "distribution", "event distribution", "localization", "localization error", "localization technique", "sensor network", "spotlight system", "laser", "transmission", "range-free scheme", "wireless sensor network", "accuracy"], "stemmed_label": ["range-bas local", "perform", "distribut", "event distribut", "local", "local error", "local techniqu", "sensor network", "spotlight system", "laser", "transmiss", "range-fre scheme", "wireless sensor network", "accuraci"]}
{"doc": "In this paper , we propose an adaptive task allocation framework to perform BLAST searches in a grid environment against sequence database segments . The framework , called PackageBLAST , provides an infrastructure to choose or incorporate task allocation strategies . Furthermore , we propose a mechanism to compute grid nodes execution weight , adapting the chosen allocation policy to the current computational power of the nodes . Our results present very good speedups and also show that no single allocation strategy is able to achieve the lowest execution times for all scenarios.", "label": ["blast search", "bioinformatic", "heterogeneous non-dedicated platform", "segmented genetic database", "grid environment", "packageblast", "grid computing", "task allocation", "pss", "package weighted adaptive self-scheduling", "adaptive multi-policy grid service", "genome project", "biological sequence comparison", "bioinformatics", "computational biology"], "stemmed_label": ["blast search", "bioinformat", "heterogen non-ded platform", "segment genet databas", "grid environ", "packageblast", "grid comput", "task alloc", "pss", "packag weight adapt self-schedul", "adapt multi-polici grid servic", "genom project", "biolog sequenc comparison", "bioinformat", "comput biolog"]}
{"doc": "CONFLEX-G is the grid-enabled version of a molecular conformational space search program called CONFLEX . We have implemented CONFLEX-G using a grid RPC system called OmniRPC . In this paper , we report the performance of CONFLEX-G in a grid testbed of several geographically distributed PC clusters . In order to explore many conformation of large bio-molecules , CONFLEX-G generates trial structures of the molecules and allocates jobs to optimize a trial structure with a reliable molecular mechanics method in the grid . OmniRPC provides a restricted persistence model to support the parametric search applications . In this model , when the initialization procedure is defined in the RPC module , the module is automatically initialized at the time of invocation by calling the initialization procedure . This can eliminate unnecessary communication and initialization at each call in CONFLEX-G . CONFLEXG can achieve performance comparable to CONFLEX MPI and can exploit more computing resources by allowing the use of a cluster of multiple clusters in the grid . The experimental result shows that CONFLEX-G achieved a speedup of 56.5 times in the case of the 1BL1 molecule , where the molecule consists of a large number of atoms , and each trial structure optimization requires significant time . The load imbalance of the optimization time of the trial structure may also cause performance degradation.", "label": ["molecular mechanic", "omnirpc", "rpc module", "conflex-g", "grid computing", "computational chemistry", "automatic initializable module", "grid rpc system", "initialization procedure", "conformational space search", "pc cluster", "mpus", "bio-molecule"], "stemmed_label": ["molecular mechan", "omnirpc", "rpc modul", "conflex-g", "grid comput", "comput chemistri", "automat initializ modul", "grid rpc system", "initi procedur", "conform space search", "pc cluster", "mpu", "bio-molecul"]}
{"doc": "Grids are inherently heterogeneous and dynamic . One important problem in grid computing is resource selection , that is , finding an appropriate resource set for the application . Another problem is adaptation to the changing characteristics of the grid environment . Existing solutions to these two problems require that a performance model for an application is known . However , constructing such models is a complex task . In this paper , we investigate an approach that does not require performance models . We start an application on any set of resources . During the application run , we periodically collect the statistics about the application run and deduce application requirements from these statistics . Then , we adjust the resource set to better fit the application needs . This approach allows us to avoid performance bottlenecks , such as overloaded WAN links or very slow processors , and therefore can yield significant performance improvements . We evaluate our approach in a number of scenarios typical for the Grid.", "label": ["resource selection", "divide-and-conquer", "self-adaptivity", "network link", "lower-bandwidth wide-area network", "grid environment", "degree of parallelism", "grid computing", "communication time", "heterogeneity of resource", "parallelism degree", "idle time of the processor", "overloaded resource", "homogeneous parallel environment", "the processor idle time", "high-bandwidth local-area network", "parallel computing", "resource heterogeneity"], "stemmed_label": ["resourc select", "divide-and-conqu", "self-adapt", "network link", "lower-bandwidth wide-area network", "grid environ", "degre of parallel", "grid comput", "commun time", "heterogen of resourc", "parallel degre", "idl time of the processor", "overload resourc", "homogen parallel environ", "the processor idl time", "high-bandwidth local-area network", "parallel comput", "resourc heterogen"]}
{"doc": "In recent years , overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet . Typically , nodes self-organize with the goal of forming an efficient overlay tree , one that meets performance targets without placing undue burden on the underlying network . In this paper , we target high-bandwidth data distribution from a single source to a large number of receivers . Applications include large-file transfers and real-time multimedia streaming . For these applications , we argue that an overlay mesh , rather than a tree , can deliver fundamentally higher bandwidth and reliability relative to typical tree structures . This paper presents Bullet , a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh . We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network . Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel . Key contributions of this work include: i) an algorithm that sends data to different points in the overlay such that any data object is equally likely to appear at any node , ii) a scalable and decentralized algorithm that allows nodes to locate and recover missing data items , and iii) a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances . In addition , we find that , relative to tree-based solutions , Bullet reduces the need to perform expensive bandwidth probing . In a tree , it is critical that a node\"s parent delivers a high rate of application data to each child . In Bullet however , nodes simultaneously receive data from multiple sources in parallel , making it less important to locate any single source capable of sustaining a high transmission rate.", "label": ["tfrc", "large-file transfer", "ip multicast", "content delivery", "bandwidth", "ransub", "overlay mesh", "high-bandwidth data distribution", "bullet", "multipoint communication", "bandwidth probing", "overlay", "peer-to-peer", "data dissemination", "real-time multimedia streaming", "overlay network"], "stemmed_label": ["tfrc", "large-fil transfer", "ip multicast", "content deliveri", "bandwidth", "ransub", "overlay mesh", "high-bandwidth data distribut", "bullet", "multipoint commun", "bandwidth probe", "overlay", "peer-to-p", "data dissemin", "real-tim multimedia stream", "overlay network"]}
{"doc": "Many organizations are required to author documents for various purposes , and such documents may need to be accessible by all member of the organization . This access may be needed for editing or simply viewing a document . In some cases these documents are shared between authors , via email , to be edited . This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document . There may even be multiple different documents in the process of being edited . The user may be required to search for a particular document , which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user\"s machine . Another problem arises when a document is made available on a user\"s machine and that user is offline , in which case the document is no longer accessible . In this paper we present Apocrita , a revolutionary distributed P2P file sharing system for Intranets.", "label": ["peer-to-peer distribution model", "document", "apocrita", "jxta", "distributed indexing", "index file", "file sharing system", "file share", "p2p", "idle query", "peer-to-peer", "intranet", "author", "p2p searching", "incoming file"], "stemmed_label": ["peer-to-p distribut model", "document", "apocrita", "jxta", "distribut index", "index file", "file share system", "file share", "p2p", "idl queri", "peer-to-p", "intranet", "author", "p2p search", "incom file"]}
{"doc": "Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task . They require strong consistency for shared persistent data and efficient access to fine-grained objects . These properties are difficult to provide in wide-area networks because of high network latency . BuddyCache is a new transactional caching approach that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments . The challenge is to improve performance while providing the correctness and availability properties of a transactional caching protocol in the presence of node failures and slow peers . We have implemented a BuddyCache prototype and evaluated its performance . Analytical results , confirmed by measurements of the BuddyCache prototype using the multiuser 007 benchmark indicate that for typical Internet latencies , e.g . ranging from 40 to 80 milliseconds round trip time to the storage server , peers using BuddyCache can reduce by up to 50% the latency of access to shared objects compared to accessing the remote servers directly.", "label": ["optimistic system", "cooperative cache", "wide-area network", "buddycache", "fine-grain share", "collaborative strong-consistency application", "cooperative web caching", "fault-tolerance property", "dominant performance cost", "fault-tolerance", "transaction", "multi-user oo7 benchmark", "peer fetch", "object storage system", "fine-grain sharing"], "stemmed_label": ["optimist system", "cooper cach", "wide-area network", "buddycach", "fine-grain share", "collabor strong-consist applic", "cooper web cach", "fault-toler properti", "domin perform cost", "fault-toler", "transact", "multi-us oo7 benchmark", "peer fetch", "object storag system", "fine-grain share"]}
{"doc": "How to provide appropriate context information is a challenging problem in context-aware computing . Most existing approaches use a centralized selection mechanism to decide which context information is appropriate . In this paper , we propose a novel approach based on negotiation with rewards to solving such problem . Distributed context providers negotiate with each other to decide who can provide context and how they allocate proceeds . In order to support our approach , we have designed a concrete negotiation model with rewards . We also evaluate our approach and show that it indeed can choose an appropriate context provider and allocate the proceeds fairly.", "label": ["concrete negotiation model", "pervasive computing", "quality of context", "context provider", "context-awareness", "persuasive argument", "context quality", "reputation", "negotiation", "distributed application", "context-aware computing"], "stemmed_label": ["concret negoti model", "pervas comput", "qualiti of context", "context provid", "context-awar", "persuas argument", "context qualiti", "reput", "negoti", "distribut applic", "context-awar comput"]}
{"doc": "Security schemes of pairwise key establishment , which enable sensors to communicate with each other securely , play a fundamental role in research on security issue in wireless sensor networks . A new kind of cluster deployed sensor networks distribution model is presented , and based on which , an innovative Hierarchical Hypercube model - H(k,u,m,v,n) and the mapping relationship between cluster deployed sensor networks and the H(k,u,m,v,n) are proposed . By utilizing nice properties of H(k,u,m,v,n) model , a new general framework for pairwise key predistribution and a new pairwise key establishment algorithm are designed , which combines the idea of KDC(Key Distribution Center) and polynomial pool schemes . Furthermore , the working performance of the newly proposed pairwise key establishment algorithm is seriously inspected . Theoretic analysis and experimental figures show that the new algorithm has better performance and provides higher possibilities for sensor to establish pairwise key , compared with previous related works.", "label": ["key pool", "node code", "high fault-tolerance", "hierarchical hypercube model", "pairwise key establishment algorithm", "cluster-based distribution model", "pairwise key", "encryption", "sensor network", "polynomial key", "security", "key predistribution"], "stemmed_label": ["key pool", "node code", "high fault-toler", "hierarch hypercub model", "pairwis key establish algorithm", "cluster-bas distribut model", "pairwis key", "encrypt", "sensor network", "polynomi key", "secur", "key predistribut"]}
{"doc": "Publish/subscribe systems provide an efficient , event-based , wide-area distributed communications infrastructure . Large scale publish/subscribe systems are likely to employ components of the event transport network owned by cooperating , but independent organisations . As the number of participants in the network increases , security becomes an increasing concern . This paper extends previous work to present and evaluate a secure multi-domain publish/subscribe infrastructure that supports and enforces fine-grained access control over the individual attributes of event types . Key refresh allows us to ensure forward and backward security when event brokers join and leave the network . We demonstrate that the time and space overheads can be minimised by careful consideration of encryption techniques , and by the use of caching to decrease unnecessary decryptions . We show that our approach has a smaller overall communication overhead than existing approaches for achieving the same degree of control over security in publish/subscribe networks.", "label": ["secure publish/subscribe system", "distribute access control", "multi-domain", "performance", "distributed access control", "distributed systems-distributed application", "overall communication overhead", "encryption", "multiple administrative domain", "administrative domain", "congestion charge service", "attribute encryption"], "stemmed_label": ["secur publish/subscrib system", "distribut access control", "multi-domain", "perform", "distribut access control", "distribut systems-distribut applic", "overal commun overhead", "encrypt", "multipl administr domain", "administr domain", "congest charg servic", "attribut encrypt"]}
{"doc": "This paper presents a simple and scalable framework for architecting peer-to-peer overlays called Peer-to-peer Receiverdriven Overlay (or PRO) . PRO is designed for non-interactive streaming applications and its primary design goal is to maximize delivered bandwidth (and thus delivered quality) to peers with heterogeneous and asymmetric bandwidth . To achieve this goal , PRO adopts a receiver-driven approach where each receiver (or participating peer) (i) independently discovers other peers in the overlay through gossiping , and (ii) selfishly determines the best subset of parent peers through which to connect to the overlay to maximize its own delivered bandwidth . Participating peers form an unstructured overlay which is inherently robust to high churn rate . Furthermore , each receiver leverages congestion controlled bandwidth from its parents as implicit signal to detect and react to long-term changes in network or overlay condition without any explicit coordination with other participating peers . Independent parent selection by individual peers dynamically converge to an efficient overlay structure.", "label": ["receiver-driven parent selection", "efficient overlay structure", "congestion control", "measurement", "peer-to-peer stream", "distributed system", "receiver-driven overlay", "receiver-driven approach", "design", "pro", "gossip-based peer discovery", "proper subset of parent peer", "peer-to-peer streaming"], "stemmed_label": ["receiver-driven parent select", "effici overlay structur", "congest control", "measur", "peer-to-p stream", "distribut system", "receiver-driven overlay", "receiver-driven approach", "design", "pro", "gossip-bas peer discoveri", "proper subset of parent peer", "peer-to-p stream"]}
{"doc": "Best effort packet-switched networks , like the Internet , do not offer a reliable transmission of packets to applications with real-time constraints such as voice . Thus , the loss of packets impairs the application-level utility . For voice this utility impairment is twofold: on one hand , even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal playout is interrupted . On the other hand , some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets . We first develop an end-to-end model based on loss runlengths with which we can describe the loss distribution within a flow . These packet-level metrics are then linked to user-level objective speech quality metrics . Using this framework , we find that for low-compressing sample-based codecs (PCM) with loss concealment isolated packet losses can be concealed well , whereas burst losses have a higher perceptual impact . For high-compressing frame-based codecs (G.729) on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories , though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state . Contrary to sample-based codecs we show that the concealment performance may break at transitions within the speech signal however . We then propose mechanisms which differentiate between packets within a voice data flow to minimize the impact of packet loss . We designate these methods as intra-flow loss recovery and control . At the end-to-end level , identification of packets sensitive to loss (sender) as well as loss concealment (receiver) takes place . Hop-by-hop support schemes then allow to (statistically) trade the loss of one packet , which is considered more important , against another one of the same flow which is of lower importance . As both packets require the same cost in terms of network transmission , a gain in user perception is obtainable . We show that significant speech quality improvements can be achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term.", "label": ["loss metric", "quality of service", "packet-level metric", "frame-based codec", "voip traffic sensitivity", "queue management algorithm", "end-to-end loss recovery", "service quality", "loss sensitivity", "sensitivity of voip traffic", "voice over ip", "end-to-end model", "objective speech quality measurement", "voip traffic", "intra-flow loss control", "general markov model", "loss concealment", "differentiate service", "network support for real-time multimedia", "loss recovery and control", "sample-based codec", "queue management"], "stemmed_label": ["loss metric", "qualiti of servic", "packet-level metric", "frame-bas codec", "voip traffic sensit", "queue manag algorithm", "end-to-end loss recoveri", "servic qualiti", "loss sensit", "sensit of voip traffic", "voic over ip", "end-to-end model", "object speech qualiti measur", "voip traffic", "intra-flow loss control", "gener markov model", "loss conceal", "differenti servic", "network support for real-tim multimedia", "loss recoveri and control", "sample-bas codec", "queue manag"]}
{"doc": "Newly emerging game-based application systems such as Second Life1 provide 3D virtual environments where multiple users interact with each other in real-time . They are filled with autonomous , mutable virtual content which is continuously augmented by the users . To make the systems highly scalable and dynamically extensible , they are usually built on a client-server based grid subspace division where the virtual worlds are partitioned into manageable sub-worlds . In each sub-world , the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint , rather than retrieving them from a local storage medium . In such systems , the determination of the set of objects that are visible from a user\"s viewpoint is one of the primary factors that affect server throughput and scalability . Specifically , performing real-time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting . We recognize that the described challenges are closely related to a spatial database problem , and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query . Unfortunately , existing spatial indexing methods are unsuitable for this kind of new environments . The main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real-time visibility determination . We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality , system throughput , and resource utilization . We expect our approach to lay the groundwork for next-generation virtual frameworks that may merge into existing web-based services in the near future.", "label": ["dynamic virtual environment", "mutable virtual content", "real-time visibility test", "3d object stream", "object pop problem", "object popping", "visibility model", "game-based application", "object-initiated view model", "spatial database", "edge indexing", "spatial index", "3d spatial extension", "spatial indexing method"], "stemmed_label": ["dynam virtual environ", "mutabl virtual content", "real-tim visibl test", "3d object stream", "object pop problem", "object pop", "visibl model", "game-bas applic", "object-initi view model", "spatial databas", "edg index", "spatial index", "3d spatial extens", "spatial index method"]}
{"doc": "The convergence of advances in storage , encoding , and networking technologies has brought us to an environment where huge amounts of continuous media content is routinely stored and exchanged between network enabled devices . Keeping track of (or managing) such content remains challenging due to the sheer volume of data . Storing live continuous media (such as TV or radio content) adds to the complexity in that this content has no well defined start or end and is therefore cumbersome to deal with . Networked storage allows content that is logically viewed as part of the same collection to in fact be distributed across a network , making the task of content management all but impossible to deal with without a content management system . In this paper we present the design and implementation of the Spectrum content management system , which deals with rich media content effectively in this environment . Spectrum has a modular architecture that allows its application to both stand-alone and various networked scenarios . A unique aspect of Spectrum is that it requires one (or more) retention policies to apply to every piece of content that is stored in the system . This means that there are no eviction policies . Content that no longer has a retention policy applied to it is simply removed from the system . Different retention policies can easily be applied to the same content thus naturally facilitating sharing without duplication . This approach also allows Spectrum to easily apply time based policies which are basic building blocks required to deal with the storage of live continuous media , to content . We not only describe the details of the Spectrum architecture but also give typical use cases.", "label": ["network enabled dvr", "policy manager", "uniform resource locator", "spectrum content management system", "distribute content management", "carrier-grade spectrum manager", "high-performance database system", "continuous media storage", "application program interface", "home-networking scenario", "content distribution network"], "stemmed_label": ["network enabl dvr", "polici manag", "uniform resourc locat", "spectrum content manag system", "distribut content manag", "carrier-grad spectrum manag", "high-perform databas system", "continu media storag", "applic program interfac", "home-network scenario", "content distribut network"]}
{"doc": "Operational Transformation (OT) is a technique for consistency maintenance and group undo , and is being applied to an increasing number of collaborative applications . The theoretical foundation for OT is crucial in determining its capability to solve existing and new problems , as well as the quality of those solutions . The theory of causality has been the foundation of all prior OT systems , but it is inadequate to capture essential correctness requirements . Past research had invented various patches to work around this problem , resulting in increasingly intricate and complicated OT algorithms . After having designed , implemented , and experimented with a series of OT algorithms , we reflected on what had been learned and set out to develop a new theoretical framework for better understanding and resolving OT problems , reducing its complexity , and supporting its continual evolution . In this paper , we report the main results of this effort: the theory of operation context and the COT (Context-based OT) algorithm . The COT algorithm is capable of supporting both do and undo of any operations at anytime , without requiring transformation functions to preserve Reversibility Property , Convergence Property 2 , Inverse Properties 2 and 3 . The COT algorithm is not only simpler and more efficient than prior OT control algorithms , but also simplifies the design of transformation functions . We have implemented the COT algorithm in a generic collaboration engine and used it for supporting a range of novel collaborative applications.", "label": ["context-base ot", "vector representation of operation context", "concurrency relation", "exclusion transformation", "operation context vector representation", "causal-dependency", "concurrency condition", "cot", "operation context", "group editor", "distribute application", "context-based ot", "undo", "document state", "inverse cluster", "consistency maintenance", "inverse operation", "operational transformation", "original operation", "history buffer", "transformed operation", "ot"], "stemmed_label": ["context-bas ot", "vector represent of oper context", "concurr relat", "exclus transform", "oper context vector represent", "causal-depend", "concurr condit", "cot", "oper context", "group editor", "distribut applic", "context-bas ot", "undo", "document state", "invers cluster", "consist mainten", "invers oper", "oper transform", "origin oper", "histori buffer", "transform oper", "ot"]}
{"doc": "While market-based systems have long been proposed as solutions for distributed resource allocation , few have been deployed for production use in real computer systems . Towards this end , we present our initial experience using Mirage , a microeconomic resource allocation system based on a repeated combinatorial auction . Mirage allocates time on a heavily-used 148-node wireless sensor network testbed . In particular , we focus on observed strategic user behavior over a four-month period in which 312,148 node hours were allocated across 11 research projects . Based on these results , we present a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions . Finally , we propose refinements to the system\"s current auction scheme to mitigate the strategies observed to date and also comment on some initial steps toward building an approximately strategyproof repeated combinatorial auction.", "label": [""], "stemmed_label": [""]}
{"doc": "As the idea of virtualisation of compute power , storage and bandwidth becomes more and more important , grid computing evolves and is applied to a rising number of applications . The environment for decentralized adaptive services (EDAS) provides a grid-like infrastructure for user-accessed , longterm services (e.g . webserver , source-code repository etc.) . It aims at supporting the autonomous execution and evolution of services in terms of scalability and resource-aware distribution . EDAS offers flexible service models based on distributed mobile objects ranging from a traditional clientserver scenario to a fully peer-to-peer based approach . Automatic , dynamic resource management allows optimized use of available resources while minimizing the administrative complexity.", "label": ["local limit", "client", "node", "decentralized adaptive service", "home environment", "infrastructure", "grid computing", "global limit", "adaptability", "long-term service", "resource", "fragment object", "resource management", "eda"], "stemmed_label": ["local limit", "client", "node", "decentr adapt servic", "home environ", "infrastructur", "grid comput", "global limit", "adapt", "long-term servic", "resourc", "fragment object", "resourc manag", "eda"]}
{"doc": "In recent years , document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization , automatic topic extraction , and fast information retrieval or filtering . In this paper , we propose a novel method for clustering documents using regularization . Unlike traditional globally regularized clustering methods , our method first construct a local regularized linear label predictor for each document vector , and then combine all those local regularizers with a global smoothness regularizer . So we call our algorithm Clustering with Local and Global Regularization (CLGR) . We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix , which can be efficiently solved by iterative methods . Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.", "label": ["global regularization", "specified search", "partitioning method", "label prediction", "manifold", "function estimation", "document cluster", "regularization", "spectrum", "hierarchical method", "cluster hierarchy", "document clustering"], "stemmed_label": ["global regular", "specifi search", "partit method", "label predict", "manifold", "function estim", "document cluster", "regular", "spectrum", "hierarch method", "cluster hierarchi", "document cluster"]}
{"doc": "Relevance feedback is a powerful technique to enhance ContentBased Image Retrieval (CBIR) performance . It solicits the user\"s relevance judgments on the retrieved images returned by the CBIR systems . The user\"s labeling is then used to learn a classifier to distinguish between relevant and irrelevant images . However , the top returned images may not be the most informative ones . The challenge is thus to determine which unlabeled images would be the most informative (i.e. , improve the classifier the most) if they were labeled and used as training samples . In this paper , we propose a novel active learning algorithm , called Laplacian Optimal Design (LOD) , for relevance feedback image retrieval . Our algorithm is based on a regression model which minimizes the least square error on the measured (or , labeled) images and simultaneously preserves the local geometrical structure of the image space . Specifically , we assume that if two images are sufficiently close to each other , then their measurements (or , labels) are close as well . By constructing a nearest neighbor graph , the geometrical structure of the image space can be described by the graph Laplacian . We discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of images , which gives us the most amount of information . Experimental results on Corel database suggest that the proposed approach achieves higher precision in relevance feedback image retrieval.", "label": ["contentbased image retrieval", "patten recognition", "intrinsic geometrical structure", "labelling", "active learn", "active learning", "image representation", "least square regression model", "top returned image", "precision rate", "relevance feedback", "optimal experimental design", "image retrieval"], "stemmed_label": ["contentbas imag retriev", "patten recognit", "intrins geometr structur", "label", "activ learn", "activ learn", "imag represent", "least squar regress model", "top return imag", "precis rate", "relev feedback", "optim experiment design", "imag retriev"]}
{"doc": "The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users . In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets . We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library . These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets , and so caching documents in RAM is essential for a fast snippet generation process . Using simulation , we examine snippet generation performance for different size RAM caches . Finally we propose and analyse document reordering and compaction , revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality . This scheme effectively doubles the number of documents that can fit in a fixed size cache.", "label": ["special-purpose filesystem", "snippet generation", "ram", "performance", "web summary", "semi-static compression", "vbyte coding scheme", "document cache", "search engine", "link graph measure", "document compaction", "document caching", "precomputed final result page", "text fragment"], "stemmed_label": ["special-purpos filesystem", "snippet gener", "ram", "perform", "web summari", "semi-stat compress", "vbyte code scheme", "document cach", "search engin", "link graph measur", "document compact", "document cach", "precomput final result page", "text fragment"]}
{"doc": "Web search engines present lists of captions , comprising title , snippet , and URL , to help users decide which search results to visit . Understanding the influence of features of these captions on Web search behavior may help validate algorithms and guidelines for their improved generation . In this paper we develop a methodology to use clickthrough logs from a commercial search engine to study user behavior when interacting with search result captions . The findings of our study suggest that relatively simple caption features such as the presence of all terms query terms , the readability of the snippet , and the length of the URL shown in the caption , can significantly influence users\" Web search behavior.", "label": ["summarization", "clickthrough pattern", "snippet", "extractive summarization", "web search behavior", "significant word", "web search", "query log", "human factor", "clickthrough inversion", "query term match", "caption feature", "query re-formulation"], "stemmed_label": ["summar", "clickthrough pattern", "snippet", "extract summar", "web search behavior", "signific word", "web search", "queri log", "human factor", "clickthrough invers", "queri term match", "caption featur", "queri re-formul"]}
{"doc": "We present a novel Web search interaction feature which , for a given query , provides links to websites frequently visited by other users with similar information needs . These popular destinations complement traditional search results , allowing direct navigation to authoritative resources for the query topic . Destinations are identified using the history of search and browsing behavior of many users over an extended time period , whose collective behavior provides a basis for computing source authority . We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries , as well as with traditional , unaided Web search . Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks , with best performance obtained from mining past user behavior at query-level granularity.", "label": ["web search interaction", "search destination", "popular destination", "information-seeking experience", "retrieval performance", "improving query", "user study", "log-based evaluation", "enhance web search", "session trail", "related query", "lookup-based approach", "query trail"], "stemmed_label": ["web search interact", "search destin", "popular destin", "information-seek experi", "retriev perform", "improv queri", "user studi", "log-bas evalu", "enhanc web search", "session trail", "relat queri", "lookup-bas approach", "queri trail"]}
{"doc": "In this paper we study the trade-offs in designing efficient caching systems for Web search engines . We explore the impact of different approaches , such as static vs . dynamic caching , and caching query results vs . caching posting lists . Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers . We propose a new algorithm for static caching of posting lists , which outperforms previous methods . We also study the problem of finding the optimal way to split the static cache between answers and posting lists . Finally , we measure how the changes in the query log affect the effectiveness of static caching , given our observation that the distribution of the queries changes slowly over time . Our results and observations are applicable to different levels of the data-access hierarchy , for instance , for a memory/disk layer or a broker/remote server layer.", "label": ["static caching effectiveness", "caching posting list", "static cache", "cache", "static caching", "dynamic caching", "caching query result", "query log", "efficient caching system", "effectiveness of static caching", "disk layer", "web search", "data-access hierarchy", "web search engine", "distribution of the query", "information retrieval system", "answer and posting list", "remote server layer", "the query distribution"], "stemmed_label": ["static cach effect", "cach post list", "static cach", "cach", "static cach", "dynam cach", "cach queri result", "queri log", "effici cach system", "effect of static cach", "disk layer", "web search", "data-access hierarchi", "web search engin", "distribut of the queri", "inform retriev system", "answer and post list", "remot server layer", "the queri distribut"]}
{"doc": "The Web search engines maintain large-scale inverted indexes which are queried thousands of times per second by users eager for information . In order to cope with the vast amounts of query loads , search engines prune their index to keep documents that are likely to be returned as top results , and use this pruned index to compute the first batches of results . While this approach can improve performance by reducing the size of the index , if we compute the top results only from the pruned index we may notice a significant degradation in the result quality: if a document should be in the top results but was not included in the pruned index , it will be placed behind the results computed from the pruned index . Given the fierce competition in the online search market , this phenomenon is clearly undesirable . In this paper , we study how we can avoid any degradation of result quality due to the pruning-based performance optimization , while still realizing most of its benefit . Our contribution is a number of modifications in the pruning techniques for creating the pruned index and a new result computation algorithm that guarantees that the top-matching pages are always placed at the top search results , even though we are computing the first batch from the pruned index most of the time . We also show how to determine the optimal size of a pruned index and we experimentally evaluate our algorithms on a collection of 130 million Web pages.", "label": ["large-scale inverted index", "degradation of result quality", "invert index", "result computation algorithm", "prune", "correctness guarantee", "result quality degradation", "pruning-based performance optimization", "top search result", "optimal size", "web search engine", "pruning technique", "top-matching page", "pruned index", "online search market", "query load"], "stemmed_label": ["large-scal invert index", "degrad of result qualiti", "invert index", "result comput algorithm", "prune", "correct guarante", "result qualiti degrad", "pruning-bas perform optim", "top search result", "optim size", "web search engin", "prune techniqu", "top-match page", "prune index", "onlin search market", "queri load"]}
{"doc": "Topic detection and tracking 26 and topic segmentation 15 play an important role in capturing the local and sequential information of documents . Previous work in this area usually focuses on single documents , although similar multiple documents are available in many domains . In this paper , we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights . The basic idea is that the optimal segmentation maximizes MI(or WMI) . Our approach can detect shared topics among documents . It can find the optimal boundaries in a document , and align segments among documents at the same time . It also can handle single-document segmentation as a special case of the multi-document segmentation and alignment . Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents . Our experimental results show that our algorithm works well for the tasks of single-document segmentation , shared topic detection , and multi-document segmentation . Utilizing information from multiple documents can tremendously improve the performance of topic segmentation , and using WMI is even better than using MI for the multi-document segmentation.", "label": ["term weight", "single-document segmentation", "multiple document", "performance of topic segmentation", "topic segmentation performance", "cue term", "wmus", "shared topic", "multi-document segmentation", "local and sequential information of document", "topic detection", "optimal boundary", "tracking", "stop word", "topic alignment", "mutual information", "topic segmentation", "single document", "document local and sequential information", "share topic detection"], "stemmed_label": ["term weight", "single-docu segment", "multipl document", "perform of topic segment", "topic segment perform", "cue term", "wmu", "share topic", "multi-docu segment", "local and sequenti inform of document", "topic detect", "optim boundari", "track", "stop word", "topic align", "mutual inform", "topic segment", "singl document", "document local and sequenti inform", "share topic detect"]}
{"doc": "We consider the problem of analyzing word trajectories in both time and frequency domains , with the specific goal of identifying important and less-reported , periodic and aperiodic words . A set of words with identical trends can be grouped together to reconstruct an event in a completely unsupervised manner . The document frequency of each word across time is treated like a time series , where each element is the document frequency - inverse document frequency (DFIDF) score at one time point . In this paper , we 1) first applied spectral analysis to categorize features for different event characteristics: important and less-reported , periodic and aperiodic; 2) modeled aperiodic features with Gaussian density and periodic features with Gaussian mixture densities , and subsequently detected each feature\"s burst by the truncated Gaussian approach; 3) proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events . All of the above methods can be applied to time series data in general . We extensively evaluated our methods on the 1-year Reuters News Corpus 3 and showed that they were able to uncover meaningful aperiodic and periodic events.", "label": ["feature categorization", "gaussian", "dft", "aperiodic event", "text stream", "topic tracking", "word signal", "topic detection", "time series", "news stream", "periodic event", "word trajectory", "event detection", "spectral analysis"], "stemmed_label": ["featur categor", "gaussian", "dft", "aperiod event", "text stream", "topic track", "word signal", "topic detect", "time seri", "news stream", "period event", "word trajectori", "event detect", "spectral analysi"]}
{"doc": "The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval . In this paper we propose to improve such Web queries by expanding them with terms collected from each user\"s Personal Information Repository , thus implicitly personalizing the search output . We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels , ranging from term and compound level analysis up to global co-occurrence statistics , as well as to using external thesauri . Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well , especially on ambiguous queries , producing a very strong increase in the quality of the output rankings . Subsequently , we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query . A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.", "label": ["personalize web search", "external thesaurus", "static expansion approach", "each query various feature", "web query", "web retrieval", "granularity level", "search output", "personalized search framework", "adaptive algorithm", "short keyword query", "expansion process", "extensive empirical analysis", "desktop profile", "query expansion", "keyword extraction", "term and compound level analysis", "output ranking", "personal information repository", "keyword co-occurrence", "additional query keyword", "global co-occurrence statistics", "significant improvement", "quality", "various feature of each query", "ambiguous query"], "stemmed_label": ["person web search", "extern thesauru", "static expans approach", "each queri variou featur", "web queri", "web retriev", "granular level", "search output", "person search framework", "adapt algorithm", "short keyword queri", "expans process", "extens empir analysi", "desktop profil", "queri expans", "keyword extract", "term and compound level analysi", "output rank", "person inform repositori", "keyword co-occurr", "addit queri keyword", "global co-occurr statist", "signific improv", "qualiti", "variou featur of each queri", "ambigu queri"]}
{"doc": "New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e . not reported previously) . With the overwhelming volume of news available today , there is an increasing need for a NED system which is able to detect new events more efficiently and accurately . In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically . Moreover , based on the observation that terms of different types have different effects for NED task , two term reweighting approaches are proposed to improve NED accuracy . In the first approach , we propose to adjust term weights dynamically based on previous story clusters and in the second approach , we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories . Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly , compared to the baseline system and other existing systems.", "label": ["term weight", "baseline system", "topic detection and track", "name entity", "new event detection", "training data", "volume of news", "news story stream", "named entity reweighting mode", "linguistic data consortium", "news indexing-tree", "class of story", "term reweighting approach", "real-time index", "story class", "stream of news story", "ned accuracy", "news volume", "speed up the ned task", "existing system", "statistics"], "stemmed_label": ["term weight", "baselin system", "topic detect and track", "name entiti", "new event detect", "train data", "volum of news", "news stori stream", "name entiti reweight mode", "linguist data consortium", "news indexing-tre", "class of stori", "term reweight approach", "real-tim index", "stori class", "stream of news stori", "ned accuraci", "news volum", "speed up the ned task", "exist system", "statist"]}
{"doc": "We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy , while dealing in realtime with the query volume of a commercial web search engine . We use a blind feedback technique: given a query , we determine its topic by classifying the web search results retrieved by the query . Motivated by the needs of search advertising , we primarily focus on rare queries , which are the hardest from the point of view of machine learning , yet in aggregation account for a considerable fraction of search engine traffic . Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported . We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.", "label": ["blind relevance feedback", "conditional probability", "relevance feedback", "affinity score", "web search", "voting scheme", "search advertising", "crawling", "topical taxonomy", "information retrieval", "machine learning", "search engine", "adaptation", "query classification"], "stemmed_label": ["blind relev feedback", "condit probabl", "relev feedback", "affin score", "web search", "vote scheme", "search advertis", "crawl", "topic taxonomi", "inform retriev", "machin learn", "search engin", "adapt", "queri classif"]}
{"doc": "One way to help all users of commercial Web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing , and use this knowledge to benefit everyone . In this paper we study the interaction logs of advanced search engine users (and those not so advanced) to better understand how these user groups search . The results show that there are marked differences in the queries , result clicks , post-query browsing , and search success of users we classify as advanced (based on their use of query operators) , relative to those classified as non-advanced . Our findings have implications for how advanced users should be supported during their searches , and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies.", "label": ["relevance", "relevance feedback", "navigation behavior", "expert search", "search behavior", "advance search feature", "searching strategy", "advanced syntax", "query", "query syntax", "search engine", "search success", "tolerable latency", "relevant information"], "stemmed_label": ["relev", "relev feedback", "navig behavior", "expert search", "search behavior", "advanc search featur", "search strategi", "advanc syntax", "queri", "queri syntax", "search engin", "search success", "toler latenc", "relev inform"]}
{"doc": "In this paper we study term-based feedback for information retrieval in the language modeling approach . With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents , taking full control of the query expansion process . We propose a cluster-based method for selecting terms to present to the user for judgment , as well as effective algorithms for constructing refined query language models from user term feedback . Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline , and achieve comparable performance to relevance feedback . They are helpful even when there are no relevant documents in the top.", "label": ["interactive retrieval", "query model", "language modeling", "kl-divergence", "query expansion", "retrieval performance", "interactive adhoc search", "probability", "information retrieval", "term-based feedback", "query expansion process", "presentation term"], "stemmed_label": ["interact retriev", "queri model", "languag model", "kl-diverg", "queri expans", "retriev perform", "interact adhoc search", "probabl", "inform retriev", "term-bas feedback", "queri expans process", "present term"]}
{"doc": "Machine learning is commonly used to improve ranked retrieval systems . Due to computational difficulties , few learning techniques have been developed to directly optimize for mean average precision (MAP) , despite its widespread use in evaluating such systems . Existing approaches optimizing MAP either do not find a globally optimal solution , or are computationally expensive . In contrast , we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP . We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g) , comparing against SVMs optimized for accuracy and ROCArea . In most cases we show our method to produce statistically significant improvements in MAP scores.", "label": ["support vector machine", "relaxation of map", "learning technique", "mean average precision", "map relaxation", "surrogate measure", "rank", "supervised learning", "ranked retrieval system", "loss function", "machine learn for information retrieval", "information retrieval system", "optimal solution", "probability", "machine learning"], "stemmed_label": ["support vector machin", "relax of map", "learn techniqu", "mean averag precis", "map relax", "surrog measur", "rank", "supervis learn", "rank retriev system", "loss function", "machin learn for inform retriev", "inform retriev system", "optim solut", "probabl", "machin learn"]}
{"doc": "Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents , but ignore an important statistical dimension: the risk or variance associated with either the individual document models , or their combination . Treating the baseline feedback method as a black box , and the output feedback model as a random variable , we estimate a posterior distribution for the feedback model by resampling a given query\"s top-retrieved documents , using the posterior mean or mode as the enhanced feedback model . We then perform model combination over several enhanced models , each based on a slightly modified query sampled from the original query . We find that resampling documents helps increase individual feedback model precision by removing noise terms , while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects . The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.", "label": ["language modeling", "query expansion", "feedback model", "pseudo-relevance feedback", "probability distribution", "risk", "enhanced feedback model", "vector space-based algorithm", "posterior distribution", "estimating uncertainty", "information retrieval", "feedback method", "feedback distribution"], "stemmed_label": ["languag model", "queri expans", "feedback model", "pseudo-relev feedback", "probabl distribut", "risk", "enhanc feedback model", "vector space-bas algorithm", "posterior distribut", "estim uncertainti", "inform retriev", "feedback method", "feedback distribut"]}
{"doc": "User query is an element that specifies an information need , but it is not the only one . Studies in literature have found many contextual factors that strongly influence the interpretation of a query . Recent studies have tried to consider the user\"s interests by creating a user profile . However , a single profile for a user may not be sufficient for a variety of queries of the user . In this study , we propose to use query-specific contexts instead of user-centric ones , including context around query and context within query . The former specifies the environment of a query such as the domain of interest , while the latter refers to context words within the query , which is particularly useful for the selection of relevant term relations . In this paper , both types of context are integrated in an IR model based on language modeling . Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.", "label": ["knowledge ambiguity problem", "term relation", "interest domain", "domain of interest", "query context", "information need", "radical solution", "domain knowledge", "utilization of general knowledge", "general knowledge utilization", "context-independent", "user-centric one", "context information", "domain model", "context factor", "google personalized search", "user profile", "language model", "problem of knowledge ambiguity", "search context", "word sense disambiguation", "query-specific context"], "stemmed_label": ["knowledg ambigu problem", "term relat", "interest domain", "domain of interest", "queri context", "inform need", "radic solut", "domain knowledg", "util of gener knowledg", "gener knowledg util", "context-independ", "user-centr one", "context inform", "domain model", "context factor", "googl person search", "user profil", "languag model", "problem of knowledg ambigu", "search context", "word sens disambigu", "query-specif context"]}
{"doc": "Query expansion , in the form of pseudo-relevance feedback or relevance feedback , is a common technique used to improve retrieval effectiveness . Most previous approaches have ignored important issues , such as the role of features and the importance of modeling term dependencies . In this paper , we propose a robust query expansion technique based on the Markov random field model for information retrieval . The technique , called latent concept expansion , provides a mechanism for modeling term dependencies during expansion . Furthermore , the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features that are implicitly used by most other expansion techniques . We evaluate our technique against relevance models , a state-of-the-art language modeling query expansion technique . Our model demonstrates consistent and significant improvements in retrieval effectiveness across several TREC data sets . We also describe how our technique can be used to generate meaningful multi-term concepts for tasks such as query suggestion/reformulation.", "label": ["language modeling framework", "rocchio algorithm", "document routing", "query expansion", "mrf", "mrf model", "web search", "language modeling approach", "pseudo-relevance feedback", "markov random field", "rm3", "robust query expansion technique", "information retrieval", "language modeling query expansion technique", "relevance feedback", "relevance distribution", "ad-hoc retrieval"], "stemmed_label": ["languag model framework", "rocchio algorithm", "document rout", "queri expans", "mrf", "mrf model", "web search", "languag model approach", "pseudo-relev feedback", "markov random field", "rm3", "robust queri expans techniqu", "inform retriev", "languag model queri expans techniqu", "relev feedback", "relev distribut", "ad-hoc retriev"]}
{"doc": "Many variants of language models have been proposed for information retrieval . Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model . In this paper , we propose and study a new family of query generation models based on Poisson distribution . We show that while in their simplest forms , the new family of models and the existing multinomial models are equivalent , they behave differently for many smoothing methods . We show that the Poisson model has several advantages over the multinomial model , including naturally accommodating per-term smoothing and allowing for more accurate background modeling . We present several variants of the new model corresponding to different smoothing methods , and evaluate them on four representative TREC test collections . The results show that while their basic models perform comparably , the Poisson model can outperform multinomial model with per-term smoothing . The performance can be further improved with two-stage smoothing.", "label": ["language model", "multinomial distribution", "speech recognition", "term dependent smooth", "query generation probabilistic model", "query generation", "two-stage smoothing", "formal model", "vocabulary set", "multivariate bernoullus distribution", "new term-dependent smoothing algorithm", "perterm smoothing", "poisson process", "poisson distribution", "term frequency", "homogeneous poisson process", "single pseudo term"], "stemmed_label": ["languag model", "multinomi distribut", "speech recognit", "term depend smooth", "queri gener probabilist model", "queri gener", "two-stag smooth", "formal model", "vocabulari set", "multivari bernoullu distribut", "new term-depend smooth algorithm", "perterm smooth", "poisson process", "poisson distribut", "term frequenc", "homogen poisson process", "singl pseudo term"]}
{"doc": "Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets . This is insufficient as they do not address the novelty factor that a definitional nugget must also possess . This paper proposes to address the deficiency by building a Human Interest Model from external knowledge . It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic . We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.", "label": ["sentence fragment", "human interest", "unique quality", "use of linguistic", "interesting nugget", "manual labor", "news corpus", "baseline system", "human interest computation", "definitional question answer", "informative nugget", "human reader", "linguistic use", "interest", "question topic", "external knowledge", "computation of human interest", "surprise factor", "lexical pattern"], "stemmed_label": ["sentenc fragment", "human interest", "uniqu qualiti", "use of linguist", "interest nugget", "manual labor", "news corpu", "baselin system", "human interest comput", "definit question answer", "inform nugget", "human reader", "linguist use", "interest", "question topic", "extern knowledg", "comput of human interest", "surpris factor", "lexic pattern"]}
{"doc": "Personal Information Management (PIM) is a rapidly growing area of research concerned with how people store , manage and re-find information . A feature of PIM research is that many systems have been designed to assist users manage and re-find information , but very few have been evaluated . This has been noted by several scholars and explained by the difficulties involved in performing PIM evaluations . The difficulties include that people re-find information from within unique personal collections; researchers know little about the tasks that cause people to re-find information; and numerous privacy issues concerning personal information . In this paper we aim to facilitate PIM evaluations by addressing each of these difficulties . In the first part , we present a diary study of information re-finding tasks . The study examines the kind of tasks that require users to re-find information and produces a taxonomy of re-finding tasks for email messages and web pages . In the second part , we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using two different methods of task creation.", "label": ["measurement", "taxonomy", "re-find information", "user evaluation", "human factor", "experimenter", "email message", "individual collection", "privacy issue", "naturalistic approach", "personal information management", "laboratory-based study"], "stemmed_label": ["measur", "taxonomi", "re-find inform", "user evalu", "human factor", "experiment", "email messag", "individu collect", "privaci issu", "naturalist approach", "person inform manag", "laboratory-bas studi"]}
{"doc": "This paper examines a new approach to information distillation over temporally ordered documents , and proposes a novel evaluation scheme for such a framework . It combines the strengths of and extends beyond conventional adaptive filtering , novelty detection and non-redundant passage ranking with respect to long-lasting information needs (‘tasks\" with multiple queries) . Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text , and leverages such information for utility optimization in adaptive settings . For our experiments , we defined hypothetical tasks based on news events in the TDT4 corpus , with multiple queries per task . Answer keys (nuggets) were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses . We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty . Our results show encouraging utility enhancements using the new approach , compared to the baseline systems without incremental learning or the novelty detection components.", "label": ["new evaluation methodology", "adaptive filter", "passage rank", "answer key", "utility-based information distillation", "passage ranking", "unified framework", "nugget-matching rule", "adaptive filtering", "novelty detection", "temporally ordered document", "evaluation methodology", "ndcg metric", "utility-base distillation", "unify framework", "ﬂexible user feedback", "ad-hoc retrieval"], "stemmed_label": ["new evalu methodolog", "adapt filter", "passag rank", "answer key", "utility-bas inform distil", "passag rank", "unifi framework", "nugget-match rule", "adapt filter", "novelti detect", "tempor order document", "evalu methodolog", "ndcg metric", "utility-bas distil", "unifi framework", "ﬂexibl user feedback", "ad-hoc retriev"]}
{"doc": "A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user\"s interest . A system serving millions of users can learn a better user profile for a new user , or a user with little feedback , by borrowing information from other users through the use of a Bayesian hierarchical model . Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive . The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications . This paper proposes a new fast learning technique to learn a large number of individual user profiles . The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.", "label": ["classification", "information filter", "collaborative filtering", "ir", "learning technique", "recommender system", "bayesian hierarchical model", "recommendation system", "rating", "content-based", "em algorithm", "personalization", "linear regression", "modeling", "parameter"], "stemmed_label": ["classif", "inform filter", "collabor filter", "ir", "learn techniqu", "recommend system", "bayesian hierarch model", "recommend system", "rate", "content-bas", "em algorithm", "person", "linear regress", "model", "paramet"]}
{"doc": "Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments . While these judgments are very useful for a one-time evaluation , it is not clear that they can be trusted when re-used to evaluate new systems . In this work , we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments . We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort . Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems , we can reliably evaluate a larger set of ten systems . Even the smallest sets of judgments can be useful for evaluation of new systems.", "label": ["evaluation", "test collection", "relevance judgement", "variance", "distribution of relevance", "rtc", "reusability", "expectation", "information retrieval", "relevance distribution", "mtc", "lowerest-confidence comparison"], "stemmed_label": ["evalu", "test collect", "relev judgement", "varianc", "distribut of relev", "rtc", "reusabl", "expect", "inform retriev", "relev distribut", "mtc", "lowerest-confid comparison"]}
{"doc": "Effective organization of search results is critical for improving the utility of any search engine . Clustering search results is an effective way to organize search results , which allows a user to navigate into relevant documents quickly . However , two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user\"s perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster . In this paper , we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users . We evaluate our proposed method on a commercial search engine log data . Compared with the traditional methods of clustering search results , our method can give better result organization and more meaningful labels.", "label": ["interest aspect", "ranking function", "history collection", "clickthrough", "centroid prototype", "monothetic clustering algorithm", "reciprocal rank", "centroid-based method", "search result organization", "mean average precision", "clustering view", "past query", "cosine similarity", "similarity threshold parameter", "log-based method", "meaningful cluster label", "pseudo-document", "retrieval model", "ambiguity", "search engine log", "star clustering algorithm", "search result snippet", "pairwise similarity graph", "suffix tree clustering algorithm"], "stemmed_label": ["interest aspect", "rank function", "histori collect", "clickthrough", "centroid prototyp", "monothet cluster algorithm", "reciproc rank", "centroid-bas method", "search result organ", "mean averag precis", "cluster view", "past queri", "cosin similar", "similar threshold paramet", "log-bas method", "meaning cluster label", "pseudo-docu", "retriev model", "ambigu", "search engin log", "star cluster algorithm", "search result snippet", "pairwis similar graph", "suffix tree cluster algorithm"]}
{"doc": "Intelligent agents that are intended to work in dynamic environments must be able to gracefully handle unsuccessful tasks and plans . In addition , such agents should be able to make rational decisions about an appropriate course of action , which may include aborting a task or plan , either as a result of the agent\"s own deliberations , or potentially at the request of another agent . In this paper we investigate the incorporation of aborts into a BDI-style architecture . We discuss some conditions under which aborting a task or plan is appropriate , and how to determine the consequences of such a decision . We augment each plan with an optional abort-method , analogous to the failure method found in some agent programming languages . We provide an operational semantics for the execution cycle in the presence of aborts in the abstract agent language CAN , which enables us to specify a BDI-based execution model without limiting our attention to a particular agent system (such as JACK , Jadex , Jason , or SPARK) . A key technical challenge we address is the presence of parallel execution threads and of sub-tasks , which require the agent to ensure that the abort methods for each plan are carried out in an appropriate sequence.", "label": ["reactive and deliberative architecture", "operational semantics", "goal", "intelligent agent", "formal model of agency", "failure", "agent", "abort-method", "dealing", "task", "agency formal model", "cleanup method", "goal construct"], "stemmed_label": ["reactiv and delib architectur", "oper semant", "goal", "intellig agent", "formal model of agenc", "failur", "agent", "abort-method", "deal", "task", "agenc formal model", "cleanup method", "goal construct"]}
{"doc": "This article deals with the problem of collaborative learning in a multi-agent system . Here each agent can update incrementally its beliefs B (the concept representation) so that it is in a way kept consistent with the whole set of information K (the examples) that he has received from the environment or other agents . We extend this notion of consistency (or soundness) to the whole MAS and discuss how to obtain that , at any moment , a same consistent concept representation is present in each agent . The corresponding protocol is applied to supervised concept learning . The resulting method SMILE (standing for Sound Multiagent Incremental LEarning) is described and experimented here . Surprisingly some difficult boolean formulas are better learned , given the same learning set , by a Multi agent system than by a single agent.", "label": ["learning process", "multi-agent learning", "incremental learning", "agent", "collaborative concept learning", "knowledge", "mas-consistency", "multi-agent learn", "update mechanism", "synchronization"], "stemmed_label": ["learn process", "multi-ag learn", "increment learn", "agent", "collabor concept learn", "knowledg", "mas-consist", "multi-ag learn", "updat mechan", "synchron"]}
{"doc": "Reasoning about agents that we observe in the world is challenging . Our available information is often limited to observations of the agent\"s external behavior in the past and present . To understand these actions , we need to deduce the agent\"s internal state , which includes not only rational elements (such as intentions and plans) , but also emotive ones (such as fear) . In addition , we often want to predict the agent\"s future actions , which are constrained not only by these inward characteristics , but also by the dynamics of the agent\"s interaction with its environment . BEE (Behavior Evolution and Extrapolation) uses a faster-than-real-time agentbased model of the environment to characterize agents\" internal state by evolution against observed behavior , and then predict their future behavior , taking into account the dynamics of their interaction with the environment.", "label": ["agent's goal", "pheromone flavor", "plan recognition", "future behavior", "evolution", "swarm intelligence", "bdi", "agent behavior prediction", "internal state", "nonlinear dynamical system", "emotion", "prediction", "behavioral evolution and extrapolation", "external behavior", "agent reasoning", "disposition", "plan inference", "dynamics", "digital pheromone"], "stemmed_label": ["agent' goal", "pheromon flavor", "plan recognit", "futur behavior", "evolut", "swarm intellig", "bdi", "agent behavior predict", "intern state", "nonlinear dynam system", "emot", "predict", "behavior evolut and extrapol", "extern behavior", "agent reason", "disposit", "plan infer", "dynam", "digit pheromon"]}
{"doc": "This paper investigates the problem of estimating the value of probabilistic parameters needed for decision making in environments in which an agent , operating within a multi-agent system , has no a priori information about the structure of the distribution of parameter values . The agent must be able to produce estimations even when it may have made only a small number of direct observations , and thus it must be able to operate with sparse data . The paper describes a mechanism that enables the agent to significantly improve its estimation by augmenting its direct observations with those obtained by other agents with which it is coordinating . To avoid undesirable bias in relatively heterogeneous environments while effectively using relevant data to improve its estimations , the mechanism weighs the contributions of other agents\" observations based on a real-time estimation of the level of similarity between each of these agents and itself . The coordination autonomy module of a coordination-manager system provided an empirical setting for evaluation . Simulation-based evaluations demonstrated that the proposed mechanism outperforms estimations based exclusively on an agent\"s own observations as well as estimations based on an unweighted aggregate of all other agents\" observations.", "label": ["adjustable autonomy", "interruption management", "decision making", "learning mechanism", "probabilistic parameter", "agent", "information sharing", "parameter estimation", "selective-sharing", "fast-paced environment", "multi-agent distributed system"], "stemmed_label": ["adjust autonomi", "interrupt manag", "decis make", "learn mechan", "probabilist paramet", "agent", "inform share", "paramet estim", "selective-shar", "fast-pac environ", "multi-ag distribut system"]}
{"doc": "The dominant existing routing strategies employed in peerto-peer(P2P) based information retrieval(IR) systems are similarity-based approaches . In these approaches , agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions . However , such a heuristic is myopic in that the neighboring agents may not be connected to more relevant agents . In this paper , an online reinforcement-learning based approach is developed to take advantage of the dynamic run-time characteristics of P2P IR systems as represented by information about past search sessions . Specifically , agents maintain estimates on the downstream agents\" abilities to provide relevant documents for incoming queries . These estimates are updated gradually by learning from the feedback information returned from previous search sessions . Based on this information , the agents derive corresponding routing policies . Thereafter , these agents route the queries based on the learned policies and update the estimates based on the new routing policies . Experimental results demonstrate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies.", "label": ["peer-to-peer information retrieval", "distributed search algorithm", "reinforcement learning", "distribute search control", "routing decision", "routing policy", "learning algorithm", "utility", "peer-to-peer information retrieval system", "query", "multi-agent learn", "network"], "stemmed_label": ["peer-to-p inform retriev", "distribut search algorithm", "reinforc learn", "distribut search control", "rout decis", "rout polici", "learn algorithm", "util", "peer-to-p inform retriev system", "queri", "multi-ag learn", "network"]}
{"doc": "Finding the right agents in a large and dynamic network to provide the needed resources in a timely fashion , is a long standing problem . This paper presents a method for information searching and sharing that combines routing indices with tokenbased methods . The proposed method enables agents to search effectively by acquiring their neighbors\" interests , advertising their information provision abilities and maintaining indices for routing queries , in an integrated way . Specifically , the paper demonstrates through performance experiments how static and dynamic networks of agents can be ‘tuned\" to answer queries effectively as they gather evidence for the interests and information provision abilities of others , without altering the topology or imposing an overlay structure to the network of acquaintances.", "label": ["decentralized partially-observable markov decision process", "artificial social system", "gradient search scheme", "performance", "scalability", "robustness", "cooperative agent", "myopic algorithm", "knn approach", "peer to peer search network", "dependability", "social network", "decentralized control", "dynamic and large scale network", "information searching and sharing", "peer-to-peer system"], "stemmed_label": ["decentr partially-observ markov decis process", "artifici social system", "gradient search scheme", "perform", "scalabl", "robust", "cooper agent", "myopic algorithm", "knn approach", "peer to peer search network", "depend", "social network", "decentr control", "dynam and larg scale network", "inform search and share", "peer-to-p system"]}
{"doc": "In this paper we present an advanced bidding agent that participates in first-price sealed bid auctions to allocate advertising space on BluScreen - an experimental public advertisement system that detects users through the presence of their Bluetooth enabled devices . Our bidding agent is able to build probabilistic models of both the behaviour of users who view the adverts , and the auctions that it participates within . It then uses these models to maximise the exposure that its adverts receive . We evaluate the effectiveness of this bidding agent through simulation against a range of alternative selection mechanisms including a simple bidding strategy , random allocation , and a centralised optimal allocation with perfect foresight . Our bidding agent significantly outperforms both the simple bidding strategy and the random allocation , and in a mixed population of agents it is able to expose its adverts to 25% more users than the simple bidding strategy . Moreover , its performance is within 7.5% of that of the centralised optimal allocation despite the highly uncertain environment in which it must operate.", "label": ["distributed artificial intelligence", "decision theoretic approach", "bluscreen", "stochastic optimisation algorithm", "decentralised multi-agent auction mechanism", "experimental public advertisement system", "independent poisson process", "probabilistic model", "bluetooth", "auction", "public display", "bid agent", "centralised optimal allocation", "advanced bidding agent"], "stemmed_label": ["distribut artifici intellig", "decis theoret approach", "bluscreen", "stochast optimis algorithm", "decentralis multi-ag auction mechan", "experiment public advertis system", "independ poisson process", "probabilist model", "bluetooth", "auction", "public display", "bid agent", "centralis optim alloc", "advanc bid agent"]}
{"doc": "The paper deals with on-board planning for a satellite swarm via communication and negotiation . We aim at defining individual behaviours that result in a global behaviour that meets the mission requirements . We will present the formalization of the problem , a communication protocol , a solving method based on reactive decision rules , and first results . Categories and Subject Descriptors", "label": ["communication and negotiation", "prospecting ant", "cooperation and teamwork", "on-board planning", "task and resource allocation", "multiagent system", "teamagent", "cooperative distribute problem solve", "dip", "coordination", "information system application", "satellite swarm", "reactive decision rule", "objectagent architecture"], "stemmed_label": ["commun and negoti", "prospect ant", "cooper and teamwork", "on-board plan", "task and resourc alloc", "multiag system", "teamag", "cooper distribut problem solv", "dip", "coordin", "inform system applic", "satellit swarm", "reactiv decis rule", "objectag architectur"]}
{"doc": "We derive optimal bidding strategies for a global bidding agent that participates in multiple , simultaneous second-price auctions with perfect substitutes . We first consider a model where all other bidders are local and participate in a single auction . For this case , we prove that , assuming free disposal , the global bidder should always place non-zero bids in all available auctions , irrespective of the local bidders\" valuation distribution . Furthermore , for non-decreasing valuation distributions , we prove that the problem of finding the optimal bids reduces to two dimensions . These results hold both in the case where the number of local bidders is known and when this number is determined by a Poisson distribution . This analysis extends to online markets where , typically , auctions occur both concurrently and sequentially . In addition , by combining analytical and simulation results , we demonstrate that similar results hold in the case of several global bidders , provided that the market consists of both global and local bidders . Finally , we address the efficiency of the overall market , and show that information about the number of local bidders is an important determinant for the way in which a global bidder affects efficiency.", "label": ["global bidding agent", "online market", "social and behavioral science", "perfect substitute", "simultaneous auction", "simultaneous second-price auction", "utilitymaximising strategy", "optimal bidding strategy", "vickrey auction", "non-decreasing valuation distribution", "multiagent system", "market efficiency"], "stemmed_label": ["global bid agent", "onlin market", "social and behavior scienc", "perfect substitut", "simultan auction", "simultan second-pric auction", "utilitymaximis strategi", "optim bid strategi", "vickrey auction", "non-decreas valuat distribut", "multiag system", "market effici"]}
{"doc": "Preference aggregation is used in a variety of multiagent applications , and as a result , voting theory has become an important topic in multiagent system research . However , power indices (which reflect how much real power a voter has in a weighted voting system) have received relatively little attention , although they have long been studied in political science and economics . The Banzhaf power index is one of the most popular; it is also well-defined for any simple coalitional game . In this paper , we examine the computational complexity of calculating the Banzhaf power index within a particular multiagent domain , a network flow game . Agents control the edges of a graph; a coalition wins if it can send a flow of a given size from a source vertex to a target vertex . The relative power of each edge/agent reflects its significance in enabling such a flow , and in real-world networks could be used , for example , to allocate resources for maintaining parts of the network . We show that calculating the Banzhaf power index of each agent in this network flow domain is #P-complete . We also show that for some restricted network flow domains there exists a polynomial algorithm to calculate agents\" Banzhaf power indices.", "label": ["social choice theory", "network flow game", "banzhaf power index", "preference aggregation", "automated agent voting", "power index", "analysis of algorithm and problem complexity", "probabilistic model", "connectivity game", "multiagent application", "algorithm and problem complexity analysis", "computational complexity", "vote", "voting theory"], "stemmed_label": ["social choic theori", "network flow game", "banzhaf power index", "prefer aggreg", "autom agent vote", "power index", "analysi of algorithm and problem complex", "probabilist model", "connect game", "multiag applic", "algorithm and problem complex analysi", "comput complex", "vote", "vote theori"]}
{"doc": "We investigate a framework where agents search for satisfying products by using referrals from other agents . Our model of a mechanism for transmitting word-of-mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a module governing the structure and function of the underlying network of agents . Local behaviour incorporates a satisficing model of choice , a set of rules governing the interactions between agents , including learning about the trustworthiness of other agents over time , and external constraints on behaviour that may be imposed by market barriers or switching costs . Local behaviour takes place on a network substrate across which agents exchange positive and negative information about products . We use various degree distributions dictating the extent of connectivity , and incorporate both small-world effects and the notion of preferential attachment in our network models . We compare the effectiveness of referral systems over various network structures for easy and hard choice tasks , and evaluate how this effectiveness changes with the imposition of market barriers.", "label": ["psychological affinity", "switching cost", "word-of-mouth communication", "cognitive model", "artificial social system", "market barrier", "referral system", "switching behaviour", "social psychology", "social network", "purchasing behaviour", "agent-based model", "consumer choice", "marketing system", "defection behaviour"], "stemmed_label": ["psycholog affin", "switch cost", "word-of-mouth commun", "cognit model", "artifici social system", "market barrier", "referr system", "switch behaviour", "social psycholog", "social network", "purchas behaviour", "agent-bas model", "consum choic", "market system", "defect behaviour"]}
{"doc": "Human team members often develop shared expectations to predict each other\"s needs and coordinate their behaviors . In this paper the concept Shared Belief Map is proposed as a basis for developing realistic shared expectations among a team of Human-Agent-Pairs (HAPs) . The establishment of shared belief maps relies on inter-agent information sharing , the effectiveness of which highly depends on agents\" processing loads and the instantaneous cognitive loads of their human partners . We investigate HMM-based cognitive load models to facilitate team members to share the right information with the right party at the right time . The shared belief map concept and the cognitive/processing load models have been implemented in a cognitive agent architectureSMMall . A series of experiments were conducted to evaluate the concept , the models , and their impacts on the evolving of shared mental models of HAP teams.", "label": ["teamwork", "cognitive model", "collaboration", "problem-solving", "task performance", "multi-party communication", "human-agent team performance", "reasoning", "expectation", "human performance", "human-center teamwork", "info-sharing", "resource allocation", "teamwork schema", "heuristic", "shared belief map", "multiagent teamwork", "share belief map", "cognitive load theory"], "stemmed_label": ["teamwork", "cognit model", "collabor", "problem-solv", "task perform", "multi-parti commun", "human-ag team perform", "reason", "expect", "human perform", "human-cent teamwork", "info-shar", "resourc alloc", "teamwork schema", "heurist", "share belief map", "multiag teamwork", "share belief map", "cognit load theori"]}
{"doc": "This paper presents a two-sided economic search model in which agents are searching for beneficial pairwise partnerships . In each search stage , each of the agents is randomly matched with several other agents in parallel , and makes a decision whether to accept a potential partnership with one of them . The distinguishing feature of the proposed model is that the agents are not restricted to maintaining a synchronized (instantaneous) decision protocol and can sequentially accept and reject partnerships within the same search stage . We analyze the dynamics which drive the agents\" strategies towards a stable equilibrium in the new model and show that the proposed search strategy weakly dominates the one currently in use for the two-sided parallel economic search model . By identifying several unique characteristics of the equilibrium we manage to efficiently bound the strategy space that needs to be explored by the agents and propose an efficient means for extracting the distributed equilibrium strategies in common environments.", "label": ["equilibrium strategy", "sequential decision making", "two-side search", "bounding methodology", "parallel interaction", "multi-equilibrium scenario", "coalition formation", "partnership", "information processing", "costly environment", "pairwise partnership", "two-sided search", "utility", "search cost", "decision", "search performance", "peer-to-peer application", "partnership formation", "match", "instantaneous decision making"], "stemmed_label": ["equilibrium strategi", "sequenti decis make", "two-sid search", "bound methodolog", "parallel interact", "multi-equilibrium scenario", "coalit format", "partnership", "inform process", "costli environ", "pairwis partnership", "two-sid search", "util", "search cost", "decis", "search perform", "peer-to-p applic", "partnership format", "match", "instantan decis make"]}
{"doc": "We consider the problem of managing schedules in an uncertain , distributed environment . We assume a team of collaborative agents , each responsible for executing a portion of a globally pre-established schedule , but none possessing a global view of either the problem or solution . The goal is to maximize the joint quality obtained from the activities executed by all agents , given that , during execution , unexpected events will force changes to some prescribed activities and reduce the utility of executing others . We describe an agent architecture for solving this problem that couples two basic mechanisms: (1) a flexible times representation of the agent\"s schedule (using a Simple Temporal Network) and (2) an incremental rescheduling procedure . The former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions , and the latter acts to revise the agent\"s schedule when execution is forced outside of this set of solutions or when execution events reduce the expected value of this feasible solution set . Basic coordination with other agents is achieved simply by communicating schedule changes to those agents with inter-dependent activities . Then , as time permits , the core local problem solving infra-structure is used to drive an inter-agent option generation and query process , aimed at identifying opportunities for solution improvement through joint change . Using a simulator to model the environment , we compare the performance of our multi-agent system with that of an expected optimal (but non-scalable) centralized MDP solver.", "label": ["shortest path algorithm", "agent architecture", "scheduler-execution", "performance", "slack", "flexible time", "management", "conflict-driven approach", "multi-agent schedule", "inter-dependent activity", "managing schedule", "optimistic synchronization", "inter-agent coordination", "activity allocator", "centralized planning", "geographical separation", "distributed environment", "schedule"], "stemmed_label": ["shortest path algorithm", "agent architectur", "scheduler-execut", "perform", "slack", "flexibl time", "manag", "conflict-driven approach", "multi-ag schedul", "inter-depend activ", "manag schedul", "optimist synchron", "inter-ag coordin", "activ alloc", "central plan", "geograph separ", "distribut environ", "schedul"]}
{"doc": "This paper proposes a new variant of the task allocation problem , where the agents are connected in a social network and tasks arrive at the agents distributed over the network . We show that the complexity of this problem remains NPhard . Moreover , it is not approximable within some factor . We develop an algorithm based on the contract-net protocol . Our algorithm is completely distributed , and it assumes that agents have only local knowledge about tasks and resources . We conduct a set of experiments to evaluate the performance and scalability of the proposed algorithm in terms of solution quality and computation time . Three different types of networks , namely small-world , random and scale-free networks , are used to represent various social relationships among agents in realistic applications . The results demonstrate that our algorithm works well and that it scales well to large-scale applications.", "label": ["algorithm", "agent", "strategic agent", "task allocation", "social network", "utility", "communication message", "interaction", "resource", "computational complexity", "social relationship", "multiagent system", "behavior", "allocation"], "stemmed_label": ["algorithm", "agent", "strateg agent", "task alloc", "social network", "util", "commun messag", "interact", "resourc", "comput complex", "social relationship", "multiag system", "behavior", "alloc"]}
{"doc": "Agents that must reach agreements with other agents need to reason about how their preferences , judgments , and beliefs might be aggregated with those of others by the social choice mechanisms that govern their interactions . The recently emerging field of judgment aggregation studies aggregation from a logical perspective , and considers how multiple sets of logical formulae can be aggregated to a single consistent set . As a special case , judgment aggregation can be seen to subsume classical preference aggregation . We present a modal logic that is intended to support reasoning about judgment aggregation scenarios (and hence , as a special case , about preference aggregation): the logical language is interpreted directly in judgment aggregation rules . We present a sound and complete axiomatisation of such rules . We show that the logic can express aggregation rules such as majority voting; rule properties such as independence; and results such as the discursive paradox , Arrow\"s theorem and Condorcet\"s paradox - which are derivable as formal theorems of the logic . The logic is parameterised in such a way that it can be used as a general framework for comparing the logical properties of different types of aggregation - including classical preference aggregation.", "label": ["syntax and semantics of jal", "arrow logic", "discursive paradox", "jal syntax and semantics", "arrow's theorem", "modal logic", "judgment aggregation rule", "judgment aggregation", "preference aggregation", "social welfare function", "complete axiomatisation", "knowledge representation formalism", "expressivity", "jal", "unanimity", "non-dictatorship"], "stemmed_label": ["syntax and semant of jal", "arrow logic", "discurs paradox", "jal syntax and semant", "arrow' theorem", "modal logic", "judgment aggreg rule", "judgment aggreg", "prefer aggreg", "social welfar function", "complet axiomatis", "knowledg represent formal", "express", "jal", "unanim", "non-dictatorship"]}
{"doc": "Multiagent environments are often not cooperative nor collaborative; in many cases , agents have conflicting interests , leading to adversarial interactions . This paper presents a formal Adversarial Environment model for bounded rational agents operating in a zero-sum environment . In such environments , attempts to use classical utility-based search methods can raise a variety of difficulties (e.g. , implicitly modeling the opponent as an omniscient utility maximizer , rather than leveraging a more nuanced , explicit opponent model) . We define an Adversarial Environment by describing the mental states of an agent in such an environment . We then present behavioral axioms that are intended to serve as design principles for building such adversarial agents . We explore the application of our approach by analyzing log files of completed Connect-Four games , and present an empirical analysis of the axioms\" appropriateness.", "label": ["bilateral and multilateral instantiation", "multiagent environment", "modal logic", "empirical study", "eval value", "adversarial interaction", "beneficial action", "agent", "connect-four game", "behavioral axiom", "axiomatized model", "interaction", "treatment group", "adversarial environment", "zero-sum encounter", "multiagent system", "evaluation function"], "stemmed_label": ["bilater and multilater instanti", "multiag environ", "modal logic", "empir studi", "eval valu", "adversari interact", "benefici action", "agent", "connect-four game", "behavior axiom", "axiomat model", "interact", "treatment group", "adversari environ", "zero-sum encount", "multiag system", "evalu function"]}
{"doc": "Up to now , the way institutions and organizations have been used in the development of open systems has not often gone further than a useful heuristics . In order to develop systems actually implementing institutions and organizations , formal methods should take the place of heuristic ones . The paper presents a formal semantics for the notion of institution and its components (abstract and concrete norms , empowerment of agents , roles) and defines a formal relation between institutions and organizational structures . As a result , it is shown how institutional norms can be refined to constructsorganizational structures-which are closer to an implemented system . It is also shown how such a refinement process can be fully formalized and it is therefore amenable to rigorous verification.", "label": ["dynamic logic", "terminological axiom", "institutional norm", "formal method", "institution", "formalism for representing organizational structure", "property", "role", "organizational structure", "infrastructure", "norm", "logic", "description logic", "abstract constraint", "entity"], "stemmed_label": ["dynam logic", "terminolog axiom", "institut norm", "formal method", "institut", "formal for repres organiz structur", "properti", "role", "organiz structur", "infrastructur", "norm", "logic", "descript logic", "abstract constraint", "entiti"]}
{"doc": "Norm-governed virtual organizations define , govern and facilitate coordinated resource sharing and problem solving in societies of agents . With an explicit account of norms , openness in virtual organizations can be achieved: new components , designed by various parties , can be seamlessly accommodated . We focus on virtual organizations realised as multi-agent systems , in which human and software agents interact to achieve individual and global goals . However , any realistic account of norms should address their dynamic nature: norms will change as agents interact with each other and their environment . Due to the changing nature of norms or due to norms stemming from different virtual organizations , there will be situations when an action is simultaneously permitted and prohibited , that is , a conflict arises . Likewise , there will be situations when an action is both obliged and prohibited , that is , an inconsistency arises . We introduce an approach , based on first-order unification , to detect and resolve such conflicts and inconsistencies . In our proposed solution , we annotate a norm with the set of values their variables should not have in order to avoid a conflict or an inconsistency with another norm . Our approach neatly accommodates the domain-dependent interrelations among actions and the indirect conflicts/inconsistencies these may cause . More generally , we can capture a useful notion of inter-agent (and inter-role) delegation of actions and norms associated to them , and use it to address conflicts/inconsistencies caused by action delegation . We illustrate our approach with an e-Science example in which agents support Grid services.", "label": ["conflicting prohibition", "external agent", "governor agent", "norm conflict", "agent", "norm-regulated vo", "norm inconsistency", "virtual organization", "artificial social systems", "multi-agent system"], "stemmed_label": ["conflict prohibit", "extern agent", "governor agent", "norm conflict", "agent", "norm-regul vo", "norm inconsist", "virtual organ", "artifici social system", "multi-ag system"]}
{"doc": "Norms are widely recognised as a means of coordinating multi-agent systems . The distributed management of norms is a challenging issue and we observe a lack of truly distributed computational realisations of normative models . In order to regulate the behaviour of autonomous agents that take part in multiple , related activities , we propose a normative model , the Normative Structure (NS) , an artifact that is based on the propagation of normative positions (obligations , prohibitions , permissions) , as consequences of agents\" actions . Within a NS , conflicts may arise due to the dynamic nature of the MAS and the concurrency of agents\" actions . However , ensuring conflict-freedom of a NS at design time is computationally intractable . We show this by formalising the notion of conflict , providing a mapping of NSs into Coloured Petri Nets and borrowing well-known theoretical results from that field . Since online conflict resolution is required , we present a tractable algorithm to be employed distributedly . We then demonstrate that this algorithm is paramount for the distributed enactment of a NS.", "label": ["algorithm", "protocol", "normative scene", "organisation", "prohibition", "norm conflict", "normative position", "normative structure", "conflict", "scenario", "electronic institution", "bi-partite graph", "permission overlap", "activity", "coordination", "normative transition rule", "token", "regulate multi-agent system"], "stemmed_label": ["algorithm", "protocol", "norm scene", "organis", "prohibit", "norm conflict", "norm posit", "norm structur", "conflict", "scenario", "electron institut", "bi-partit graph", "permiss overlap", "activ", "coordin", "norm transit rule", "token", "regul multi-ag system"]}
{"doc": "A negotiation chain is formed when multiple related negotiations are spread over multiple agents . In order to appropriately order and structure the negotiations occurring in the chain so as to optimize the expected utility , we present an extension to a singleagent concurrent negotiation framework . This work is aimed at semi-cooperative multi-agent systems , where each agent has its own goals and works to maximize its local utility; however , the performance of each individual agent is tightly related to other agent\"s cooperation and the system\"s overall performance . We introduce a pre-negotiation phase that allows agents to transfer meta-level information . Using this information , the agent can build a more accurate model of the negotiation in terms of modeling the relationship of flexibility and success probability . This more accurate model helps the agent in choosing a better negotiation solution in the global negotiation chain context . The agent can also use this information to allocate appropriate time for each negotiation , hence to find a good ordering of all related negotiations . The experimental data shows that these mechanisms improve the agents\" and the system\"s overall performance significantly.", "label": ["distributed setting", "complex supply-chain scenario", "flexibility", "pre-negotiation", "multi-link negotiation", "sub-task relocation", "agent", "multiple agent", "multi-linked negotiation", "virtual organization", "reputation mechanism", "multiple concurrent task", "negotiation framework", "semi-cooperative multi-agent system", "negotiation chain"], "stemmed_label": ["distribut set", "complex supply-chain scenario", "flexibl", "pre-negoti", "multi-link negoti", "sub-task reloc", "agent", "multipl agent", "multi-link negoti", "virtual organ", "reput mechan", "multipl concurr task", "negoti framework", "semi-coop multi-ag system", "negoti chain"]}
{"doc": "Distributed applications require distributed techniques for efficient resource allocation . These techniques need to take into account the heterogeneity and potential unreliability of resources and resource consumers in a distributed environments . In this paper we propose a distributed algorithm that solves the resource allocation problem in distributed multiagent systems . Our solution is based on the self-organisation of agents , which does not require any facilitator or management layer . The resource allocation in the system is a purely emergent effect . We present results of the proposed resource allocation mechanism in the simulated static and dynamic multi-server environment.", "label": ["dynamically allocated task", "distribute control", "self-organisation", "network of server", "server network", "adaptive process", "agent", "competition", "server utilisation", "predictor", "multi-agent system", "resource allocation", "distributed algorithm"], "stemmed_label": ["dynam alloc task", "distribut control", "self-organis", "network of server", "server network", "adapt process", "agent", "competit", "server utilis", "predictor", "multi-ag system", "resourc alloc", "distribut algorithm"]}
{"doc": "This paper proposes dynamic semantics for agent communication languages (ACLs) as a method for tackling some of the fundamental problems associated with agent communication in open multiagent systems . Based on the idea of providing alternative semantic variants for speech acts and transition rules between them that are contingent on previous agent behaviour , our framework provides an improved notion of grounding semantics in ongoing interaction , a simple mechanism for distinguishing between compliant and expected behaviour , and a way to specify sanction and reward mechanisms as part of the ACL itself . We extend a common framework for commitment-based ACL semantics to obtain these properties , discuss desiderata for the design of concrete dynamic semantics together with examples , and analyse their properties.", "label": ["state transition system", "commitment-based semantics", "agent communication language", "recovery mechanism", "mutuality of expectation", "expectation mutuality", "non-redundancy", "social reason", "dynamic semantics", "social reasoning", "reputation-based adaptation"], "stemmed_label": ["state transit system", "commitment-bas semant", "agent commun languag", "recoveri mechan", "mutual of expect", "expect mutual", "non-redund", "social reason", "dynam semant", "social reason", "reputation-bas adapt"]}
{"doc": "Making commitments , e.g. , through promises and threats , enables a player to exploit the strengths of his own strategic position as well as the weaknesses of that of his opponents . Which commitments a player can make with credibility depends on the circumstances . In some , a player can only commit to the performance of an action , in others , he can commit himself conditionally on the actions of the other players . Some situations even allow for commitments on commitments or for commitments to randomized actions . We explore the formal properties of these types of (conditional) commitment and their interrelationships . So as to preclude inconsistencies among conditional commitments , we assume an order in which the players make their commitments . Central to our analyses is the notion of an extortion , which we define , for a given order of the players , as a profile that contains , for each player , an optimal commitment given the commitments of the players that committed earlier . On this basis , we investigate for different commitment types whether it is advantageous to commit earlier rather than later , and how the outcomes obtained through extortions relate to backward induction and Pareto efficiency.", "label": ["game theory", "strategic position", "credibility", "distributed computing", "decision making", "freedom of action", "commitment", "electronic market", "induction hypothesis", "stackleberg setting", "optimal conditional commitment", "sequential commitment type", "extortion", "pareto efficient conditional extortion", "action freedom", "multiagent system", "pareto efficiency"], "stemmed_label": ["game theori", "strateg posit", "credibl", "distribut comput", "decis make", "freedom of action", "commit", "electron market", "induct hypothesi", "stackleberg set", "optim condit commit", "sequenti commit type", "extort", "pareto effici condit extort", "action freedom", "multiag system", "pareto effici"]}
{"doc": "Interactions between agents in an open system such as the Internet require a significant degree of flexibility . A crucial aspect of the development of such methods is the notion of commitments , which provides a mechanism for coordinating interactive behaviors among agents . In this paper , we investigate an approach to model commitments with tight integration with protocol actions . This means that there is no need to have an explicit mapping from protocols actions to operations on commitments and an external mechanism to process and enforce commitments . We show how agents can reason about commitments and protocol actions to achieve the end results of protocols using a reasoning system based on temporal linear logic , which incorporates both temporal and resource-sensitive reasoning . We also discuss the application of this framework to scenarios such as online commerce.", "label": ["request message", "classical conjunction", "multiplicative conjunction", "linear logic", "multi-agent environment", "temporal constraint", "interaction protocol", "logic and formal model of agency and multi-agent system", "agent communication language and protocol", "predictability level", "pre-commitment", "conditional commitment", "interactive behavior", "causal relationship", "emergent protocol", "level of predictability", "linear implication"], "stemmed_label": ["request messag", "classic conjunct", "multipl conjunct", "linear logic", "multi-ag environ", "tempor constraint", "interact protocol", "logic and formal model of agenc and multi-ag system", "agent commun languag and protocol", "predict level", "pre-commit", "condit commit", "interact behavior", "causal relationship", "emerg protocol", "level of predict", "linear implic"]}
{"doc": "When designing a mechanism there are several desirable properties to maintain such as incentive compatibility (IC) , individual rationality (IR) , and budget balance (BB) . It is well known 15 that it is impossible for a mechanism to maximize social welfare whilst also being IR , IC , and BB . There have been several attempts to circumvent 15 by trading welfare for BB , e.g. , in domains such as double-sided auctions 13 , distributed markets 3 and supply chain problems 2 , 4 . In this paper we provide a procedure called a Generalized Trade Reduction (GTR) for single-value players , which given an IR and IC mechanism , outputs a mechanism which is IR , IC and BB with a loss of welfare . We bound the welfare achieved by our procedure for a wide range of domains . In particular , our results improve on existing solutions for problems such as double sided markets with homogenous goods , distributed markets and several kinds of supply chains . Furthermore , our solution provides budget balanced mechanisms for several open problems such as combinatorial double-sided auctions and distributed markets with strategic transportation edges.", "label": ["budget-balanced mechanism", "inequality in welfare", "external competition", "homogeneous good", "multi-minded player", "efficiency", "generalized trade reduction", "player power", "internal competition", "optimality", "budget balance", "gtr", "power of player", "spatially distributed market", "trade reduction"], "stemmed_label": ["budget-balanc mechan", "inequ in welfar", "extern competit", "homogen good", "multi-mind player", "effici", "gener trade reduct", "player power", "intern competit", "optim", "budget balanc", "gtr", "power of player", "spatial distribut market", "trade reduct"]}
{"doc": "Online reviews have become increasingly popular as a way to judge the quality of various products and services . Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult . In this paper , we investigate underlying factors that influence user behavior when reporting feedback . We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review , and patterns in the time sequence of reports . We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature . Second , we show that a user\"s rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews . Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias . Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.", "label": ["utility of the product", "the product utility", "semantic orientation of product evaluation", "brag-and-moan model", "great probability bi-modal", "clear incentive absence", "rating", "u-shaped distribution", "correlation", "reputation mechanism", "large span of time", "feature-by-feature estimator of quality", "absence of clear incentive", "online review"], "stemmed_label": ["util of the product", "the product util", "semant orient of product evalu", "brag-and-moan model", "great probabl bi-mod", "clear incent absenc", "rate", "u-shap distribut", "correl", "reput mechan", "larg span of time", "feature-by-featur estim of qualiti", "absenc of clear incent", "onlin review"]}
{"doc": "In a wide range of markets , individual buyers and sellers often trade through intermediaries , who determine prices via strategic considerations . Typically , not all buyers and sellers have access to the same intermediaries , and they trade at correspondingly different prices that reflect their relative amounts of power in the market . We model this phenomenon using a game in which buyers , sellers , and traders engage in trade on a graph that represents the access each buyer and seller has to the traders . In this model , traders set prices strategically , and then buyers and sellers react to the prices they are offered . We show that the resulting game always has a subgame perfect Nash equilibrium , and that all equilibria lead to an efficient (i.e . socially optimal) allocation of goods . We extend these results to a more general type of matching market , such as one finds in the matching of job applicants and employers . Finally , we consider how the profits obtained by the traders depend on the underlying graph - roughly , a trader can command a positive profit if and only if it has an essential connection in the network structure , thus providing a graph-theoretic basis for quantifying the amount of competition among traders . Our work differs from recent studies of how price is affected by network structure through our modeling of price-setting as a strategic activity carried out by a subset of agents in the system , rather than studying prices set via competitive equilibrium or by a truthful mechanism.", "label": ["initial endowment of money", "bid price", "economics and finance", "maximum and minimum amount", "market", "perfect competition", "algorithmic game theory", "interaction of buyer and seller", "trader strategic behavior", "benefit", "strategic behavior of trader", "monopoly", "trade network", "buyer and seller interaction", "trading network", "money initial endowment", "complementary slackness"], "stemmed_label": ["initi endow of money", "bid price", "econom and financ", "maximum and minimum amount", "market", "perfect competit", "algorithm game theori", "interact of buyer and seller", "trader strateg behavior", "benefit", "strateg behavior of trader", "monopoli", "trade network", "buyer and seller interact", "trade network", "money initi endow", "complementari slack"]}
{"doc": "The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices . While this problem is in general NPhard , it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs) . Formally , an item graph is a graph whose nodes are in one-to-one correspondence with items , and edges are such that for any bid , the items occurring in it induce a connected subgraph . Note that many item graphs might be associated with a given combinatorial auction , depending on the edges selected for guaranteeing the connectedness . In fact , the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so , computing one) was left as a crucial open problem . In this paper , we solve this problem by proving that the existence of a structured item graph is computationally intractable , even for treewidth 3 . Motivated by this bad news , we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions . We show that the notion of hypertree decomposition , a recently introduced measure of hypergraph cyclicity , turns out to be most useful here . Indeed , we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width . Even more surprisingly , we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.", "label": ["structured item graph complexity", "simplification of the primal graph", "hypergraph", "structured item graph", "hypertree-based decomposition method", "hypergraph hg", "the primal graph simplification", "polynomial time", "combinatorial auction", "fixed treewidth", "accepted bid price", "well-known mechanism for resource and task allocation", "complexity of structured item graph", "hypertree decomposition"], "stemmed_label": ["structur item graph complex", "simplif of the primal graph", "hypergraph", "structur item graph", "hypertree-bas decomposit method", "hypergraph hg", "the primal graph simplif", "polynomi time", "combinatori auction", "fix treewidth", "accept bid price", "well-known mechan for resourc and task alloc", "complex of structur item graph", "hypertre decomposit"]}
{"doc": "This paper addresses the problem of fair equilibrium selection in graphical games . Our approach is based on the data structure called the best response policy , which was proposed by Kearns et al . 13 as a way to represent all Nash equilibria of a graphical game . In 9 , it was shown that the best response policy has polynomial size as long as the underlying graph is a path . In this paper , we show that if the underlying graph is a bounded-degree tree and the best response policy has polynomial size then there is an efficient algorithm which constructs a Nash equilibrium that guarantees certain payoffs to all participants . Another attractive solution concept is a Nash equilibrium that maximizes the social welfare . We show that , while exactly computing the latter is infeasible (we prove that solving this problem may involve algebraic numbers of an arbitrarily high degree) , there exists an FPTAS for finding such an equilibrium as long as the best response policy has polynomial size . These two algorithms can be combined to produce Nash equilibria that satisfy various fairness criteria.", "label": ["strategy profile", "nash equilibrium", "approximation", "exponential-time algorithm", "approximation scheme", "overall payoff", "several drawback", "social welfare", "distributing profit", "graphical game", "degree-bounded graph", "various sociallydesirable property", "integer-payoff graphical game g"], "stemmed_label": ["strategi profil", "nash equilibrium", "approxim", "exponential-tim algorithm", "approxim scheme", "overal payoff", "sever drawback", "social welfar", "distribut profit", "graphic game", "degree-bound graph", "variou sociallydesir properti", "integer-payoff graphic game g"]}
{"doc": "Multiattribute auction mechanisms generally either remain agnostic about traders\" preferences , or presume highly restrictive forms , such as full additivity . Real preferences often exhibit dependencies among attributes , yet may possess some structure that can be usefully exploited to streamline communication and simplify operation of a multiattribute auction . We develop such a structure using the theory of measurable value functions , a cardinal utility representation based on an underlying order over preference differences . A set of local conditional independence relations over such differences supports a generalized additive preference representation , which decomposes utility across overlapping clusters of related attributes . We introduce an iterative auction mechanism that maintains prices on local clusters of attributes rather than the full space of joint configurations . When traders\" preferences are consistent with the auction\"s generalized additive structure , the mechanism produces approximately optimal allocations , at approximate VCG prices.", "label": ["mvf", "multiattribute auction", "gai based auction", "auction", "measurable value function theory", "iterative auction mechanism", "preference handling", "theory of measurable value function", "gaus"], "stemmed_label": ["mvf", "multiattribut auction", "gai base auction", "auction", "measur valu function theori", "iter auction mechan", "prefer handl", "theori of measur valu function", "gau"]}
{"doc": "We consider the problem of makespan minimization on m unrelated machines in the context of algorithmic mechanism design , where the machines are the strategic players . This is a multidimensional scheduling domain , and the only known positive results for makespan minimization in such a domain are O(m)-approximation truthful mechanisms 22 , 20 . We study a well-motivated special case of this problem , where the processing time of a job on each machine may either be low or high , and the low and high values are public and job-dependent . This preserves the multidimensionality of the domain , and generalizes the restricted-machines (i.e. , pj , ∞ ) setting in scheduling . We give a general technique to convert any c-approximation algorithm to a 3capproximation truthful-in-expectation mechanism . This is one of the few known results that shows how to export approximation algorithms for a multidimensional problem into truthful mechanisms in a black-box fashion . When the low and high values are the same for all jobs , we devise a deterministic 2-approximation truthful mechanism . These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain . Our constructions are novel in two respects . First , we do not utilize or rely on explicit price definitions to prove truthfulness; instead we design algorithms that satisfy cycle monotonicity . Cycle monotonicity 23 is a necessary and sufficient condition for truthfulness , is a generalization of value monotonicity for multidimensional domains . However , whereas value monotonicity has been used extensively and successfully to design truthful mechanisms in singledimensional domains , ours is the first work that leverages cycle monotonicity in the multidimensional setting . Second , our randomized mechanisms are obtained by first constructing a fractional truthful mechanism for a fractional relaxation of the problem , and then converting it into a truthfulin-expectation mechanism . This builds upon a technique of 16 , and shows the usefulness of fractional mechanisms in truthful mechanism design.", "label": ["algorithm", "fractional mechanism usefulness", "makespan minimization", "multi-dimensional scheduling", "usefulness of fractional mechanism", "randomized mechanism", "fractional domain", "truthful mechanism design", "scheduling", "cycle monotonicity", "schedule", "approximation algorithm", "mechanism design"], "stemmed_label": ["algorithm", "fraction mechan use", "makespan minim", "multi-dimension schedul", "use of fraction mechan", "random mechan", "fraction domain", "truth mechan design", "schedul", "cycl monoton", "schedul", "approxim algorithm", "mechan design"]}
{"doc": "A mediator is a reliable entity , which can play on behalf of agents in a given game . A mediator however can not enforce the use of its services , and each agent is free to participate in the game directly . In this paper we introduce a study of mediators for games with incomplete information , and apply it to the context of position auctions , a central topic in electronic commerce . VCG position auctions , which are currently not used in practice , possess some nice theoretical properties , such as the optimization of social surplus and having dominant strategies . These properties may not be satisfied by current position auctions and their variants . We therefore concentrate on the search for mediators that will allow to transform current position auctions into VCG position auctions . We require that accepting the mediator services , and reporting honestly to the mediator , will form an ex post equilibrium , which satisfies the following rationality condition: an agent\"s payoff can not be negative regardless of the actions taken by the agents who did not choose the mediator\"s services , or by the agents who report false types to the mediator . We prove the existence of such desired mediators for the next-price (Google-like) position auctions , as well as for a richer class of position auctions , including all k-price position auctions , k 1 . For k=1 , the self-price position auction , we show that the existence of such mediator depends on the tie breaking rule used in the auction.", "label": ["mediator", "richer class of position auction", "electronic commerce", "vcg outcome function", "t-strategy", "self-price position auction", "next-price position auction", "agent", "auction", "position auction", "equilibrium", "ex post equilibrium", "multi-agent system"], "stemmed_label": ["mediat", "richer class of posit auction", "electron commerc", "vcg outcom function", "t-strategi", "self-pric posit auction", "next-pric posit auction", "agent", "auction", "posit auction", "equilibrium", "ex post equilibrium", "multi-ag system"]}
{"doc": "For allocation problems with one or more items , the wellknown Vickrey-Clarke-Groves (VCG) mechanism is efficient , strategy-proof , individually rational , and does not incur a deficit . However , the VCG mechanism is not (strongly) budget balanced: generally , the agents\" payments will sum to more than 0 . If there is an auctioneer who is selling the items , this may be desirable , because the surplus payment corresponds to revenue for the auctioneer . However , if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves , any surplus payment is undesirable , because it will have to flow out of the system of agents . In 2006 , Cavallo 3 proposed a mechanism that redistributes some of the VCG payment back to the agents , while maintaining efficiency , strategy-proofness , individual rationality , and the non-deficit property . In this paper , we extend this result in a restricted setting . We study allocation settings where there are multiple indistinguishable units of a single good , and agents have unit demand . (For this specific setting , Cavallo\"s mechanism coincides with a mechanism proposed by Bailey in 1997 2 .) Here we propose a family of mechanisms that redistribute some of the VCG payment back to the agents . All mechanisms in the family are efficient , strategyproof , individually rational , and never incur a deficit . The family includes the Bailey-Cavallo mechanism as a special case . We then provide an optimization model for finding the optimal mechanism-that is , the mechanism that maximizes redistribution in the worst case-inside the family , and show how to cast this model as a linear program . We give both numerical and analytical solutions of this linear program , and the (unique) resulting mechanism shows significant improvement over the Bailey-Cavallo mechanism (in the worst case) . Finally , we prove that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties.", "label": ["individually rational mechanism", "mechanism", "payment redistribution", "transformation to linear programming", "efficient mechanism", "worst-case optimal mechanism", "analytical characterization", "strategy-proofness", "redistribution payment", "vickrey-clarke-grove mechanism", "vickrey-clarke-grove", "linear vcg redistribution mechanism", "mechanism design"], "stemmed_label": ["individu ration mechan", "mechan", "payment redistribut", "transform to linear program", "effici mechan", "worst-cas optim mechan", "analyt character", "strategy-proof", "redistribut payment", "vickrey-clarke-grov mechan", "vickrey-clarke-grov", "linear vcg redistribut mechan", "mechan design"]}
{"doc": "In barter-exchange markets , agents seek to swap their items with one another , in order to improve their own utilities . These swaps consist of cycles of agents , with each agent receiving the item of the next agent in the cycle . We focus mainly on the upcoming national kidney-exchange market , where patients with kidney disease can obtain compatible donors by swapping their own willing but incompatible donors . With over 70,000 patients already waiting for a cadaver kidney in the US , this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease . The clearing problem involves finding a social welfare maximizing exchange when the maximum length of a cycle is fixed . Long cycles are forbidden , since , for incentive reasons , all transplants in a cycle must be performed simultaneously . Also , in barter-exchanges generally , more agents are affected if one drops out of a longer cycle . We prove that the clearing problem with this cycle-length constraint is NP-hard . Solving it exactly is one of the main challenges in establishing a national kidney exchange . We present the first algorithm capable of clearing these markets on a nationwide scale . The key is incremental problem formulation . We adapt two paradigms for the task: constraint generation and column generation . For each , we develop techniques that dramatically improve both runtime and memory usage . We conclude that column generation scales drastically better than constraint generation . Our algorithm also supports several generalizations , as demanded by real-world kidney exchanges . Our algorithm replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation , one of the leading kidney exchanges . The match runs are conducted every two weeks and transplants based on our optimizations have already been conducted.", "label": ["barter", "instance generator", "column generation", "matching", "edge formulation", "kidney", "cycle formulation", "match", "branch-and-price", "exchange", "transplant", "barter-exchange market", "solution approach", "market characteristic"], "stemmed_label": ["barter", "instanc gener", "column gener", "match", "edg formul", "kidney", "cycl formul", "match", "branch-and-pric", "exchang", "transplant", "barter-exchang market", "solut approach", "market characterist"]}
{"doc": "Information markets , which are designed specifically to aggregate traders\" information , are becoming increasingly popular as a means for predicting future events . Recent research in information markets has resulted in two new designs , market scoring rules and dynamic parimutuel markets . We develop an analytic method to guide the design and strategic analysis of information markets . Our central contribution is a new abstract betting game , the projection game , that serves as a useful model for information markets . We demonstrate that this game can serve as a strategic model of dynamic parimutuel markets , and also captures the essence of the strategies in market scoring rules . The projection game is tractable to analyze , and has an attractive geometric visualization that makes the strategic moves and interactions more transparent . We use it to prove several strategic properties about the dynamic parimutuel market . We also prove that a special form of the projection game is strategically equivalent to the spherical scoring rule , and it is strategically similar to other scoring rules . Finally , we illustrate two applications of the model to analysis of complex strategic scenarios: we analyze the precision of a market in which traders have inertia , and a market in which a trader can profit by manipulating another trader\"s beliefs.", "label": ["liquidation time", "dpm", "msr", "projection game model", "social and behavioral sciences-economics", "strategic analysis", "prediction market", "dynamic parimutuel market", "market scoring rule", "information market", "long-range manipulative strategy", "projection game", "spherical scoring rule"], "stemmed_label": ["liquid time", "dpm", "msr", "project game model", "social and behavior sciences-econom", "strateg analysi", "predict market", "dynam parimutuel market", "market score rule", "inform market", "long-rang manipul strategi", "project game", "spheric score rule"]}
{"doc": "We consider a permutation betting scenario , where people wager on the final ordering of n candidates: for example , the outcome of a horse race . We examine the auctioneer problem of risklessly matching up wagers or , equivalently , finding arbitrage opportunities among the proposed wagers . Requiring bidders to explicitly list the orderings that they\"d like to bet on is both unnatural and intractable , because the number of orderings is n! and the number of subsets of orderings is 2n! . We propose two expressive betting languages that seem natural for bidders , and examine the computational complexity of the auctioneer problem in each case . Subset betting allows traders to bet either that a candidate will end up ranked among some subset of positions in the final ordering , for example , horse A will finish in positions 4 , 9 , or 13-21 , or that a position will be taken by some subset of candidates , for example horse A , B , or D will finish in position 2 . For subset betting , we show that the auctioneer problem can be solved in polynomial time if orders are divisible . Pair betting allows traders to bet on whether one candidate will end up ranked higher than another candidate , for example horse A will beat horse B . We prove that the auctioneer problem becomes NP-hard for pair betting . We identify a sufficient condition for the existence of a pair betting match that can be verified in polynomial time . We also show that a natural greedy algorithm gives a poor approximation for indivisible orders.", "label": ["pair-betting market", "minimum feedback", "permutation combinatoric", "greedy algorithm", "order match", "complex polynomial transformation", "expressive bet", "permutation betting", "prediction market", "subset betting", "computational complexity", "information aggregation", "polynomial-time algorithm", "bilateral trading partner", "bipartite graph"], "stemmed_label": ["pair-bet market", "minimum feedback", "permut combinator", "greedi algorithm", "order match", "complex polynomi transform", "express bet", "permut bet", "predict market", "subset bet", "comput complex", "inform aggreg", "polynomial-tim algorithm", "bilater trade partner", "bipartit graph"]}
{"doc": "In set-system auctions , there are several overlapping teams of agents , and a task that can be completed by any of these teams . The auctioneer\"s goal is to hire a team and pay as little as possible . Examples of this setting include shortest-path auctions and vertex-cover auctions . Recently , Karlin , Kempe and Tamir introduced a new definition of frugality ratio for this problem . Informally , the frugality ratio is the ratio of the total payment of a mechanism to a desired payment bound . The ratio captures the extent to which the mechanism overpays , relative to perceived fair cost in a truthful auction . In this paper , we propose a new truthful polynomial-time auction for the vertex cover problem and bound its frugality ratio . We show that the solution quality is with a constant factor of optimal and the frugality ratio is within a constant factor of the best possible worst-case bound; this is the first auction for this problem to have these properties . Moreover , we show how to transform any truthful auction into a frugal one while preserving the approximation ratio . Also , we consider two natural modifications of the definition of Karlin et al. , and we analyse the properties of the resulting payment bounds , such as monotonicity , computational hardness , and robustness with respect to the draw-resolution rule . We study the relationships between the different payment bounds , both for general set systems and for specific set-system auctions , such as path auctions and vertex-cover auctions . We use these new definitions in the proof of our main result for vertex-cover auctions via a bootstrapping technique , which may be of independent interest.", "label": ["consecutive payment bound", "vertex cover", "frugality ratio", "bootstrapping technique", "polynomial-time", "auction", "frugality", "nonmonotonicity", "transferable utility", "co-operation", "vertex-cover auction", "monotone allocation rule"], "stemmed_label": ["consecut payment bound", "vertex cover", "frugal ratio", "bootstrap techniqu", "polynomial-tim", "auction", "frugal", "nonmonoton", "transfer util", "co-oper", "vertex-cov auction", "monoton alloc rule"]}
{"doc": "We develop a framework for trading in compound securities: financial instruments that pay off contingent on the outcomes of arbitrary statements in propositional logic . Buying or selling securities-which can be thought of as betting on or against a particular future outcome-allows agents both to hedge risk and to profit (in expectation) on subjective predictions . A compound securities market allows agents to place bets on arbitrary boolean combinations of events , enabling them to more closely achieve their optimal risk exposure , and enabling the market as a whole to more closely achieve the social optimum . The tradeoff for allowing such expressivity is in the complexity of the agents\" and auctioneer\"s optimization problems . We develop and motivate the concept of a compound securities market , presenting the framework through a series of formal definitions and examples . We then analyze in detail the auctioneer\"s matching problem . We show that , with n events , the matching problem is co-NP-complete in the divisible case and Σp 2-complete in the indivisible case . We show that the latter hardness result holds even under severe language restrictions on bids . With log n events , the problem is polynomial in the divisible case and NP-complete in the indivisible case . We briefly discuss matching algorithms and tractable special cases.", "label": ["combinatorial betting", "gamble", "speculate", "bayesian network", "combined-value trading", "arbitrary logical combination", "compound security market", "approximation algorithm", "effective probability assessment", "base security", "combinatorial bet", "trade in financial instrument base on logical formula", "risk allocation", "payoff vector", "tractable case", "hedge", "bet", "information aggregation", "compound security", "computational complexity of match"], "stemmed_label": ["combinatori bet", "gambl", "specul", "bayesian network", "combined-valu trade", "arbitrari logic combin", "compound secur market", "approxim algorithm", "effect probabl assess", "base secur", "combinatori bet", "trade in financi instrument base on logic formula", "risk alloc", "payoff vector", "tractabl case", "hedg", "bet", "inform aggreg", "compound secur", "comput complex of match"]}
{"doc": "Much recent research concerns systems , such as the Internet , whose components are owned and operated by different parties , each with his own selfish goal . The field of Algorithmic Mechanism Design handles the issue of private information held by the different parties in such computational settings . This paper deals with a complementary problem in such settings: handling the hidden actions that are performed by the different parties . Our model is a combinatorial variant of the classical principalagent problem from economic theory . In our setting a principal must motivate a team of strategic agents to exert costly effort on his behalf , but their actions are hidden from him . Our focus is on cases where complex combinations of the efforts of the agents influence the outcome . The principal motivates the agents by offering to them a set of contracts , which together put the agents in an equilibrium point of the induced game . We present formal models for this setting , suggest and embark on an analysis of some basic issues , but leave many questions open.", "label": ["combinatorial agency", "nash equilibrium", "contractible action", "k-orbit", "price of unaccountability", "unaccountability price", "con", "quality of service", "classical principalagent", "principal-agent model", "agency theory", "series-parallel network", "service quality", "anonymous technology", "optimal set of contract", "incentive", "contract optimal set"], "stemmed_label": ["combinatori agenc", "nash equilibrium", "contract action", "k-orbit", "price of unaccount", "unaccount price", "con", "qualiti of servic", "classic principalag", "principal-ag model", "agenc theori", "series-parallel network", "servic qualiti", "anonym technolog", "optim set of contract", "incent", "contract optim set"]}
{"doc": "A sequence of prices and demands are rationalizable if there exists a concave , continuous and monotone utility function such that the demands are the maximizers of the utility function over the budget set corresponding to the price . Afriat 1 presented necessary and sufficient conditions for a finite sequence to be rationalizable . Varian 20 and later Blundell et al . 3 , 4 continued this line of work studying nonparametric methods to forecasts demand . Their results essentially characterize learnability of degenerate classes of demand functions and therefore fall short of giving a general degree of confidence in the forecast . The present paper complements this line of research by introducing a statistical model and a measure of complexity through which we are able to study the learnability of classes of demand functions and derive a degree of confidence in the forecasts . Our results show that the class of all demand functions has unbounded complexity and therefore is not learnable , but that there exist interesting and potentially useful classes that are learnable from finite samples . We also present a learning algorithm that is an adaptation of a new proof of Afriat\"s theorem due to Teo and Vohra 17 .", "label": ["machine learn", "finite set of observation", "complexity problem", "rationalizability", "reveal preference", "income-lipschitz", "learning from revealed preference", "fat shattering dimension", "monotone concave utility function", "fat shatter", "probably approximately correct", "observation finite set", "forecast", "demand function"], "stemmed_label": ["machin learn", "finit set of observ", "complex problem", "rationaliz", "reveal prefer", "income-lipschitz", "learn from reveal prefer", "fat shatter dimens", "monoton concav util function", "fat shatter", "probabl approxim correct", "observ finit set", "forecast", "demand function"]}
{"doc": "We present an approximately-efficient and approximatelystrategyproof auction mechanism for a single-good multi-unit allocation problem . The bidding language in our auctions allows marginal-decreasing piecewise constant curves . First , we develop a fully polynomial-time approximation scheme for the multi-unit allocation problem , which computes a (1 + )approximation in worst-case time T = O(n3 / ) , given n bids each with a constant number of pieces . Second , we embed this approximation scheme within a Vickrey-Clarke-Groves (VCG) mechanism and compute payments to n agents for an asymptotic cost of O(T log n) . The maximal possible gain from manipulation to a bidder in the combined scheme is bounded by /(1+ )V , where V is the total surplus in the efficient outcome.", "label": ["reverse auction", "bidding language", "forward auction", "strategyproof", "approximation algorithm", "single-good multi-unit allocation problem", "multi-unit auction", "equilibrium", "approximately-efficient and approximately strategyproof auction mechanism", "marginal-decreasing piecewise constant curve", "dynamic programming", "vickrey-clarke-grove", "fully polynomial-time approximation scheme"], "stemmed_label": ["revers auction", "bid languag", "forward auction", "strategyproof", "approxim algorithm", "single-good multi-unit alloc problem", "multi-unit auction", "equilibrium", "approximately-effici and approxim strategyproof auction mechan", "marginal-decreas piecewis constant curv", "dynam program", "vickrey-clarke-grov", "fulli polynomial-tim approxim scheme"]}
{"doc": "Internet search companies sell advertisement slots based on users\" search queries via an auction . While there has been previous work on the auction process and its game-theoretic aspects , most of it focuses on the Internet company . In this work , we focus on the advertisers , who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return (the number of user clicks on their ads) for a given budget . We model the entire process and study this budget optimization problem . While most variants are NP-hard , we show , perhaps surprisingly , that simply randomizing between two uniform strategies that bid equally on all the keywords works well . More precisely , this strategy gets at least a 1 − 1/e fraction of the maximum clicks possible . As our preliminary experiments show , such uniform strategies are likely to be practical . We also present inapproximability results , and optimal algorithms for variants of the budget optimization problem.", "label": ["bid", "game theory", "internet", "keyword", "search-based advertising auction", "advertiser", "lp", "optimization", "budget optimization", "auction", "generalized second price", "vickrey clark grove", "uniform bidding strategy", "intriguing heuristic", "sponsor search"], "stemmed_label": ["bid", "game theori", "internet", "keyword", "search-bas advertis auction", "advertis", "lp", "optim", "budget optim", "auction", "gener second price", "vickrey clark grove", "uniform bid strategi", "intrigu heurist", "sponsor search"]}
{"doc": "While traditional mechanism design typically assumes isomorphism between the agents\" type- and action spaces , in many situations the agents face strict restrictions on their action space due to , e.g. , technical , behavioral or regulatory reasons . We devise a general framework for the study of mechanism design in single-parameter environments with restricted action spaces . Our contribution is threefold . First , we characterize sufficient conditions under which the information-theoretically optimal social-choice rule can be implemented in dominant strategies , and prove that any multilinear social-choice rule is dominant-strategy implementable with no additional cost . Second , we identify necessary conditions for the optimality of action-bounded mechanisms , and fully characterize the optimal mechanisms and strategies in games with two players and two alternatives . Finally , we prove that for any multilinear social-choice rule , the optimal mechanism with k actions incurs an expected loss of O( 1 k2 ) compared to the optimal mechanisms with unrestricted action spaces . Our results apply to various economic and computational settings , and we demonstrate their applicability to signaling games , public-good models and routing in networks.", "label": ["implementation", "multilinear function", "single-crossing condition", "communication complexity", "probability of success", "single-cross condition", "action-bounded mechanism", "social-choice function", "bounded action space", "decision function", "dominant strategy", "optimal mechanism", "mechansm design", "success probability"], "stemmed_label": ["implement", "multilinear function", "single-cross condit", "commun complex", "probabl of success", "single-cross condit", "action-bound mechan", "social-choic function", "bound action space", "decis function", "domin strategi", "optim mechan", "mechansm design", "success probabl"]}
{"doc": "In multiagent systems , strategic settings are often analyzed under the assumption that the players choose their strategies simultaneously . However , this model is not always realistic . In many settings , one player is able to commit to a strategy before the other player makes a decision . Such models are synonymously referred to as leadership , commitment , or Stackelberg models , and optimal play in such models is often significantly different from optimal play in the model where strategies are selected simultaneously . The recent surge in interest in computing game-theoretic solutions has so far ignored leadership models (with the exception of the interest in mechanism design , where the designer is implicitly in a leadership position) . In this paper , we study how to compute optimal strategies to commit to under both commitment to pure strategies and commitment to mixed strategies , in both normal-form and Bayesian games . We give both positive results (efficient algorithms) and negative results (NP-hardness results).", "label": ["normal-form game", "nash equilibrium", "np-hardness", "game theory", "bayesian game", "pure strategy", "commitment", "stackelberg", "leadership model", "leadership", "optimal strategy", "stackelberg model", "simultaneous manner", "mixed strategy", "multiagent system"], "stemmed_label": ["normal-form game", "nash equilibrium", "np-hard", "game theori", "bayesian game", "pure strategi", "commit", "stackelberg", "leadership model", "leadership", "optim strategi", "stackelberg model", "simultan manner", "mix strategi", "multiag system"]}
{"doc": "Graphical games have been proposed as a game-theoretic model of large-scale distributed networks of non-cooperative agents . When the number of players is large , and the underlying graph has low degree , they provide a concise way to represent the players\" payoffs . It has recently been shown that the problem of finding Nash equilibria in a general degree-3 graphical game with two actions per player is complete for the complexity class PPAD , indicating that it is unlikely that there is any polynomial-time algorithm for this problem . In this paper , we study the complexity of graphical games with two actions per player on bounded-degree trees . This setting was first considered by Kearns , Littman and Singh , who proposed a dynamic programming-based algorithm that computes all Nash equilibria of such games . The running time of their algorithm is exponential , though approximate equilibria can be computed efficiently . Later , Littman , Kearns and Singh proposed a modification to this algorithm that can find a single Nash equilibrium in polynomial time . We show that this modified algorithm is incorrect - the output is not always a Nash equilibrium . We then propose a new algorithm that is based on the ideas of Kearns et al . and computes all Nash equilibria in quadratic time if the input graph is a path , and in polynomial time if it is an arbitrary graph of maximum degree 2 . Moreover , our algorithm can be used to compute Nash equilibria of graphical games on arbitrary trees , but the running time can be exponential , even when the tree has bounded degree . We show that this is inevitable - any algorithm of this type will take exponential time , even on bounded-degree trees with pathwidth 2 . It is an open question whether our algorithm runs in polynomial time on graphs with pathwidth 1 , but we show that finding a Nash equilibrium for a 2-action graphical game in which the underlying graph has maximum degree 3 and constant pathwidth is PPAD-complete (so is unlikely to be tractable).", "label": ["nash equilibrium", "bounded-degree tree", "dynamic programming-based algorithm", "degree", "response policy", "large-scale distributed network", "graphical game", "downstream pass", "breakpoint policy", "ppad-completeness", "generic algorithm"], "stemmed_label": ["nash equilibrium", "bounded-degre tree", "dynam programming-bas algorithm", "degre", "respons polici", "large-scal distribut network", "graphic game", "downstream pass", "breakpoint polici", "ppad-complet", "gener algorithm"]}
{"doc": "Keyword auctions lie at the core of the business models of today\"s leading search engines . Advertisers bid for placement alongside search results , and are charged for clicks on their ads . Advertisers are typically ranked according to a score that takes into account their bids and potential clickthrough rates . We consider a family of ranking rules that contains those typically used to model Yahoo! and Google\"s auction designs as special cases . We find that in general neither of these is necessarily revenue-optimal in equilibrium , and that the choice of ranking rule can be guided by considering the correlation between bidders\" values and click-through rates . We propose a simple approach to determine a revenue-optimal ranking rule within our family , taking into account effects on advertiser satisfaction and user experience . We illustrate the approach using Monte-Carlo simulations based on distributions fitted to Yahoo! bid and click-through rate data for a high-volume keyword.", "label": ["profit", "revenue-optimal ranking", "rank-by-revenue", "ranking rule", "optimal auction design problem", "rank-by-bid", "revenue", "advertising revenue", "advertisement", "pricing search keyword", "keyword auction", "search engine", "sponsor search", "sponsored search"], "stemmed_label": ["profit", "revenue-optim rank", "rank-by-revenu", "rank rule", "optim auction design problem", "rank-by-bid", "revenu", "advertis revenu", "advertis", "price search keyword", "keyword auction", "search engin", "sponsor search", "sponsor search"]}
{"doc": "In many settings , competing technologies - for example , operating systems , instant messenger systems , or document formatscan be seen adopting a limited amount of compatibility with one another; in other words , the difficulty in using multiple technologies is balanced somewhere between the two extremes of impossibility and effortless interoperability . There are a range of reasons why this phenomenon occurs , many of which - based on legal , social , or business considerations - seem to defy concise mathematical models . Despite this , we show that the advantages of limited compatibility can arise in a very simple model of diffusion in social networks , thus offering a basic explanation for this phenomenon in purely strategic terms . Our approach builds on work on the diffusion of innovations in the economics literature , which seeks to model how a new technology A might spread through a social network of individuals who are currently users of technology B . We consider several ways of capturing the compatibility of A and B , focusing primarily on a model in which users can choose to adopt A , adopt B , or - at an extra cost - adopt both A and B . We characterize how the ability of A to spread depends on both its quality relative to B , and also this additional cost of adopting both , and find some surprising non-monotonicity properties in the dependence on these parameters: in some cases , for one technology to survive the introduction of another , the cost of adopting both technologies must be balanced within a narrow , intermediate range . We also extend the framework to the case of multiple technologies , where we find that a simple This work has been supported in part by NSF grants CCF0325453 , IIS-0329064 , CNS-0403340 , and BCS-0537606 , a Google Research Grant , a Yahoo! Research Alliance Grant , the Institute for the Social Sciences at Cornell , and the John D . and Catherine T . MacArthur Foundation . model captures the phenomenon of two firms adopting a limited strategic alliance to defend against a new , third technology.", "label": ["potential function", "interoperability", "diffusion process", "diffusion of innovation", "contagion on network", "game-theoretic diffusion model", "limited compatibility", "algorithmic game theory", "non-convexity property", "morris's theorem", "bilinguality", "contagion threshold", "contagion game", "strategic incompatibility", "innovation diffusion", "characterization"], "stemmed_label": ["potenti function", "interoper", "diffus process", "diffus of innov", "contagion on network", "game-theoret diffus model", "limit compat", "algorithm game theori", "non-convex properti", "morris' theorem", "bilingu", "contagion threshold", "contagion game", "strateg incompat", "innov diffus", "character"]}
{"doc": "In this work we study cost sharing connection games , where each player has a source and sink he would like to connect , and the cost of the edges is either shared equally (fair connection games) or in an arbitrary way (general connection games) . We study the graph topologies that guarantee the existence of a strong equilibrium (where no coalition can improve the cost of each of its members) regardless of the specific costs on the edges . Our main existence results are the following: (1) For a single source and sink we show that there is always a strong equilibrium (both for fair and general connection games) . (2) For a single source multiple sinks we show that for a series parallel graph a strong equilibrium always exists (both for fair and general connection games) . (3) For multi source and sink we show that an extension parallel graph always admits a strong equilibrium in fair connection games . As for the quality of the strong equilibrium we show that in any fair connection games the cost of a strong equilibrium is Θ(log n) from the optimal solution , where n is the number of players . (This should be contrasted with the Ω(n) price of anarchy for the same setting.) For single source general connection games and single source single sink fair connection games , we show that a strong equilibrium is always an optimal solution.", "label": ["specific cost", "extension parallel graph", "single source multiple sink", "cost sharing connection game", "player number", "graph topology", "strong equilibrium", "multi source and sink", "general connection game", "fair connection game", "cost share game", "cost of the edge", "game theory", "nash equilibrium", "the edge cost", "anarchy price", "network design", "coalition", "strong price of anarchy", "optimal solution", "number of player", "single source and sink", "price of anarchy"], "stemmed_label": ["specif cost", "extens parallel graph", "singl sourc multipl sink", "cost share connect game", "player number", "graph topolog", "strong equilibrium", "multi sourc and sink", "gener connect game", "fair connect game", "cost share game", "cost of the edg", "game theori", "nash equilibrium", "the edg cost", "anarchi price", "network design", "coalit", "strong price of anarchi", "optim solut", "number of player", "singl sourc and sink", "price of anarchi"]}
{"doc": "According to economic theory-supported by empirical and laboratory evidence-the equilibrium price of a financial security reflects all of the information regarding the security\"s value . We investigate the computational process on the path toward equilibrium , where information distributed among traders is revealed step-by-step over time and incorporated into the market price . We develop a simplified model of an information market , along with trading strategies , in order to formalize the computational properties of the process . We show that securities whose payoffs cannot be expressed as weighted threshold functions of distributed input bits are not guaranteed to converge to the proper equilibrium predicted by economic theory . On the other hand , securities whose payoffs are threshold functions are guaranteed to converge , for all prior probability distributions . Moreover , these threshold securities converge in at most n rounds , where n is the number of bits of distributed information . We also prove a lower bound , showing a type of threshold security that requires at least n/2 rounds to converge in the worst case.", "label": ["worst case", "number of bit", "distributed information", "round", "information market", "computational property of the process", "simplified model", "lower bound", "financial security", "convergence to equilibrium", "market price", "economic theory", "trader", "empirical and laboratory evidence", "security's value", "trading strategy", "efficient market hypothesis", "probability distribution", "information aggregation", "bit number", "threshold function", "path toward equilibrium", "computational process", "payoff", "distribute information market", "rational expectation", "equilibrium price", "market computation", "security"], "stemmed_label": ["worst case", "number of bit", "distribut inform", "round", "inform market", "comput properti of the process", "simplifi model", "lower bound", "financi secur", "converg to equilibrium", "market price", "econom theori", "trader", "empir and laboratori evid", "security' valu", "trade strategi", "effici market hypothesi", "probabl distribut", "inform aggreg", "bit number", "threshold function", "path toward equilibrium", "comput process", "payoff", "distribut inform market", "ration expect", "equilibrium price", "market comput", "secur"]}
